{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(112358)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from random import randint \n",
    "\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Track ID</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Key</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>Sunflower - Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>3KkXRkHbMCARz0aVfEt68P</td>\n",
       "      <td>90</td>\n",
       "      <td>0.55600</td>\n",
       "      <td>0.760</td>\n",
       "      <td>158040</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>-5.574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>89.911</td>\n",
       "      <td>4</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lil Baby</td>\n",
       "      <td>Drip Too Hard (Lil Baby &amp; Gunna)</td>\n",
       "      <td>78QR3Wp35dqAhFEc2qAGjE</td>\n",
       "      <td>86</td>\n",
       "      <td>0.08520</td>\n",
       "      <td>0.897</td>\n",
       "      <td>145543</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>-6.903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>112.511</td>\n",
       "      <td>4</td>\n",
       "      <td>0.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Travis Scott</td>\n",
       "      <td>SICKO MODE</td>\n",
       "      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n",
       "      <td>89</td>\n",
       "      <td>0.00513</td>\n",
       "      <td>0.834</td>\n",
       "      <td>312820</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>-3.714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>155.008</td>\n",
       "      <td>4</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Juice WRLD</td>\n",
       "      <td>Lucid Dreams</td>\n",
       "      <td>285pBltuF7vW8TeWk8hdRR</td>\n",
       "      <td>88</td>\n",
       "      <td>0.34900</td>\n",
       "      <td>0.511</td>\n",
       "      <td>239836</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>83.903</td>\n",
       "      <td>4</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>YNW Melly</td>\n",
       "      <td>Murder On My Mind</td>\n",
       "      <td>7eBqSVxrzQZtK2mmgRG6lC</td>\n",
       "      <td>86</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.759</td>\n",
       "      <td>268434</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>-7.985</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>115.007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Artist                                     Track Name                Track ID  Popularity  Acousticness  Danceability  Duration_ms  Energy  Instrumentalness  Key  Liveness  Loudness  Mode  Speechiness    Tempo  Time Signature  Valence\n",
       "0           0   Post Malone  Sunflower - Spider-Man: Into the Spider-Verse  3KkXRkHbMCARz0aVfEt68P          90       0.55600         0.760       158040   0.479          0.000000    2    0.0703    -5.574     1       0.0466   89.911               4    0.913\n",
       "1           1      Lil Baby               Drip Too Hard (Lil Baby & Gunna)  78QR3Wp35dqAhFEc2qAGjE          86       0.08520         0.897       145543   0.662          0.000000    1    0.5340    -6.903     0       0.2920  112.511               4    0.389\n",
       "2           2  Travis Scott                                     SICKO MODE  2xLMifQCjDGFmkHkpNLD9h          89       0.00513         0.834       312820   0.730          0.000000    8    0.1240    -3.714     1       0.2220  155.008               4    0.446\n",
       "3           3    Juice WRLD                                   Lucid Dreams  285pBltuF7vW8TeWk8hdRR          88       0.34900         0.511       239836   0.566          0.000000    6    0.3400    -7.230     0       0.2000   83.903               4    0.218\n",
       "4           4     YNW Melly                              Murder On My Mind  7eBqSVxrzQZtK2mmgRG6lC          86       0.14500         0.759       268434   0.730          0.000003    0    0.1100    -7.985     0       0.0516  115.007               4    0.740"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 2018 songs\n",
    "songs_df = pd.read_csv('data/songs_10000.csv')\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop additional index column\n",
    "songs_df = songs_df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# calculate summary statistics\n",
    "display(songs_df.describe())\n",
    "\n",
    "# print out variable types\n",
    "print(songs_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of pairwise scatterplots\n",
    "scatter_matrix(songs_df, alpha = 0.8, figsize = (30, 20), diagonal = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new binary response variable 'tophit'\n",
    "# classify as top hit if popularity > 60 (about halfway split)\n",
    "songs_df['tophit'] = np.where(songs_df['Popularity'] > 60, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for modeling\n",
    "songs_df_clean = songs_df.drop(columns = ['Artist', 'Track Name', 'Track ID', 'Popularity'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(songs_df_clean.loc[:, songs_df_clean.columns != 'tophit'], \n",
    "                                                    songs_df_clean.tophit, test_size = 0.2, \n",
    "                                                    random_state = 100, stratify = songs_df_clean.tophit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5dff457b70d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcvmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcvmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_meanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5dff457b70d9>\u001b[0m in \u001b[0;36mcalc_meanstd\u001b[0;34m(X_train, y_train, depths)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# get cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcvmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcvstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit cross-validated single decision tree\n",
    "depths = list(range(1, 21))\n",
    "\n",
    "def calc_meanstd(X_train, y_train, depths):\n",
    "    cvmeans = {}\n",
    "    cvstds = {}\n",
    "    train_scores = {}\n",
    "    for i in depths:\n",
    "        model = DecisionTreeClassifier(max_depth = i)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_train)\n",
    "        # get training set scores\n",
    "        train_scores[i] = accuracy_score(y_train, y_pred)\n",
    "        # get cross-validation scores\n",
    "        score = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5, n_jobs = -1)\n",
    "        cvmeans[i] = score.mean()\n",
    "        cvstds[i] = score.std()\n",
    "    return cvmeans, cvstds, train_scores\n",
    "\n",
    "cvmeans, cvstds, train_scores = calc_meanstd(X_train, y_train, depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best depth: 3\n",
      "Classification accuracy on training set: 0.645\n",
      "Classification accuracy on test set: 0.6395\n"
     ]
    }
   ],
   "source": [
    "# report best tree depth from cross-validation\n",
    "best_depth = sorted(cvmeans, key = cvmeans.get, reverse = True)[0]\n",
    "print('Best depth:', best_depth)\n",
    "\n",
    "# refit on best tree depth, then report classification accuracies\n",
    "best_model = DecisionTreeClassifier(max_depth = best_depth)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "best_cv_tree_train_score = accuracy_score(y_train, y_train_pred)\n",
    "print('Classification accuracy on training set:', best_cv_tree_train_score)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "best_cv_tree_test_score = accuracy_score(y_test, y_test_pred)\n",
    "print('Classification accuracy on test set:', best_cv_tree_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['Key', 'Time Signature', 'Mode']\n",
    "X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "num_features = X_train_num.columns.tolist()\n",
    "num_index_train = X_train.index.tolist()\n",
    "num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "X_train_dum = X_train[cat_cols]\n",
    "X_test_dum = X_test[cat_cols]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.299492</td>\n",
       "      <td>0.256368</td>\n",
       "      <td>0.716427</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.231333</td>\n",
       "      <td>0.850080</td>\n",
       "      <td>0.073395</td>\n",
       "      <td>0.394990</td>\n",
       "      <td>0.246939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832329</td>\n",
       "      <td>0.492386</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>0.619231</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.813003</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.591706</td>\n",
       "      <td>0.465306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7125</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210842</td>\n",
       "      <td>0.576650</td>\n",
       "      <td>0.267664</td>\n",
       "      <td>0.302591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086815</td>\n",
       "      <td>0.821384</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.378979</td>\n",
       "      <td>0.218367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527107</td>\n",
       "      <td>0.390863</td>\n",
       "      <td>0.346355</td>\n",
       "      <td>0.362713</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.076553</td>\n",
       "      <td>0.794274</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>0.538967</td>\n",
       "      <td>0.188776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.748223</td>\n",
       "      <td>0.348043</td>\n",
       "      <td>0.724443</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.066918</td>\n",
       "      <td>0.864429</td>\n",
       "      <td>0.051760</td>\n",
       "      <td>0.524237</td>\n",
       "      <td>0.666327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.616244</td>\n",
       "      <td>0.250428</td>\n",
       "      <td>0.744484</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.188397</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.038923</td>\n",
       "      <td>0.581293</td>\n",
       "      <td>0.767347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>0.982741</td>\n",
       "      <td>0.351473</td>\n",
       "      <td>0.590172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054456</td>\n",
       "      <td>0.821585</td>\n",
       "      <td>0.263975</td>\n",
       "      <td>0.590611</td>\n",
       "      <td>0.273469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.627411</td>\n",
       "      <td>0.200290</td>\n",
       "      <td>0.561113</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.195727</td>\n",
       "      <td>0.712118</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>0.454586</td>\n",
       "      <td>0.121429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795180</td>\n",
       "      <td>0.356345</td>\n",
       "      <td>0.378925</td>\n",
       "      <td>0.290567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304639</td>\n",
       "      <td>0.809695</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>0.653706</td>\n",
       "      <td>0.343878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.237563</td>\n",
       "      <td>0.253184</td>\n",
       "      <td>0.928856</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.864920</td>\n",
       "      <td>0.448240</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>0.283673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559236</td>\n",
       "      <td>0.873096</td>\n",
       "      <td>0.226852</td>\n",
       "      <td>0.688371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209341</td>\n",
       "      <td>0.835710</td>\n",
       "      <td>0.072567</td>\n",
       "      <td>0.511415</td>\n",
       "      <td>0.763265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.852792</td>\n",
       "      <td>0.213696</td>\n",
       "      <td>0.544079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>0.823149</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.772684</td>\n",
       "      <td>0.164286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.168713</td>\n",
       "      <td>0.567126</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.428212</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.467871</td>\n",
       "      <td>0.123469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072689</td>\n",
       "      <td>0.638579</td>\n",
       "      <td>0.290009</td>\n",
       "      <td>0.609211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069431</td>\n",
       "      <td>0.834123</td>\n",
       "      <td>0.079814</td>\n",
       "      <td>0.468067</td>\n",
       "      <td>0.421429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.348392</td>\n",
       "      <td>0.968528</td>\n",
       "      <td>0.205972</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.118232</td>\n",
       "      <td>0.796643</td>\n",
       "      <td>0.379917</td>\n",
       "      <td>0.581570</td>\n",
       "      <td>0.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965863</td>\n",
       "      <td>0.615228</td>\n",
       "      <td>0.229943</td>\n",
       "      <td>0.218421</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.681142</td>\n",
       "      <td>0.070083</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.538776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.225381</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.417740</td>\n",
       "      <td>0.165162</td>\n",
       "      <td>0.066563</td>\n",
       "      <td>0.449593</td>\n",
       "      <td>0.037143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.243781</td>\n",
       "      <td>0.873745</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.289978</td>\n",
       "      <td>0.902356</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>0.558812</td>\n",
       "      <td>0.386735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.726904</td>\n",
       "      <td>0.328276</td>\n",
       "      <td>0.542075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.154244</td>\n",
       "      <td>0.635977</td>\n",
       "      <td>0.394898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>0.396954</td>\n",
       "      <td>0.303313</td>\n",
       "      <td>0.650294</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.841096</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.787141</td>\n",
       "      <td>0.205102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588352</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.621235</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.173735</td>\n",
       "      <td>0.837878</td>\n",
       "      <td>0.087578</td>\n",
       "      <td>0.708849</td>\n",
       "      <td>0.412245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.967513</td>\n",
       "      <td>0.212773</td>\n",
       "      <td>0.555101</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.088910</td>\n",
       "      <td>0.881660</td>\n",
       "      <td>0.078675</td>\n",
       "      <td>0.458934</td>\n",
       "      <td>0.466327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6979</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513051</td>\n",
       "      <td>0.716751</td>\n",
       "      <td>0.247447</td>\n",
       "      <td>0.380749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.845365</td>\n",
       "      <td>0.338509</td>\n",
       "      <td>0.294681</td>\n",
       "      <td>0.388776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.151605</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.233786</td>\n",
       "      <td>0.491974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.829474</td>\n",
       "      <td>0.078364</td>\n",
       "      <td>0.590462</td>\n",
       "      <td>0.268367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.727911</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.219042</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.270081</td>\n",
       "      <td>0.647036</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.454423</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.264132</td>\n",
       "      <td>0.729453</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.866284</td>\n",
       "      <td>0.043375</td>\n",
       "      <td>0.558821</td>\n",
       "      <td>0.520408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.564467</td>\n",
       "      <td>0.322924</td>\n",
       "      <td>0.672338</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.853813</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>0.539086</td>\n",
       "      <td>0.268392</td>\n",
       "      <td>0.848694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073830</td>\n",
       "      <td>0.908770</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.477194</td>\n",
       "      <td>0.387755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.958835</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.099799</td>\n",
       "      <td>0.359707</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.073725</td>\n",
       "      <td>0.779099</td>\n",
       "      <td>0.305383</td>\n",
       "      <td>0.382578</td>\n",
       "      <td>0.848980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065159</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092052</td>\n",
       "      <td>0.854237</td>\n",
       "      <td>0.365424</td>\n",
       "      <td>0.680530</td>\n",
       "      <td>0.723469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.521827</td>\n",
       "      <td>0.251391</td>\n",
       "      <td>0.362713</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>0.746178</td>\n",
       "      <td>0.045756</td>\n",
       "      <td>0.345163</td>\n",
       "      <td>0.076837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.273091</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>0.303535</td>\n",
       "      <td>0.508006</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.867602</td>\n",
       "      <td>0.052588</td>\n",
       "      <td>0.464718</td>\n",
       "      <td>0.690816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.574619</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.412814</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.839777</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.372010</td>\n",
       "      <td>0.479592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117468</td>\n",
       "      <td>0.458883</td>\n",
       "      <td>0.288229</td>\n",
       "      <td>0.758512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127657</td>\n",
       "      <td>0.899473</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.485868</td>\n",
       "      <td>0.304082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237950</td>\n",
       "      <td>0.665990</td>\n",
       "      <td>0.195294</td>\n",
       "      <td>0.380749</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>0.867424</td>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.622452</td>\n",
       "      <td>0.954082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703815</td>\n",
       "      <td>0.567513</td>\n",
       "      <td>0.388915</td>\n",
       "      <td>0.422834</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.067232</td>\n",
       "      <td>0.730355</td>\n",
       "      <td>0.201863</td>\n",
       "      <td>0.336221</td>\n",
       "      <td>0.194898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633533</td>\n",
       "      <td>0.168528</td>\n",
       "      <td>0.246161</td>\n",
       "      <td>0.319626</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.414598</td>\n",
       "      <td>0.645383</td>\n",
       "      <td>0.035611</td>\n",
       "      <td>0.507781</td>\n",
       "      <td>0.034796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.255018</td>\n",
       "      <td>0.565482</td>\n",
       "      <td>0.303818</td>\n",
       "      <td>0.512014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.844381</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.708963</td>\n",
       "      <td>0.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222890</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.221952</td>\n",
       "      <td>0.856711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503613</td>\n",
       "      <td>0.877235</td>\n",
       "      <td>0.244306</td>\n",
       "      <td>0.572211</td>\n",
       "      <td>0.784694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.621235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.902132</td>\n",
       "      <td>0.083954</td>\n",
       "      <td>0.442537</td>\n",
       "      <td>0.413265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072387</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.381846</td>\n",
       "      <td>0.608209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359095</td>\n",
       "      <td>0.863557</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.599680</td>\n",
       "      <td>0.531633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.146584</td>\n",
       "      <td>0.534010</td>\n",
       "      <td>0.240864</td>\n",
       "      <td>0.412814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.753615</td>\n",
       "      <td>0.270408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.433420</td>\n",
       "      <td>0.563117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294167</td>\n",
       "      <td>0.842728</td>\n",
       "      <td>0.169772</td>\n",
       "      <td>0.613424</td>\n",
       "      <td>0.447959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370481</td>\n",
       "      <td>0.758376</td>\n",
       "      <td>0.249414</td>\n",
       "      <td>0.558107</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.085768</td>\n",
       "      <td>0.795593</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.681352</td>\n",
       "      <td>0.563265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458834</td>\n",
       "      <td>0.773604</td>\n",
       "      <td>0.206872</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>0.867245</td>\n",
       "      <td>0.107660</td>\n",
       "      <td>0.704192</td>\n",
       "      <td>0.813265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>0.641624</td>\n",
       "      <td>0.177856</td>\n",
       "      <td>0.728451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>0.831754</td>\n",
       "      <td>0.077536</td>\n",
       "      <td>0.734701</td>\n",
       "      <td>0.415306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.725888</td>\n",
       "      <td>0.151669</td>\n",
       "      <td>0.618229</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.172688</td>\n",
       "      <td>0.871871</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.800172</td>\n",
       "      <td>0.288776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.883249</td>\n",
       "      <td>0.304406</td>\n",
       "      <td>0.665324</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.568058</td>\n",
       "      <td>0.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035440</td>\n",
       "      <td>0.939086</td>\n",
       "      <td>0.260079</td>\n",
       "      <td>0.737470</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.667940</td>\n",
       "      <td>0.503061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>0.752284</td>\n",
       "      <td>0.190576</td>\n",
       "      <td>0.557105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080637</td>\n",
       "      <td>0.847533</td>\n",
       "      <td>0.098033</td>\n",
       "      <td>0.636332</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300199</td>\n",
       "      <td>0.326904</td>\n",
       "      <td>0.432783</td>\n",
       "      <td>0.755506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120327</td>\n",
       "      <td>0.852695</td>\n",
       "      <td>0.284679</td>\n",
       "      <td>0.285503</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.689758</td>\n",
       "      <td>0.712690</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.144413</td>\n",
       "      <td>0.843487</td>\n",
       "      <td>0.156315</td>\n",
       "      <td>0.759108</td>\n",
       "      <td>0.231633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296183</td>\n",
       "      <td>0.656853</td>\n",
       "      <td>0.238091</td>\n",
       "      <td>0.646285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095193</td>\n",
       "      <td>0.916525</td>\n",
       "      <td>0.032195</td>\n",
       "      <td>0.608867</td>\n",
       "      <td>0.439796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161645</td>\n",
       "      <td>0.792893</td>\n",
       "      <td>0.306218</td>\n",
       "      <td>0.629251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073934</td>\n",
       "      <td>0.876743</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.536086</td>\n",
       "      <td>0.293878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488955</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.387787</td>\n",
       "      <td>0.313613</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.699133</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.654124</td>\n",
       "      <td>0.230612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.869036</td>\n",
       "      <td>0.152085</td>\n",
       "      <td>0.546083</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.131846</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.731489</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.366464</td>\n",
       "      <td>0.868020</td>\n",
       "      <td>0.179658</td>\n",
       "      <td>0.307601</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.759208</td>\n",
       "      <td>0.075776</td>\n",
       "      <td>0.408875</td>\n",
       "      <td>0.177551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046986</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.269306</td>\n",
       "      <td>0.880759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073097</td>\n",
       "      <td>0.920369</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.626913</td>\n",
       "      <td>0.772449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684738</td>\n",
       "      <td>0.411168</td>\n",
       "      <td>0.453074</td>\n",
       "      <td>0.420830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053304</td>\n",
       "      <td>0.835308</td>\n",
       "      <td>0.035197</td>\n",
       "      <td>0.641184</td>\n",
       "      <td>0.353061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.791164</td>\n",
       "      <td>0.525888</td>\n",
       "      <td>0.132431</td>\n",
       "      <td>0.442875</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.167452</td>\n",
       "      <td>0.785089</td>\n",
       "      <td>0.259834</td>\n",
       "      <td>0.372501</td>\n",
       "      <td>0.531633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Key  Time Signature  Mode  Acousticness  Danceability  Duration_ms    Energy  Instrumentalness  Liveness  Loudness  Speechiness     Tempo   Valence\n",
       "5760    2               4     1      0.002126      0.299492     0.256368  0.716427          0.000002  0.231333  0.850080     0.073395  0.394990  0.246939\n",
       "5683    2               4     1      0.832329      0.492386     0.225507  0.619231          0.000004  0.155933  0.813003     0.065942  0.591706  0.465306\n",
       "7125    6               1     1      0.210842      0.576650     0.267664  0.302591          0.000000  0.086815  0.821384     0.029089  0.378979  0.218367\n",
       "8092    2               4     1      0.527107      0.390863     0.346355  0.362713          0.000120  0.076553  0.794274     0.036128  0.538967  0.188776\n",
       "5228    8               4     1      0.262047      0.748223     0.348043  0.724443          0.000097  0.066918  0.864429     0.051760  0.524237  0.666327\n",
       "4715    2               4     1      0.000569      0.616244     0.250428  0.744484          0.000010  0.188397  0.884700     0.038923  0.581293  0.767347\n",
       "645    11               4     1      0.051303      0.982741     0.351473  0.590172          0.000000  0.054456  0.821585     0.263975  0.590611  0.273469\n",
       "4600    4               4     0      0.052307      0.627411     0.200290  0.561113          0.938000  0.195727  0.712118     0.025466  0.454586  0.121429\n",
       "6146    0               4     1      0.795180      0.356345     0.378925  0.290567          0.000000  0.304639  0.809695     0.030642  0.653706  0.343878\n",
       "7682    5               3     0      0.000010      0.237563     0.253184  0.928856          0.000039  0.097288  0.864920     0.448240  0.595200  0.283673\n",
       "3434    5               4     1      0.559236      0.873096     0.226852  0.688371          0.000000  0.209341  0.835710     0.072567  0.511415  0.763265\n",
       "4881    1               4     1      0.040058      0.852792     0.213696  0.544079          0.000000  0.100429  0.823149     0.492754  0.772684  0.164286\n",
       "9798    0               4     1      0.014255      0.560406     0.168713  0.567126          0.958000  0.428212  0.829452     0.029296  0.467871  0.123469\n",
       "2733    2               4     0      0.072689      0.638579     0.290009  0.609211          0.000000  0.069431  0.834123     0.079814  0.468067  0.421429\n",
       "1072    7               4     1      0.348392      0.968528     0.205972  0.581154          0.000002  0.118232  0.796643     0.379917  0.581570  0.489796\n",
       "8222    4               4     1      0.965863      0.615228     0.229943  0.218421          0.860000  0.094146  0.681142     0.070083  0.363673  0.538776\n",
       "9466    4               4     0      0.004626      0.225381     0.052391  0.312611          0.847000  0.417740  0.165162     0.066563  0.449593  0.037143\n",
       "1437    6               4     1      0.033130      0.786802     0.243781  0.873745          0.057900  0.289978  0.902356     0.048551  0.558812  0.386735\n",
       "201     1               4     1      0.002106      0.726904     0.328276  0.542075          0.000000  0.098335  0.819328     0.154244  0.635977  0.394898\n",
       "6710    3               4     0      0.351404      0.396954     0.303313  0.650294          0.000100  0.227144  0.841096     0.040890  0.787141  0.205102\n",
       "1868    5               4     0      0.588352      0.769543     0.221996  0.621235          0.000078  0.173735  0.837878     0.087578  0.708849  0.412245\n",
       "2018    0               4     1      0.618473      0.967513     0.212773  0.555101          0.000869  0.088910  0.881660     0.078675  0.458934  0.466327\n",
       "6979    2               4     1      0.513051      0.716751     0.247447  0.380749          0.000000  0.234475  0.845365     0.338509  0.294681  0.388776\n",
       "6392    7               4     0      0.151605      0.769543     0.233786  0.491974          0.000000  0.094146  0.829474     0.078364  0.590462  0.268367\n",
       "2611    4               4     0      0.727911      0.667005     0.219042  0.159302          0.730000  0.270081  0.647036     0.040890  0.454423  0.085000\n",
       "260     7               4     1      0.001755      0.676142     0.264132  0.729453          0.000004  0.022830  0.866284     0.043375  0.558821  0.520408\n",
       "6771   11               4     0      0.032026      0.564467     0.322924  0.672338          0.000377  0.094146  0.853813     0.066977  0.477317  0.257143\n",
       "3105    1               4     1      0.007458      0.539086     0.268392  0.848694          0.000000  0.073830  0.908770     0.054762  0.477194  0.387755\n",
       "5767    1               4     1      0.958835      0.707614     0.099799  0.359707          0.899000  0.073725  0.779099     0.305383  0.382578  0.848980\n",
       "6017    1               4     1      0.065159      0.857868     0.205877  0.581154          0.000000  0.092052  0.854237     0.365424  0.680530  0.723469\n",
       "...   ...             ...   ...           ...           ...          ...       ...               ...       ...       ...          ...       ...       ...\n",
       "7371    9               4     0      0.794176      0.521827     0.251391  0.362713          0.013900  0.084721  0.746178     0.045756  0.345163  0.076837\n",
       "9581    2               4     1      0.273091      0.867005     0.303535  0.508006          0.001970  0.058435  0.867602     0.052588  0.464718  0.690816\n",
       "1485    5               4     1      0.683734      0.574619     0.242194  0.412814          0.017000  0.055503  0.839777     0.027640  0.372010  0.479592\n",
       "6906    8               3     0      0.117468      0.458883     0.288229  0.758512          0.000000  0.127657  0.899473     0.261905  0.485868  0.304082\n",
       "1690    8               4     1      0.237950      0.665990     0.195294  0.380749          0.000456  0.084721  0.867424     0.052070  0.622452  0.954082\n",
       "409    11               4     1      0.703815      0.567513     0.388915  0.422834          0.000002  0.067232  0.730355     0.201863  0.336221  0.194898\n",
       "9326    0               4     1      0.633533      0.168528     0.246161  0.319626          0.842000  0.414598  0.645383     0.035611  0.507781  0.034796\n",
       "321     8               4     1      0.255018      0.565482     0.303818  0.512014          0.000000  0.081579  0.844381     0.030124  0.708963  0.614286\n",
       "4448   11               4     0      0.222890      0.676142     0.221952  0.856711          0.000000  0.503613  0.877235     0.244306  0.572211  0.784694\n",
       "4822    6               3     1      0.618473      0.392893     0.247304  0.621235          0.000000  0.064509  0.902132     0.083954  0.442537  0.413265\n",
       "9559    7               4     0      0.072387      0.734010     0.381846  0.608209          0.000000  0.359095  0.863557     0.158385  0.599680  0.531633\n",
       "4724    9               4     1      0.146584      0.534010     0.240864  0.412814          0.000000  0.087863  0.778808     0.046480  0.753615  0.270408\n",
       "1191   11               4     1      0.021685      0.560406     0.433420  0.563117          0.000000  0.294167  0.842728     0.169772  0.613424  0.447959\n",
       "4855    1               4     0      0.370481      0.758376     0.249414  0.558107          0.000035  0.085768  0.795593     0.071429  0.681352  0.563265\n",
       "2364    5               4     0      0.458834      0.773604     0.206872  0.739474          0.000000  0.098335  0.867245     0.107660  0.704192  0.813265\n",
       "637     0               4     1      0.045179      0.641624     0.177856  0.728451          0.000000  0.100429  0.831754     0.077536  0.734701  0.415306\n",
       "1094   11               4     0      0.099998      0.725888     0.151669  0.618229          0.000003  0.172688  0.871871     0.248447  0.800172  0.288776\n",
       "7665    9               4     1      0.007558      0.883249     0.304406  0.665324          0.341000  0.031731  0.820021     0.064182  0.568058  0.871429\n",
       "4581    1               4     1      0.035440      0.939086     0.260079  0.737470          0.000156  0.079066  0.856115     0.136646  0.667940  0.503061\n",
       "504     7               4     0      0.220882      0.752284     0.190576  0.557105          0.000000  0.080637  0.847533     0.098033  0.636332  0.628571\n",
       "1126   10               4     0      0.300199      0.326904     0.432783  0.755506          0.000000  0.120327  0.852695     0.284679  0.285503  0.400000\n",
       "3523    4               4     0      0.689758      0.712690     0.043799  0.499990          0.000002  0.144413  0.843487     0.156315  0.759108  0.231633\n",
       "7591    7               3     1      0.296183      0.656853     0.238091  0.646285          0.000000  0.095193  0.916525     0.032195  0.608867  0.439796\n",
       "5373    1               4     1      0.161645      0.792893     0.306218  0.629251          0.000000  0.073934  0.876743     0.069151  0.536086  0.293878\n",
       "7228    5               3     1      0.488955      0.670051     0.387787  0.313613          0.003490  0.087863  0.699133     0.027226  0.654124  0.230612\n",
       "4915    1               4     1      0.000601      0.869036     0.152085  0.546083          0.000973  0.131846  0.752749     0.261905  0.731489  0.050000\n",
       "2563    1               4     0      0.366464      0.868020     0.179658  0.307601          0.854000  0.087863  0.759208     0.075776  0.408875  0.177551\n",
       "223     4               4     1      0.046986      0.613198     0.269306  0.880759          0.000000  0.073097  0.920369     0.033333  0.626913  0.772449\n",
       "7161    9               3     1      0.684738      0.411168     0.453074  0.420830          0.000000  0.053304  0.835308     0.035197  0.641184  0.353061\n",
       "3638    0               4     0      0.791164      0.525888     0.132431  0.442875          0.904000  0.167452  0.785089     0.259834  0.372501  0.531633\n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 1s 95us/sample - loss: 0.6562 - accuracy: 0.6414 - val_loss: 0.6504 - val_accuracy: 0.6454\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6534 - accuracy: 0.6416 - val_loss: 0.6483 - val_accuracy: 0.6454\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6495 - accuracy: 0.6416 - val_loss: 0.6501 - val_accuracy: 0.6454\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6518 - accuracy: 0.6409 - val_loss: 0.6433 - val_accuracy: 0.6454\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6441 - accuracy: 0.6420 - val_loss: 0.6436 - val_accuracy: 0.6446\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6461 - accuracy: 0.6414 - val_loss: 0.6497 - val_accuracy: 0.6442\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6437 - accuracy: 0.6423 - val_loss: 0.6435 - val_accuracy: 0.6442\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6441 - accuracy: 0.6425 - val_loss: 0.6411 - val_accuracy: 0.6438\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6471 - accuracy: 0.6413 - val_loss: 0.6425 - val_accuracy: 0.6442\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 0s 53us/sample - loss: 0.6414 - accuracy: 0.6427 - val_loss: 0.6383 - val_accuracy: 0.6442\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 0.6402 - accuracy: 0.6432 - val_loss: 0.6391 - val_accuracy: 0.6438\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6383 - accuracy: 0.6434 - val_loss: 0.6428 - val_accuracy: 0.6429\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 0.6383 - accuracy: 0.6439 - val_loss: 0.6409 - val_accuracy: 0.6421\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 0s 51us/sample - loss: 0.6367 - accuracy: 0.6432 - val_loss: 0.6367 - val_accuracy: 0.6433\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 48us/sample - loss: 0.6360 - accuracy: 0.6432 - val_loss: 0.6459 - val_accuracy: 0.6375\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6352 - accuracy: 0.6432 - val_loss: 0.6410 - val_accuracy: 0.6408\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6356 - accuracy: 0.6443 - val_loss: 0.6437 - val_accuracy: 0.6383\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6389 - accuracy: 0.6455 - val_loss: 0.6377 - val_accuracy: 0.6429\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6345 - accuracy: 0.6446 - val_loss: 0.6433 - val_accuracy: 0.6433\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6324 - accuracy: 0.6439 - val_loss: 0.6392 - val_accuracy: 0.6429\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6322 - accuracy: 0.6459 - val_loss: 0.6395 - val_accuracy: 0.6425\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6324 - accuracy: 0.6450 - val_loss: 0.6395 - val_accuracy: 0.6408\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6290 - accuracy: 0.6457 - val_loss: 0.6411 - val_accuracy: 0.6383\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6307 - accuracy: 0.6425 - val_loss: 0.6399 - val_accuracy: 0.6392\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6273 - accuracy: 0.6475 - val_loss: 0.6430 - val_accuracy: 0.6358\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6266 - accuracy: 0.6486 - val_loss: 0.6398 - val_accuracy: 0.6383\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6285 - accuracy: 0.6446 - val_loss: 0.6434 - val_accuracy: 0.6400\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6263 - accuracy: 0.6495 - val_loss: 0.6428 - val_accuracy: 0.6404\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6254 - accuracy: 0.6482 - val_loss: 0.6450 - val_accuracy: 0.6363\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6282 - accuracy: 0.6466 - val_loss: 0.6415 - val_accuracy: 0.6388\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 0s 41us/sample - loss: 0.6230 - accuracy: 0.6489 - val_loss: 0.6473 - val_accuracy: 0.6404\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6250 - accuracy: 0.6511 - val_loss: 0.6444 - val_accuracy: 0.6371\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6242 - accuracy: 0.6500 - val_loss: 0.6454 - val_accuracy: 0.6358\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6224 - accuracy: 0.6507 - val_loss: 0.6433 - val_accuracy: 0.6404\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6240 - accuracy: 0.6496 - val_loss: 0.6419 - val_accuracy: 0.6438\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6210 - accuracy: 0.6513 - val_loss: 0.6511 - val_accuracy: 0.6379\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6208 - accuracy: 0.6507 - val_loss: 0.6502 - val_accuracy: 0.6442\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6200 - accuracy: 0.6511 - val_loss: 0.6451 - val_accuracy: 0.6388\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6178 - accuracy: 0.6539 - val_loss: 0.6462 - val_accuracy: 0.6371\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6172 - accuracy: 0.6548 - val_loss: 0.6495 - val_accuracy: 0.6371\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6191 - accuracy: 0.6521 - val_loss: 0.6501 - val_accuracy: 0.6350\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6171 - accuracy: 0.6552 - val_loss: 0.6450 - val_accuracy: 0.6325\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6163 - accuracy: 0.6571 - val_loss: 0.6482 - val_accuracy: 0.6346\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6140 - accuracy: 0.6550 - val_loss: 0.6477 - val_accuracy: 0.6338\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6135 - accuracy: 0.6479 - val_loss: 0.6520 - val_accuracy: 0.6425\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6162 - accuracy: 0.6509 - val_loss: 0.6541 - val_accuracy: 0.6413\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6121 - accuracy: 0.6582 - val_loss: 0.6502 - val_accuracy: 0.6325\n",
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6117 - accuracy: 0.6557 - val_loss: 0.6477 - val_accuracy: 0.6321\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6116 - accuracy: 0.6539 - val_loss: 0.6476 - val_accuracy: 0.6329\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6077 - accuracy: 0.6546 - val_loss: 0.6584 - val_accuracy: 0.6383\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6104 - accuracy: 0.6555 - val_loss: 0.6521 - val_accuracy: 0.6250\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6067 - accuracy: 0.6557 - val_loss: 0.6550 - val_accuracy: 0.6342\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6076 - accuracy: 0.6562 - val_loss: 0.6568 - val_accuracy: 0.6325\n",
      "Epoch 54/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6059 - accuracy: 0.6577 - val_loss: 0.6537 - val_accuracy: 0.6304\n",
      "Epoch 55/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6048 - accuracy: 0.6646 - val_loss: 0.6602 - val_accuracy: 0.6233\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6042 - accuracy: 0.6587 - val_loss: 0.6627 - val_accuracy: 0.6304\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6051 - accuracy: 0.6536 - val_loss: 0.6586 - val_accuracy: 0.6375\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6014 - accuracy: 0.6605 - val_loss: 0.6655 - val_accuracy: 0.6292\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6029 - accuracy: 0.6591 - val_loss: 0.6615 - val_accuracy: 0.6096\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6022 - accuracy: 0.6627 - val_loss: 0.6667 - val_accuracy: 0.6358\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6021 - accuracy: 0.6555 - val_loss: 0.6660 - val_accuracy: 0.6329\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5993 - accuracy: 0.6591 - val_loss: 0.6618 - val_accuracy: 0.6258\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5970 - accuracy: 0.6587 - val_loss: 0.6701 - val_accuracy: 0.6212\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5979 - accuracy: 0.6643 - val_loss: 0.6697 - val_accuracy: 0.6229\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5976 - accuracy: 0.6664 - val_loss: 0.6776 - val_accuracy: 0.6338\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5958 - accuracy: 0.6614 - val_loss: 0.6709 - val_accuracy: 0.6313\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5964 - accuracy: 0.6591 - val_loss: 0.6704 - val_accuracy: 0.6067\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5979 - accuracy: 0.6621 - val_loss: 0.6722 - val_accuracy: 0.6279\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5922 - accuracy: 0.6652 - val_loss: 0.6677 - val_accuracy: 0.6267\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5919 - accuracy: 0.6643 - val_loss: 0.6742 - val_accuracy: 0.6104\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5917 - accuracy: 0.6584 - val_loss: 0.6765 - val_accuracy: 0.6263\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5899 - accuracy: 0.6686 - val_loss: 0.6781 - val_accuracy: 0.6187\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5865 - accuracy: 0.6698 - val_loss: 0.6761 - val_accuracy: 0.6246\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5869 - accuracy: 0.6684 - val_loss: 0.6763 - val_accuracy: 0.6254\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5854 - accuracy: 0.6704 - val_loss: 0.6822 - val_accuracy: 0.6196\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5857 - accuracy: 0.6734 - val_loss: 0.6814 - val_accuracy: 0.6317\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5829 - accuracy: 0.6746 - val_loss: 0.6828 - val_accuracy: 0.6283\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5816 - accuracy: 0.6768 - val_loss: 0.6877 - val_accuracy: 0.6112\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5818 - accuracy: 0.6687 - val_loss: 0.6925 - val_accuracy: 0.6217\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5824 - accuracy: 0.6687 - val_loss: 0.6975 - val_accuracy: 0.6217\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5811 - accuracy: 0.6698 - val_loss: 0.6895 - val_accuracy: 0.6033\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5791 - accuracy: 0.6741 - val_loss: 0.6880 - val_accuracy: 0.6242\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5810 - accuracy: 0.6779 - val_loss: 0.6927 - val_accuracy: 0.6075\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5772 - accuracy: 0.6770 - val_loss: 0.6905 - val_accuracy: 0.6158\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5741 - accuracy: 0.6775 - val_loss: 0.7001 - val_accuracy: 0.6263\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5732 - accuracy: 0.6770 - val_loss: 0.6987 - val_accuracy: 0.6254\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5722 - accuracy: 0.6777 - val_loss: 0.7091 - val_accuracy: 0.6258\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5742 - accuracy: 0.6812 - val_loss: 0.6964 - val_accuracy: 0.6046\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5709 - accuracy: 0.6809 - val_loss: 0.7158 - val_accuracy: 0.6208\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5743 - accuracy: 0.6716 - val_loss: 0.6993 - val_accuracy: 0.6288\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5719 - accuracy: 0.6770 - val_loss: 0.6992 - val_accuracy: 0.6283\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5660 - accuracy: 0.6814 - val_loss: 0.7104 - val_accuracy: 0.6246\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5636 - accuracy: 0.6830 - val_loss: 0.7164 - val_accuracy: 0.6275\n",
      "Epoch 94/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5672 - accuracy: 0.6804 - val_loss: 0.7065 - val_accuracy: 0.6271\n",
      "Epoch 95/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5652 - accuracy: 0.6877 - val_loss: 0.7183 - val_accuracy: 0.6279\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5654 - accuracy: 0.6841 - val_loss: 0.7133 - val_accuracy: 0.6121\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5592 - accuracy: 0.6882 - val_loss: 0.7214 - val_accuracy: 0.6125\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5603 - accuracy: 0.6875 - val_loss: 0.7384 - val_accuracy: 0.6117\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5635 - accuracy: 0.6913 - val_loss: 0.7101 - val_accuracy: 0.6225\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5578 - accuracy: 0.6857 - val_loss: 0.7221 - val_accuracy: 0.6142\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5580 - accuracy: 0.6932 - val_loss: 0.7296 - val_accuracy: 0.6204\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5536 - accuracy: 0.6939 - val_loss: 0.7342 - val_accuracy: 0.6183\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5572 - accuracy: 0.6913 - val_loss: 0.7248 - val_accuracy: 0.6275\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5522 - accuracy: 0.6963 - val_loss: 0.7259 - val_accuracy: 0.6192\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5496 - accuracy: 0.6952 - val_loss: 0.7387 - val_accuracy: 0.6192\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5487 - accuracy: 0.6902 - val_loss: 0.7315 - val_accuracy: 0.6187\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5478 - accuracy: 0.6975 - val_loss: 0.7389 - val_accuracy: 0.6150\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5446 - accuracy: 0.6934 - val_loss: 0.7450 - val_accuracy: 0.6208\n",
      "Epoch 109/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.5410 - accuracy: 0.7054 - val_loss: 0.7483 - val_accuracy: 0.6167\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5445 - accuracy: 0.7027 - val_loss: 0.7545 - val_accuracy: 0.6400\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5441 - accuracy: 0.7068 - val_loss: 0.7428 - val_accuracy: 0.6150\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5410 - accuracy: 0.7055 - val_loss: 0.7522 - val_accuracy: 0.6146\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5419 - accuracy: 0.7054 - val_loss: 0.7487 - val_accuracy: 0.6217\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5381 - accuracy: 0.7027 - val_loss: 0.7562 - val_accuracy: 0.6087\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5396 - accuracy: 0.7039 - val_loss: 0.7732 - val_accuracy: 0.5954\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5419 - accuracy: 0.7004 - val_loss: 0.7543 - val_accuracy: 0.6100\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5330 - accuracy: 0.7082 - val_loss: 0.7597 - val_accuracy: 0.6242\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5370 - accuracy: 0.7034 - val_loss: 0.7595 - val_accuracy: 0.6292\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5385 - accuracy: 0.7066 - val_loss: 0.7658 - val_accuracy: 0.6237\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5328 - accuracy: 0.7086 - val_loss: 0.7579 - val_accuracy: 0.6196\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5245 - accuracy: 0.7191 - val_loss: 0.7705 - val_accuracy: 0.6121\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5282 - accuracy: 0.7173 - val_loss: 0.7607 - val_accuracy: 0.6171\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5273 - accuracy: 0.7139 - val_loss: 0.7723 - val_accuracy: 0.6229\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5270 - accuracy: 0.7159 - val_loss: 0.7786 - val_accuracy: 0.6217\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5264 - accuracy: 0.7096 - val_loss: 0.7642 - val_accuracy: 0.6067\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5229 - accuracy: 0.7179 - val_loss: 0.7789 - val_accuracy: 0.6150\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5187 - accuracy: 0.7195 - val_loss: 0.7798 - val_accuracy: 0.6167\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5184 - accuracy: 0.7221 - val_loss: 0.7823 - val_accuracy: 0.6183\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5176 - accuracy: 0.7255 - val_loss: 0.7854 - val_accuracy: 0.6117\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5147 - accuracy: 0.7259 - val_loss: 0.7919 - val_accuracy: 0.6175\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5176 - accuracy: 0.7204 - val_loss: 0.7935 - val_accuracy: 0.6246\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5104 - accuracy: 0.7236 - val_loss: 0.7881 - val_accuracy: 0.6162\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5173 - accuracy: 0.7234 - val_loss: 0.8011 - val_accuracy: 0.6150\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5141 - accuracy: 0.7223 - val_loss: 0.7950 - val_accuracy: 0.6208\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5075 - accuracy: 0.7275 - val_loss: 0.8097 - val_accuracy: 0.6150\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5061 - accuracy: 0.7309 - val_loss: 0.8010 - val_accuracy: 0.6200\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5046 - accuracy: 0.7334 - val_loss: 0.7977 - val_accuracy: 0.6142\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5060 - accuracy: 0.7323 - val_loss: 0.8090 - val_accuracy: 0.6221\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5001 - accuracy: 0.7364 - val_loss: 0.8134 - val_accuracy: 0.6146\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5048 - accuracy: 0.7314 - val_loss: 0.8158 - val_accuracy: 0.6183\n",
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5016 - accuracy: 0.7362 - val_loss: 0.8188 - val_accuracy: 0.6150\n",
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5040 - accuracy: 0.7296 - val_loss: 0.8289 - val_accuracy: 0.6196\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5016 - accuracy: 0.7334 - val_loss: 0.8164 - val_accuracy: 0.6196\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4924 - accuracy: 0.7427 - val_loss: 0.8233 - val_accuracy: 0.6208\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4937 - accuracy: 0.7352 - val_loss: 0.8361 - val_accuracy: 0.6167\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4976 - accuracy: 0.7375 - val_loss: 0.8452 - val_accuracy: 0.6150\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4911 - accuracy: 0.7454 - val_loss: 0.8308 - val_accuracy: 0.6162\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4812 - accuracy: 0.7480 - val_loss: 0.8256 - val_accuracy: 0.6167\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4804 - accuracy: 0.7482 - val_loss: 0.8584 - val_accuracy: 0.6208\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4808 - accuracy: 0.7484 - val_loss: 0.8325 - val_accuracy: 0.6142\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4817 - accuracy: 0.7437 - val_loss: 0.8479 - val_accuracy: 0.6017\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4851 - accuracy: 0.7391 - val_loss: 0.8620 - val_accuracy: 0.5867\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4910 - accuracy: 0.7464 - val_loss: 0.8443 - val_accuracy: 0.6183\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4834 - accuracy: 0.7521 - val_loss: 0.8483 - val_accuracy: 0.6242\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4801 - accuracy: 0.7379 - val_loss: 0.8483 - val_accuracy: 0.6121\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4728 - accuracy: 0.7580 - val_loss: 0.8500 - val_accuracy: 0.6075\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4730 - accuracy: 0.7502 - val_loss: 0.8678 - val_accuracy: 0.6008\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4777 - accuracy: 0.7486 - val_loss: 0.8945 - val_accuracy: 0.6154\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4732 - accuracy: 0.7479 - val_loss: 0.8694 - val_accuracy: 0.6096\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4691 - accuracy: 0.7550 - val_loss: 0.8745 - val_accuracy: 0.6204\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4709 - accuracy: 0.7552 - val_loss: 0.8818 - val_accuracy: 0.5996\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4695 - accuracy: 0.7527 - val_loss: 0.8732 - val_accuracy: 0.6079\n",
      "Epoch 163/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4670 - accuracy: 0.7564 - val_loss: 0.8851 - val_accuracy: 0.6092\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4581 - accuracy: 0.7659 - val_loss: 0.8759 - val_accuracy: 0.6029\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4587 - accuracy: 0.7650 - val_loss: 0.8811 - val_accuracy: 0.6154\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4587 - accuracy: 0.7634 - val_loss: 0.8858 - val_accuracy: 0.6058\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4550 - accuracy: 0.7623 - val_loss: 0.8952 - val_accuracy: 0.6125\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4631 - accuracy: 0.7596 - val_loss: 0.9162 - val_accuracy: 0.6104\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4538 - accuracy: 0.7713 - val_loss: 0.8966 - val_accuracy: 0.6108\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4553 - accuracy: 0.7661 - val_loss: 0.9001 - val_accuracy: 0.6021\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4533 - accuracy: 0.7659 - val_loss: 0.9029 - val_accuracy: 0.6121\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4595 - accuracy: 0.7579 - val_loss: 0.9178 - val_accuracy: 0.5971\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4616 - accuracy: 0.7634 - val_loss: 0.9341 - val_accuracy: 0.6087\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4481 - accuracy: 0.7686 - val_loss: 0.9181 - val_accuracy: 0.6012\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.4431 - accuracy: 0.7752 - val_loss: 0.9064 - val_accuracy: 0.5904\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.4446 - accuracy: 0.7741 - val_loss: 0.9221 - val_accuracy: 0.6225\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4420 - accuracy: 0.7764 - val_loss: 0.9269 - val_accuracy: 0.5983\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4392 - accuracy: 0.7761 - val_loss: 0.9074 - val_accuracy: 0.6067\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.4416 - accuracy: 0.7743 - val_loss: 0.9365 - val_accuracy: 0.6079\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4405 - accuracy: 0.7814 - val_loss: 0.9238 - val_accuracy: 0.5967\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4342 - accuracy: 0.7784 - val_loss: 0.9299 - val_accuracy: 0.6171\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4373 - accuracy: 0.7732 - val_loss: 0.9417 - val_accuracy: 0.6025\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4341 - accuracy: 0.7796 - val_loss: 0.9450 - val_accuracy: 0.6104\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4298 - accuracy: 0.7807 - val_loss: 0.9639 - val_accuracy: 0.6046\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4327 - accuracy: 0.7745 - val_loss: 0.9531 - val_accuracy: 0.5987\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4366 - accuracy: 0.7752 - val_loss: 0.9506 - val_accuracy: 0.5892\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4367 - accuracy: 0.7741 - val_loss: 0.9370 - val_accuracy: 0.6154\n",
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4336 - accuracy: 0.7823 - val_loss: 0.9574 - val_accuracy: 0.6004\n",
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4285 - accuracy: 0.7802 - val_loss: 0.9861 - val_accuracy: 0.6162\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4290 - accuracy: 0.7823 - val_loss: 0.9698 - val_accuracy: 0.6087\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4542 - accuracy: 0.7704 - val_loss: 0.9526 - val_accuracy: 0.5871\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4262 - accuracy: 0.7880 - val_loss: 0.9594 - val_accuracy: 0.5992\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4225 - accuracy: 0.7886 - val_loss: 0.9706 - val_accuracy: 0.5883\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4212 - accuracy: 0.7896 - val_loss: 0.9636 - val_accuracy: 0.6175\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4175 - accuracy: 0.7889 - val_loss: 0.9832 - val_accuracy: 0.5983\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4150 - accuracy: 0.7918 - val_loss: 0.9830 - val_accuracy: 0.6192\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4177 - accuracy: 0.7887 - val_loss: 0.9598 - val_accuracy: 0.6050\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4156 - accuracy: 0.7877 - val_loss: 0.9742 - val_accuracy: 0.6037\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4142 - accuracy: 0.7959 - val_loss: 1.0062 - val_accuracy: 0.6104\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4090 - accuracy: 0.7957 - val_loss: 1.0141 - val_accuracy: 0.6112\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4079 - accuracy: 0.7979 - val_loss: 0.9935 - val_accuracy: 0.6012\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.4085 - accuracy: 0.7979 - val_loss: 1.0296 - val_accuracy: 0.6121\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4088 - accuracy: 0.7970 - val_loss: 1.0069 - val_accuracy: 0.6108\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4030 - accuracy: 0.8021 - val_loss: 1.0040 - val_accuracy: 0.6008\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4022 - accuracy: 0.7995 - val_loss: 1.0312 - val_accuracy: 0.6012\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4008 - accuracy: 0.8005 - val_loss: 1.0322 - val_accuracy: 0.6079\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4035 - accuracy: 0.7948 - val_loss: 1.0128 - val_accuracy: 0.6104\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.4065 - accuracy: 0.7939 - val_loss: 1.0249 - val_accuracy: 0.5983\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3981 - accuracy: 0.8018 - val_loss: 1.0374 - val_accuracy: 0.6021\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3998 - accuracy: 0.8037 - val_loss: 1.0265 - val_accuracy: 0.5962\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.3970 - accuracy: 0.8052 - val_loss: 1.0607 - val_accuracy: 0.6137\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3978 - accuracy: 0.8007 - val_loss: 1.0626 - val_accuracy: 0.6129\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4004 - accuracy: 0.7989 - val_loss: 1.0481 - val_accuracy: 0.5817\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3996 - accuracy: 0.7982 - val_loss: 1.0769 - val_accuracy: 0.5900\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.3974 - accuracy: 0.8039 - val_loss: 1.0523 - val_accuracy: 0.6067\n",
      "Epoch 216/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4007 - accuracy: 0.8000 - val_loss: 1.0812 - val_accuracy: 0.6204\n",
      "Epoch 217/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3894 - accuracy: 0.8080 - val_loss: 1.0480 - val_accuracy: 0.6046\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3838 - accuracy: 0.8091 - val_loss: 1.0520 - val_accuracy: 0.5967\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3914 - accuracy: 0.8032 - val_loss: 1.0569 - val_accuracy: 0.5962\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3874 - accuracy: 0.8091 - val_loss: 1.1049 - val_accuracy: 0.6229\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3917 - accuracy: 0.8086 - val_loss: 1.0373 - val_accuracy: 0.5875\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3877 - accuracy: 0.8075 - val_loss: 1.0706 - val_accuracy: 0.6012\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3818 - accuracy: 0.8107 - val_loss: 1.0756 - val_accuracy: 0.5896\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3854 - accuracy: 0.8102 - val_loss: 1.0737 - val_accuracy: 0.6092\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3825 - accuracy: 0.8139 - val_loss: 1.0488 - val_accuracy: 0.5987\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3744 - accuracy: 0.8188 - val_loss: 1.0874 - val_accuracy: 0.6008\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3760 - accuracy: 0.8168 - val_loss: 1.1057 - val_accuracy: 0.6158\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3765 - accuracy: 0.8173 - val_loss: 1.0947 - val_accuracy: 0.6079\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3801 - accuracy: 0.8121 - val_loss: 1.0863 - val_accuracy: 0.5946\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3748 - accuracy: 0.8170 - val_loss: 1.0819 - val_accuracy: 0.5883\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3658 - accuracy: 0.8225 - val_loss: 1.0875 - val_accuracy: 0.6008\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3678 - accuracy: 0.8200 - val_loss: 1.0863 - val_accuracy: 0.5992\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3654 - accuracy: 0.8254 - val_loss: 1.1197 - val_accuracy: 0.5829\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3662 - accuracy: 0.8259 - val_loss: 1.0841 - val_accuracy: 0.6000\n",
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3671 - accuracy: 0.8255 - val_loss: 1.0885 - val_accuracy: 0.5792\n",
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3644 - accuracy: 0.8227 - val_loss: 1.1330 - val_accuracy: 0.6212\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3650 - accuracy: 0.8264 - val_loss: 1.1377 - val_accuracy: 0.6087\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3635 - accuracy: 0.8261 - val_loss: 1.1048 - val_accuracy: 0.5900\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3792 - accuracy: 0.8180 - val_loss: 1.1442 - val_accuracy: 0.5867\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3671 - accuracy: 0.8236 - val_loss: 1.1247 - val_accuracy: 0.5875\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3590 - accuracy: 0.8259 - val_loss: 1.1085 - val_accuracy: 0.6017\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3634 - accuracy: 0.8211 - val_loss: 1.1137 - val_accuracy: 0.5908\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3567 - accuracy: 0.8232 - val_loss: 1.1484 - val_accuracy: 0.5946\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3525 - accuracy: 0.8334 - val_loss: 1.1423 - val_accuracy: 0.5933\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3677 - accuracy: 0.8209 - val_loss: 1.1185 - val_accuracy: 0.5879\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3684 - accuracy: 0.8170 - val_loss: 1.1286 - val_accuracy: 0.5813\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3653 - accuracy: 0.8245 - val_loss: 1.1318 - val_accuracy: 0.5917\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3583 - accuracy: 0.8225 - val_loss: 1.1349 - val_accuracy: 0.5854\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3618 - accuracy: 0.8198 - val_loss: 1.1305 - val_accuracy: 0.6162\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.3567 - accuracy: 0.8293 - val_loss: 1.1627 - val_accuracy: 0.6000\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.3475 - accuracy: 0.8309 - val_loss: 1.1714 - val_accuracy: 0.6042\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3470 - accuracy: 0.8357 - val_loss: 1.1879 - val_accuracy: 0.5946\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3472 - accuracy: 0.8339 - val_loss: 1.1953 - val_accuracy: 0.6137\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3436 - accuracy: 0.8404 - val_loss: 1.1433 - val_accuracy: 0.5971\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3522 - accuracy: 0.8266 - val_loss: 1.1617 - val_accuracy: 0.5946\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3450 - accuracy: 0.8350 - val_loss: 1.1576 - val_accuracy: 0.5925\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3488 - accuracy: 0.8377 - val_loss: 1.1783 - val_accuracy: 0.5896\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3420 - accuracy: 0.8370 - val_loss: 1.1710 - val_accuracy: 0.5913\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.3364 - accuracy: 0.8348 - val_loss: 1.1858 - val_accuracy: 0.5846\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3403 - accuracy: 0.8384 - val_loss: 1.2281 - val_accuracy: 0.6092\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3436 - accuracy: 0.8346 - val_loss: 1.1929 - val_accuracy: 0.5942\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3387 - accuracy: 0.8391 - val_loss: 1.1952 - val_accuracy: 0.5908\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3375 - accuracy: 0.8373 - val_loss: 1.2107 - val_accuracy: 0.6012\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3409 - accuracy: 0.8389 - val_loss: 1.1684 - val_accuracy: 0.5946\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3386 - accuracy: 0.8384 - val_loss: 1.2315 - val_accuracy: 0.6079\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3368 - accuracy: 0.8420 - val_loss: 1.2175 - val_accuracy: 0.6021\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3376 - accuracy: 0.8321 - val_loss: 1.2025 - val_accuracy: 0.5946\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3321 - accuracy: 0.8375 - val_loss: 1.1952 - val_accuracy: 0.5825\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3251 - accuracy: 0.8486 - val_loss: 1.2227 - val_accuracy: 0.5933\n",
      "Epoch 270/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3311 - accuracy: 0.8413 - val_loss: 1.2221 - val_accuracy: 0.5950\n",
      "Epoch 271/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3260 - accuracy: 0.8445 - val_loss: 1.2499 - val_accuracy: 0.6133\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3310 - accuracy: 0.8402 - val_loss: 1.2221 - val_accuracy: 0.5925\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3218 - accuracy: 0.8516 - val_loss: 1.2725 - val_accuracy: 0.6108\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3307 - accuracy: 0.8411 - val_loss: 1.2344 - val_accuracy: 0.5983\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3166 - accuracy: 0.8468 - val_loss: 1.2395 - val_accuracy: 0.5971\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3223 - accuracy: 0.8461 - val_loss: 1.2725 - val_accuracy: 0.5917\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3257 - accuracy: 0.8473 - val_loss: 1.2307 - val_accuracy: 0.6029\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3286 - accuracy: 0.8455 - val_loss: 1.2471 - val_accuracy: 0.6012\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3279 - accuracy: 0.8438 - val_loss: 1.2694 - val_accuracy: 0.5871\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3163 - accuracy: 0.8468 - val_loss: 1.2519 - val_accuracy: 0.6025\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3174 - accuracy: 0.8473 - val_loss: 1.2989 - val_accuracy: 0.6058\n",
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3228 - accuracy: 0.8427 - val_loss: 1.3169 - val_accuracy: 0.6192\n",
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3241 - accuracy: 0.8489 - val_loss: 1.2626 - val_accuracy: 0.6037\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3199 - accuracy: 0.8493 - val_loss: 1.2765 - val_accuracy: 0.6021\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3206 - accuracy: 0.8470 - val_loss: 1.2805 - val_accuracy: 0.6004\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3366 - accuracy: 0.8421 - val_loss: 1.2653 - val_accuracy: 0.6125\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3460 - accuracy: 0.8325 - val_loss: 1.2758 - val_accuracy: 0.5892\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3293 - accuracy: 0.8445 - val_loss: 1.2807 - val_accuracy: 0.5921\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3144 - accuracy: 0.8545 - val_loss: 1.2706 - val_accuracy: 0.5925\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3091 - accuracy: 0.8571 - val_loss: 1.2995 - val_accuracy: 0.6112\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3179 - accuracy: 0.8512 - val_loss: 1.2914 - val_accuracy: 0.6092\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3085 - accuracy: 0.8586 - val_loss: 1.2881 - val_accuracy: 0.5925\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3094 - accuracy: 0.8550 - val_loss: 1.3041 - val_accuracy: 0.5929\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3052 - accuracy: 0.8573 - val_loss: 1.2605 - val_accuracy: 0.6121\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2996 - accuracy: 0.8645 - val_loss: 1.2855 - val_accuracy: 0.5908\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3024 - accuracy: 0.8607 - val_loss: 1.2881 - val_accuracy: 0.5904\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3016 - accuracy: 0.8609 - val_loss: 1.2886 - val_accuracy: 0.6000\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3051 - accuracy: 0.8607 - val_loss: 1.2984 - val_accuracy: 0.5850\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2988 - accuracy: 0.8607 - val_loss: 1.3381 - val_accuracy: 0.5917\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3055 - accuracy: 0.8573 - val_loss: 1.3526 - val_accuracy: 0.6075\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3052 - accuracy: 0.8552 - val_loss: 1.2981 - val_accuracy: 0.6017\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3001 - accuracy: 0.8627 - val_loss: 1.3431 - val_accuracy: 0.6058\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3002 - accuracy: 0.8621 - val_loss: 1.3245 - val_accuracy: 0.6029\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2975 - accuracy: 0.8618 - val_loss: 1.3186 - val_accuracy: 0.5917\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3073 - accuracy: 0.8611 - val_loss: 1.3526 - val_accuracy: 0.5850\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2949 - accuracy: 0.8645 - val_loss: 1.3294 - val_accuracy: 0.6008\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.2955 - accuracy: 0.8668 - val_loss: 1.3418 - val_accuracy: 0.6054\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2979 - accuracy: 0.8639 - val_loss: 1.3794 - val_accuracy: 0.6012\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3040 - accuracy: 0.8582 - val_loss: 1.3166 - val_accuracy: 0.6050\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3026 - accuracy: 0.8554 - val_loss: 1.3320 - val_accuracy: 0.5925\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2915 - accuracy: 0.8668 - val_loss: 1.3519 - val_accuracy: 0.6012\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2918 - accuracy: 0.8675 - val_loss: 1.3433 - val_accuracy: 0.5825\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2842 - accuracy: 0.8705 - val_loss: 1.3767 - val_accuracy: 0.5946\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2887 - accuracy: 0.8652 - val_loss: 1.3547 - val_accuracy: 0.5992\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2862 - accuracy: 0.8673 - val_loss: 1.3922 - val_accuracy: 0.5950\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2921 - accuracy: 0.8639 - val_loss: 1.3370 - val_accuracy: 0.6017\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2836 - accuracy: 0.8716 - val_loss: 1.3785 - val_accuracy: 0.5983\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2795 - accuracy: 0.8759 - val_loss: 1.4019 - val_accuracy: 0.6042\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2879 - accuracy: 0.8684 - val_loss: 1.3924 - val_accuracy: 0.6104\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2866 - accuracy: 0.8691 - val_loss: 1.4110 - val_accuracy: 0.6021\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2846 - accuracy: 0.8691 - val_loss: 1.3937 - val_accuracy: 0.6037\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2799 - accuracy: 0.8721 - val_loss: 1.4065 - val_accuracy: 0.6075\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2864 - accuracy: 0.8654 - val_loss: 1.4249 - val_accuracy: 0.5929\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2969 - accuracy: 0.8682 - val_loss: 1.3997 - val_accuracy: 0.5975\n",
      "Epoch 325/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2980 - accuracy: 0.8661 - val_loss: 1.3621 - val_accuracy: 0.6004\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2977 - accuracy: 0.8673 - val_loss: 1.4366 - val_accuracy: 0.5917\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2971 - accuracy: 0.8623 - val_loss: 1.4126 - val_accuracy: 0.5958\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2815 - accuracy: 0.8716 - val_loss: 1.3848 - val_accuracy: 0.5996\n",
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2781 - accuracy: 0.8750 - val_loss: 1.3796 - val_accuracy: 0.5925\n",
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2727 - accuracy: 0.8795 - val_loss: 1.3948 - val_accuracy: 0.5996\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2768 - accuracy: 0.8729 - val_loss: 1.4137 - val_accuracy: 0.5842\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2706 - accuracy: 0.8782 - val_loss: 1.4287 - val_accuracy: 0.6037\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2682 - accuracy: 0.8782 - val_loss: 1.4542 - val_accuracy: 0.6054\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2720 - accuracy: 0.8796 - val_loss: 1.3933 - val_accuracy: 0.5983\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2715 - accuracy: 0.8745 - val_loss: 1.4167 - val_accuracy: 0.6096\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2648 - accuracy: 0.8802 - val_loss: 1.4374 - val_accuracy: 0.6017\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2751 - accuracy: 0.8743 - val_loss: 1.4073 - val_accuracy: 0.6037\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2768 - accuracy: 0.8755 - val_loss: 1.4270 - val_accuracy: 0.5925\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2748 - accuracy: 0.8746 - val_loss: 1.4315 - val_accuracy: 0.5958\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2726 - accuracy: 0.8788 - val_loss: 1.4601 - val_accuracy: 0.5838\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2884 - accuracy: 0.8636 - val_loss: 1.4450 - val_accuracy: 0.6071\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2700 - accuracy: 0.8754 - val_loss: 1.4596 - val_accuracy: 0.5875\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2702 - accuracy: 0.8795 - val_loss: 1.4603 - val_accuracy: 0.6054\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2746 - accuracy: 0.8748 - val_loss: 1.4629 - val_accuracy: 0.6042\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2690 - accuracy: 0.8830 - val_loss: 1.4751 - val_accuracy: 0.6079\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2642 - accuracy: 0.8852 - val_loss: 1.4657 - val_accuracy: 0.6154\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2658 - accuracy: 0.8836 - val_loss: 1.4701 - val_accuracy: 0.6025\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2627 - accuracy: 0.8813 - val_loss: 1.4465 - val_accuracy: 0.5996\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2663 - accuracy: 0.8784 - val_loss: 1.4813 - val_accuracy: 0.5908\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2631 - accuracy: 0.8802 - val_loss: 1.4808 - val_accuracy: 0.6075\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2563 - accuracy: 0.8839 - val_loss: 1.4944 - val_accuracy: 0.5950\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2638 - accuracy: 0.8784 - val_loss: 1.4712 - val_accuracy: 0.6008\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2558 - accuracy: 0.8852 - val_loss: 1.4891 - val_accuracy: 0.6042\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2572 - accuracy: 0.8829 - val_loss: 1.5091 - val_accuracy: 0.6037\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2538 - accuracy: 0.8868 - val_loss: 1.5287 - val_accuracy: 0.6050\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2522 - accuracy: 0.8909 - val_loss: 1.4859 - val_accuracy: 0.5904\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2575 - accuracy: 0.8782 - val_loss: 1.4704 - val_accuracy: 0.5908\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2587 - accuracy: 0.8859 - val_loss: 1.4685 - val_accuracy: 0.6025\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2503 - accuracy: 0.8855 - val_loss: 1.4824 - val_accuracy: 0.5987\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2481 - accuracy: 0.8902 - val_loss: 1.5105 - val_accuracy: 0.5888\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2522 - accuracy: 0.8866 - val_loss: 1.5105 - val_accuracy: 0.5992\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2488 - accuracy: 0.8923 - val_loss: 1.5216 - val_accuracy: 0.5983\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2571 - accuracy: 0.8845 - val_loss: 1.5431 - val_accuracy: 0.5908\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.2586 - accuracy: 0.8859 - val_loss: 1.5242 - val_accuracy: 0.6012\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2553 - accuracy: 0.8852 - val_loss: 1.5359 - val_accuracy: 0.6029\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2594 - accuracy: 0.8850 - val_loss: 1.5538 - val_accuracy: 0.6104\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2678 - accuracy: 0.8809 - val_loss: 1.5161 - val_accuracy: 0.5938\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2540 - accuracy: 0.8845 - val_loss: 1.5082 - val_accuracy: 0.6000\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2453 - accuracy: 0.8889 - val_loss: 1.5769 - val_accuracy: 0.6000\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2474 - accuracy: 0.8914 - val_loss: 1.5848 - val_accuracy: 0.6008\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2421 - accuracy: 0.8889 - val_loss: 1.5527 - val_accuracy: 0.5896\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2426 - accuracy: 0.8934 - val_loss: 1.5710 - val_accuracy: 0.6042\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2435 - accuracy: 0.8934 - val_loss: 1.5624 - val_accuracy: 0.5992\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2409 - accuracy: 0.8938 - val_loss: 1.5611 - val_accuracy: 0.6029\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2404 - accuracy: 0.8934 - val_loss: 1.5507 - val_accuracy: 0.5929\n",
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2509 - accuracy: 0.8886 - val_loss: 1.5971 - val_accuracy: 0.5992\n",
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2488 - accuracy: 0.8898 - val_loss: 1.6221 - val_accuracy: 0.5992\n",
      "Epoch 378/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2459 - accuracy: 0.8943 - val_loss: 1.5752 - val_accuracy: 0.5971\n",
      "Epoch 379/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2605 - accuracy: 0.8913 - val_loss: 1.5995 - val_accuracy: 0.6017\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2671 - accuracy: 0.8775 - val_loss: 1.5472 - val_accuracy: 0.5875\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2704 - accuracy: 0.8802 - val_loss: 1.5842 - val_accuracy: 0.5854\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2624 - accuracy: 0.8841 - val_loss: 1.5542 - val_accuracy: 0.6021\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2398 - accuracy: 0.8893 - val_loss: 1.6563 - val_accuracy: 0.6008\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2497 - accuracy: 0.8889 - val_loss: 1.5862 - val_accuracy: 0.5992\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2501 - accuracy: 0.8895 - val_loss: 1.6215 - val_accuracy: 0.5721\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2464 - accuracy: 0.8907 - val_loss: 1.5841 - val_accuracy: 0.5979\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2410 - accuracy: 0.8907 - val_loss: 1.5873 - val_accuracy: 0.6037\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2331 - accuracy: 0.8957 - val_loss: 1.6072 - val_accuracy: 0.6012\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2329 - accuracy: 0.8977 - val_loss: 1.6372 - val_accuracy: 0.6108\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2318 - accuracy: 0.9020 - val_loss: 1.5990 - val_accuracy: 0.5979\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2323 - accuracy: 0.8945 - val_loss: 1.6163 - val_accuracy: 0.6042\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2282 - accuracy: 0.9014 - val_loss: 1.6211 - val_accuracy: 0.6008\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2380 - accuracy: 0.8929 - val_loss: 1.6231 - val_accuracy: 0.5938\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2318 - accuracy: 0.8993 - val_loss: 1.6533 - val_accuracy: 0.6087\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2321 - accuracy: 0.8964 - val_loss: 1.6402 - val_accuracy: 0.6087\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2230 - accuracy: 0.9002 - val_loss: 1.6309 - val_accuracy: 0.6042\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2266 - accuracy: 0.9043 - val_loss: 1.6166 - val_accuracy: 0.5975\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2324 - accuracy: 0.8959 - val_loss: 1.6139 - val_accuracy: 0.6083\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2314 - accuracy: 0.8982 - val_loss: 1.6610 - val_accuracy: 0.5825\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2303 - accuracy: 0.8986 - val_loss: 1.6369 - val_accuracy: 0.5975\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2277 - accuracy: 0.9030 - val_loss: 1.6490 - val_accuracy: 0.5929\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2245 - accuracy: 0.9054 - val_loss: 1.6363 - val_accuracy: 0.5892\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2267 - accuracy: 0.9052 - val_loss: 1.6439 - val_accuracy: 0.6075\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.2220 - accuracy: 0.9038 - val_loss: 1.6861 - val_accuracy: 0.6079\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2247 - accuracy: 0.9034 - val_loss: 1.6615 - val_accuracy: 0.5954\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2248 - accuracy: 0.8982 - val_loss: 1.6923 - val_accuracy: 0.5854\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2285 - accuracy: 0.9009 - val_loss: 1.6829 - val_accuracy: 0.5800\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2272 - accuracy: 0.9041 - val_loss: 1.6751 - val_accuracy: 0.5942\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2245 - accuracy: 0.9066 - val_loss: 1.6408 - val_accuracy: 0.6029\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2221 - accuracy: 0.9038 - val_loss: 1.6686 - val_accuracy: 0.6017\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2269 - accuracy: 0.9045 - val_loss: 1.6528 - val_accuracy: 0.5946\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2411 - accuracy: 0.8993 - val_loss: 1.6711 - val_accuracy: 0.5962\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2466 - accuracy: 0.8886 - val_loss: 1.6928 - val_accuracy: 0.5929\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2682 - accuracy: 0.8800 - val_loss: 1.7200 - val_accuracy: 0.5854\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2425 - accuracy: 0.8961 - val_loss: 1.6975 - val_accuracy: 0.6025\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.2345 - accuracy: 0.8968 - val_loss: 1.6734 - val_accuracy: 0.6042\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.2230 - accuracy: 0.9061 - val_loss: 1.6510 - val_accuracy: 0.6058\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.2174 - accuracy: 0.9093 - val_loss: 1.7074 - val_accuracy: 0.5900\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.2137 - accuracy: 0.9086 - val_loss: 1.6554 - val_accuracy: 0.5975\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.2109 - accuracy: 0.9100 - val_loss: 1.6555 - val_accuracy: 0.5871\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2121 - accuracy: 0.9107 - val_loss: 1.6755 - val_accuracy: 0.5913\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2064 - accuracy: 0.9148 - val_loss: 1.6688 - val_accuracy: 0.5900\n",
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2113 - accuracy: 0.9146 - val_loss: 1.6960 - val_accuracy: 0.5883\n",
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2155 - accuracy: 0.9082 - val_loss: 1.7281 - val_accuracy: 0.5992\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2095 - accuracy: 0.9130 - val_loss: 1.7099 - val_accuracy: 0.5821\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2129 - accuracy: 0.9084 - val_loss: 1.7359 - val_accuracy: 0.5967\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2089 - accuracy: 0.9100 - val_loss: 1.7168 - val_accuracy: 0.5992\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2121 - accuracy: 0.9082 - val_loss: 1.7059 - val_accuracy: 0.5950\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2096 - accuracy: 0.9107 - val_loss: 1.6825 - val_accuracy: 0.5904\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2092 - accuracy: 0.9120 - val_loss: 1.7250 - val_accuracy: 0.6062\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2094 - accuracy: 0.9095 - val_loss: 1.7216 - val_accuracy: 0.5921\n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2020 - accuracy: 0.9179 - val_loss: 1.7168 - val_accuracy: 0.6017\n",
      "Epoch 433/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2060 - accuracy: 0.9086 - val_loss: 1.7330 - val_accuracy: 0.5896\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2080 - accuracy: 0.9107 - val_loss: 1.7497 - val_accuracy: 0.6046\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2071 - accuracy: 0.9155 - val_loss: 1.7525 - val_accuracy: 0.6108\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2126 - accuracy: 0.9054 - val_loss: 1.7771 - val_accuracy: 0.6037\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2129 - accuracy: 0.9075 - val_loss: 1.7081 - val_accuracy: 0.5925\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2044 - accuracy: 0.9150 - val_loss: 1.7218 - val_accuracy: 0.5925\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2001 - accuracy: 0.9198 - val_loss: 1.7614 - val_accuracy: 0.5938\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1996 - accuracy: 0.9155 - val_loss: 1.7731 - val_accuracy: 0.5908\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2031 - accuracy: 0.9171 - val_loss: 1.7307 - val_accuracy: 0.5917\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.2114 - accuracy: 0.9105 - val_loss: 1.7575 - val_accuracy: 0.5962\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1994 - accuracy: 0.9170 - val_loss: 1.7324 - val_accuracy: 0.5825\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2005 - accuracy: 0.9164 - val_loss: 1.7565 - val_accuracy: 0.5962\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2040 - accuracy: 0.9109 - val_loss: 1.7904 - val_accuracy: 0.5817\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2064 - accuracy: 0.9143 - val_loss: 1.7486 - val_accuracy: 0.5888\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2057 - accuracy: 0.9148 - val_loss: 1.7638 - val_accuracy: 0.5992\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2036 - accuracy: 0.9159 - val_loss: 1.7602 - val_accuracy: 0.5721\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2087 - accuracy: 0.9136 - val_loss: 1.7562 - val_accuracy: 0.5917\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1988 - accuracy: 0.9173 - val_loss: 1.7788 - val_accuracy: 0.5917\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1985 - accuracy: 0.9189 - val_loss: 1.7779 - val_accuracy: 0.5892\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2154 - accuracy: 0.9112 - val_loss: 1.8244 - val_accuracy: 0.5975\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2474 - accuracy: 0.8950 - val_loss: 1.8365 - val_accuracy: 0.6075\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2619 - accuracy: 0.8868 - val_loss: 1.8133 - val_accuracy: 0.5996\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2216 - accuracy: 0.9082 - val_loss: 1.8165 - val_accuracy: 0.6004\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2076 - accuracy: 0.9114 - val_loss: 1.7611 - val_accuracy: 0.5929\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2009 - accuracy: 0.9112 - val_loss: 1.7988 - val_accuracy: 0.5892\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1960 - accuracy: 0.9157 - val_loss: 1.8088 - val_accuracy: 0.6000\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1888 - accuracy: 0.9214 - val_loss: 1.7930 - val_accuracy: 0.5929\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1998 - accuracy: 0.9184 - val_loss: 1.8115 - val_accuracy: 0.6017\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2033 - accuracy: 0.9159 - val_loss: 1.8152 - val_accuracy: 0.5938\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1899 - accuracy: 0.9196 - val_loss: 1.8179 - val_accuracy: 0.5967\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1886 - accuracy: 0.9248 - val_loss: 1.8280 - val_accuracy: 0.6025\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1842 - accuracy: 0.9264 - val_loss: 1.8345 - val_accuracy: 0.5875\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1887 - accuracy: 0.9261 - val_loss: 1.8132 - val_accuracy: 0.5950\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1788 - accuracy: 0.9266 - val_loss: 1.8510 - val_accuracy: 0.5904\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1832 - accuracy: 0.9261 - val_loss: 1.8352 - val_accuracy: 0.6046\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1854 - accuracy: 0.9264 - val_loss: 1.8347 - val_accuracy: 0.5925\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1864 - accuracy: 0.9236 - val_loss: 1.8250 - val_accuracy: 0.5933\n",
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1819 - accuracy: 0.9291 - val_loss: 1.8615 - val_accuracy: 0.5900\n",
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1874 - accuracy: 0.9239 - val_loss: 1.8523 - val_accuracy: 0.5962\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1894 - accuracy: 0.9187 - val_loss: 1.8435 - val_accuracy: 0.5925\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1854 - accuracy: 0.9245 - val_loss: 1.8585 - val_accuracy: 0.6000\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1828 - accuracy: 0.9286 - val_loss: 1.8397 - val_accuracy: 0.5971\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1808 - accuracy: 0.9271 - val_loss: 1.8682 - val_accuracy: 0.5996\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1803 - accuracy: 0.9270 - val_loss: 1.9014 - val_accuracy: 0.5967\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1928 - accuracy: 0.9236 - val_loss: 1.8850 - val_accuracy: 0.5917\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2028 - accuracy: 0.9170 - val_loss: 1.9363 - val_accuracy: 0.6050\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2221 - accuracy: 0.9080 - val_loss: 1.8194 - val_accuracy: 0.5962\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2142 - accuracy: 0.9123 - val_loss: 1.9275 - val_accuracy: 0.5971\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2202 - accuracy: 0.9089 - val_loss: 1.9037 - val_accuracy: 0.5938\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2104 - accuracy: 0.9139 - val_loss: 1.8349 - val_accuracy: 0.5817\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2081 - accuracy: 0.9220 - val_loss: 1.8681 - val_accuracy: 0.5767\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2127 - accuracy: 0.9136 - val_loss: 1.8378 - val_accuracy: 0.5908\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2133 - accuracy: 0.9139 - val_loss: 1.8511 - val_accuracy: 0.6029\n",
      "Epoch 486/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1961 - accuracy: 0.9212 - val_loss: 1.9004 - val_accuracy: 0.5954\n",
      "Epoch 487/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1909 - accuracy: 0.9202 - val_loss: 1.8783 - val_accuracy: 0.5996\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1875 - accuracy: 0.9221 - val_loss: 1.8900 - val_accuracy: 0.5871\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1849 - accuracy: 0.9230 - val_loss: 1.8602 - val_accuracy: 0.5879\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1798 - accuracy: 0.9246 - val_loss: 1.8699 - val_accuracy: 0.5917\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1741 - accuracy: 0.9314 - val_loss: 1.9322 - val_accuracy: 0.6100\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1771 - accuracy: 0.9300 - val_loss: 1.9147 - val_accuracy: 0.6129\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1769 - accuracy: 0.9293 - val_loss: 1.8872 - val_accuracy: 0.5933\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1726 - accuracy: 0.9300 - val_loss: 1.8857 - val_accuracy: 0.6029\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1716 - accuracy: 0.9323 - val_loss: 1.8819 - val_accuracy: 0.5842\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1760 - accuracy: 0.9325 - val_loss: 1.9022 - val_accuracy: 0.5942\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1705 - accuracy: 0.9343 - val_loss: 1.8838 - val_accuracy: 0.5933\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1700 - accuracy: 0.9309 - val_loss: 1.8711 - val_accuracy: 0.5975\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1717 - accuracy: 0.9293 - val_loss: 1.9360 - val_accuracy: 0.5983\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1695 - accuracy: 0.9343 - val_loss: 1.9312 - val_accuracy: 0.5975\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1725 - accuracy: 0.9325 - val_loss: 1.8905 - val_accuracy: 0.5896\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1720 - accuracy: 0.9316 - val_loss: 1.9066 - val_accuracy: 0.5979\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1707 - accuracy: 0.9279 - val_loss: 1.9136 - val_accuracy: 0.5883\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1754 - accuracy: 0.9286 - val_loss: 1.9261 - val_accuracy: 0.5925\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1686 - accuracy: 0.9345 - val_loss: 1.9233 - val_accuracy: 0.6000\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1638 - accuracy: 0.9354 - val_loss: 1.9119 - val_accuracy: 0.5942\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1705 - accuracy: 0.9316 - val_loss: 1.9356 - val_accuracy: 0.5879\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1641 - accuracy: 0.9339 - val_loss: 1.9326 - val_accuracy: 0.5821\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1675 - accuracy: 0.9336 - val_loss: 1.9214 - val_accuracy: 0.5913\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1714 - accuracy: 0.9318 - val_loss: 1.9794 - val_accuracy: 0.6004\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1637 - accuracy: 0.9350 - val_loss: 2.0052 - val_accuracy: 0.5996\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1705 - accuracy: 0.9325 - val_loss: 1.9428 - val_accuracy: 0.5892\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1711 - accuracy: 0.9323 - val_loss: 1.9926 - val_accuracy: 0.6079\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1795 - accuracy: 0.9234 - val_loss: 1.9809 - val_accuracy: 0.5942\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1760 - accuracy: 0.9323 - val_loss: 1.9865 - val_accuracy: 0.5846\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1857 - accuracy: 0.9230 - val_loss: 1.9721 - val_accuracy: 0.6054\n",
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1731 - accuracy: 0.9312 - val_loss: 1.9786 - val_accuracy: 0.6012\n",
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1781 - accuracy: 0.9312 - val_loss: 1.9800 - val_accuracy: 0.6008\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1714 - accuracy: 0.9305 - val_loss: 2.0128 - val_accuracy: 0.6004\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1689 - accuracy: 0.9321 - val_loss: 2.0025 - val_accuracy: 0.5867\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1667 - accuracy: 0.9330 - val_loss: 2.0054 - val_accuracy: 0.5971\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1620 - accuracy: 0.9350 - val_loss: 1.9923 - val_accuracy: 0.5825\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1631 - accuracy: 0.9359 - val_loss: 1.9699 - val_accuracy: 0.5933\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1599 - accuracy: 0.9348 - val_loss: 1.9709 - val_accuracy: 0.5896\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1610 - accuracy: 0.9361 - val_loss: 1.9986 - val_accuracy: 0.5804\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1595 - accuracy: 0.9377 - val_loss: 1.9803 - val_accuracy: 0.5983\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1647 - accuracy: 0.9332 - val_loss: 2.0003 - val_accuracy: 0.5788\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1681 - accuracy: 0.9337 - val_loss: 2.0471 - val_accuracy: 0.6008\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1603 - accuracy: 0.9345 - val_loss: 2.0538 - val_accuracy: 0.5813\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1649 - accuracy: 0.9364 - val_loss: 2.0460 - val_accuracy: 0.5896\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1716 - accuracy: 0.9321 - val_loss: 1.9526 - val_accuracy: 0.5871\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1576 - accuracy: 0.9386 - val_loss: 1.9700 - val_accuracy: 0.5871\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1676 - accuracy: 0.9355 - val_loss: 2.0712 - val_accuracy: 0.5879\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1753 - accuracy: 0.9314 - val_loss: 2.0039 - val_accuracy: 0.5867\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1971 - accuracy: 0.9204 - val_loss: 2.0302 - val_accuracy: 0.5967\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1713 - accuracy: 0.9337 - val_loss: 2.0288 - val_accuracy: 0.5871\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1825 - accuracy: 0.9279 - val_loss: 2.0782 - val_accuracy: 0.5908\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1893 - accuracy: 0.9280 - val_loss: 2.0255 - val_accuracy: 0.5896\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1941 - accuracy: 0.9180 - val_loss: 2.0512 - val_accuracy: 0.5871\n",
      "Epoch 540/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1919 - accuracy: 0.9196 - val_loss: 2.0205 - val_accuracy: 0.5858\n",
      "Epoch 541/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1661 - accuracy: 0.9336 - val_loss: 2.0570 - val_accuracy: 0.5946\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1576 - accuracy: 0.9423 - val_loss: 2.0440 - val_accuracy: 0.5933\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1679 - accuracy: 0.9359 - val_loss: 2.0463 - val_accuracy: 0.5983\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1579 - accuracy: 0.9438 - val_loss: 2.0495 - val_accuracy: 0.5954\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1608 - accuracy: 0.9391 - val_loss: 2.0091 - val_accuracy: 0.5854\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1517 - accuracy: 0.9448 - val_loss: 2.0793 - val_accuracy: 0.6004\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1539 - accuracy: 0.9430 - val_loss: 2.0996 - val_accuracy: 0.5942\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1530 - accuracy: 0.9446 - val_loss: 2.0811 - val_accuracy: 0.5875\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1493 - accuracy: 0.9421 - val_loss: 2.0672 - val_accuracy: 0.5879\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1547 - accuracy: 0.9362 - val_loss: 2.0945 - val_accuracy: 0.5996\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1520 - accuracy: 0.9421 - val_loss: 2.0709 - val_accuracy: 0.5967\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1495 - accuracy: 0.9452 - val_loss: 2.0665 - val_accuracy: 0.5829\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1477 - accuracy: 0.9439 - val_loss: 2.0723 - val_accuracy: 0.5829\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1507 - accuracy: 0.9405 - val_loss: 2.0660 - val_accuracy: 0.5975\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1505 - accuracy: 0.9430 - val_loss: 2.1022 - val_accuracy: 0.5871\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1466 - accuracy: 0.9446 - val_loss: 2.1067 - val_accuracy: 0.5954\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1453 - accuracy: 0.9452 - val_loss: 2.0853 - val_accuracy: 0.5954\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1480 - accuracy: 0.9441 - val_loss: 2.0720 - val_accuracy: 0.5883\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1429 - accuracy: 0.9463 - val_loss: 2.1351 - val_accuracy: 0.5992\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1483 - accuracy: 0.9432 - val_loss: 2.1350 - val_accuracy: 0.6104\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1513 - accuracy: 0.9375 - val_loss: 2.1197 - val_accuracy: 0.5858\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1458 - accuracy: 0.9430 - val_loss: 2.1228 - val_accuracy: 0.5917\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1495 - accuracy: 0.9404 - val_loss: 2.0739 - val_accuracy: 0.5942\n",
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1431 - accuracy: 0.9464 - val_loss: 2.0932 - val_accuracy: 0.5892\n",
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1518 - accuracy: 0.9395 - val_loss: 2.1737 - val_accuracy: 0.6017\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1609 - accuracy: 0.9386 - val_loss: 2.1331 - val_accuracy: 0.5917\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1517 - accuracy: 0.9420 - val_loss: 2.1431 - val_accuracy: 0.5913\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1429 - accuracy: 0.9445 - val_loss: 2.1093 - val_accuracy: 0.5775\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1466 - accuracy: 0.9420 - val_loss: 2.1313 - val_accuracy: 0.5842\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1447 - accuracy: 0.9450 - val_loss: 2.1423 - val_accuracy: 0.5854\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1401 - accuracy: 0.9486 - val_loss: 2.1739 - val_accuracy: 0.5946\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1433 - accuracy: 0.9448 - val_loss: 2.1497 - val_accuracy: 0.5888\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1460 - accuracy: 0.9446 - val_loss: 2.1633 - val_accuracy: 0.5900\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1452 - accuracy: 0.9423 - val_loss: 2.1273 - val_accuracy: 0.5875\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1436 - accuracy: 0.9439 - val_loss: 2.1476 - val_accuracy: 0.5838\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1432 - accuracy: 0.9418 - val_loss: 2.1239 - val_accuracy: 0.5933\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1405 - accuracy: 0.9482 - val_loss: 2.1232 - val_accuracy: 0.5892\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1413 - accuracy: 0.9461 - val_loss: 2.1617 - val_accuracy: 0.5871\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1379 - accuracy: 0.9468 - val_loss: 2.1669 - val_accuracy: 0.5950\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1331 - accuracy: 0.9525 - val_loss: 2.1356 - val_accuracy: 0.5867\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1386 - accuracy: 0.9471 - val_loss: 2.1888 - val_accuracy: 0.5983\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1455 - accuracy: 0.9445 - val_loss: 2.1312 - val_accuracy: 0.5833\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1598 - accuracy: 0.9362 - val_loss: 2.1809 - val_accuracy: 0.5888\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2160 - accuracy: 0.9187 - val_loss: 2.1779 - val_accuracy: 0.5721\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1955 - accuracy: 0.9171 - val_loss: 2.1671 - val_accuracy: 0.5879\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1525 - accuracy: 0.9414 - val_loss: 2.1174 - val_accuracy: 0.5896\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1457 - accuracy: 0.9418 - val_loss: 2.1620 - val_accuracy: 0.5925\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1391 - accuracy: 0.9491 - val_loss: 2.1750 - val_accuracy: 0.5938\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1319 - accuracy: 0.9525 - val_loss: 2.1627 - val_accuracy: 0.5950\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1395 - accuracy: 0.9450 - val_loss: 2.1025 - val_accuracy: 0.5821\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1441 - accuracy: 0.9423 - val_loss: 2.2022 - val_accuracy: 0.6000\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1298 - accuracy: 0.9518 - val_loss: 2.1813 - val_accuracy: 0.5867\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1289 - accuracy: 0.9541 - val_loss: 2.1941 - val_accuracy: 0.5983\n",
      "Epoch 594/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1285 - accuracy: 0.9541 - val_loss: 2.1851 - val_accuracy: 0.6004\n",
      "Epoch 595/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1415 - accuracy: 0.9450 - val_loss: 2.1685 - val_accuracy: 0.5883\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1359 - accuracy: 0.9466 - val_loss: 2.2002 - val_accuracy: 0.5971\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1274 - accuracy: 0.9550 - val_loss: 2.1687 - val_accuracy: 0.5896\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1274 - accuracy: 0.9543 - val_loss: 2.2175 - val_accuracy: 0.5975\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1383 - accuracy: 0.9463 - val_loss: 2.2065 - val_accuracy: 0.5962\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1249 - accuracy: 0.9564 - val_loss: 2.2403 - val_accuracy: 0.5950\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1257 - accuracy: 0.9539 - val_loss: 2.2836 - val_accuracy: 0.5938\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1402 - accuracy: 0.9463 - val_loss: 2.1738 - val_accuracy: 0.5800\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1794 - accuracy: 0.9311 - val_loss: 2.1543 - val_accuracy: 0.5933\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2021 - accuracy: 0.9179 - val_loss: 2.1609 - val_accuracy: 0.5842\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1683 - accuracy: 0.9330 - val_loss: 2.2436 - val_accuracy: 0.5808\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1439 - accuracy: 0.9446 - val_loss: 2.2117 - val_accuracy: 0.5896\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1360 - accuracy: 0.9493 - val_loss: 2.2922 - val_accuracy: 0.5975\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1389 - accuracy: 0.9496 - val_loss: 2.2156 - val_accuracy: 0.5933\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1351 - accuracy: 0.9507 - val_loss: 2.2667 - val_accuracy: 0.6050\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1341 - accuracy: 0.9495 - val_loss: 2.2270 - val_accuracy: 0.5921\n",
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1218 - accuracy: 0.9571 - val_loss: 2.2493 - val_accuracy: 0.6017\n",
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1238 - accuracy: 0.9552 - val_loss: 2.2649 - val_accuracy: 0.5888\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1334 - accuracy: 0.9530 - val_loss: 2.2509 - val_accuracy: 0.5846\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1440 - accuracy: 0.9455 - val_loss: 2.2561 - val_accuracy: 0.5992\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1346 - accuracy: 0.9495 - val_loss: 2.3140 - val_accuracy: 0.5758\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1417 - accuracy: 0.9471 - val_loss: 2.2539 - val_accuracy: 0.5783\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1379 - accuracy: 0.9471 - val_loss: 2.2751 - val_accuracy: 0.5983\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1266 - accuracy: 0.9543 - val_loss: 2.2528 - val_accuracy: 0.5908\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1199 - accuracy: 0.9579 - val_loss: 2.2396 - val_accuracy: 0.5858\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1231 - accuracy: 0.9566 - val_loss: 2.2795 - val_accuracy: 0.5996\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1284 - accuracy: 0.9568 - val_loss: 2.2629 - val_accuracy: 0.5879\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1269 - accuracy: 0.9513 - val_loss: 2.2701 - val_accuracy: 0.5929\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1221 - accuracy: 0.9555 - val_loss: 2.2743 - val_accuracy: 0.5913\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1128 - accuracy: 0.9613 - val_loss: 2.3098 - val_accuracy: 0.5846\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1195 - accuracy: 0.9577 - val_loss: 2.2902 - val_accuracy: 0.5892\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1194 - accuracy: 0.9573 - val_loss: 2.2956 - val_accuracy: 0.5854\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1375 - accuracy: 0.9448 - val_loss: 2.2681 - val_accuracy: 0.5888\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1521 - accuracy: 0.9400 - val_loss: 2.3411 - val_accuracy: 0.5817\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1939 - accuracy: 0.9268 - val_loss: 2.2931 - val_accuracy: 0.5925\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1675 - accuracy: 0.9396 - val_loss: 2.3162 - val_accuracy: 0.6067\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1475 - accuracy: 0.9464 - val_loss: 2.2534 - val_accuracy: 0.5962\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1376 - accuracy: 0.9516 - val_loss: 2.2564 - val_accuracy: 0.5925\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1428 - accuracy: 0.9498 - val_loss: 2.2521 - val_accuracy: 0.5883\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1262 - accuracy: 0.9561 - val_loss: 2.3177 - val_accuracy: 0.5921\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1200 - accuracy: 0.9548 - val_loss: 2.2963 - val_accuracy: 0.5883\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1201 - accuracy: 0.9566 - val_loss: 2.3568 - val_accuracy: 0.5979\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1236 - accuracy: 0.9538 - val_loss: 2.2873 - val_accuracy: 0.5742\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1179 - accuracy: 0.9609 - val_loss: 2.3002 - val_accuracy: 0.5950\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1223 - accuracy: 0.9523 - val_loss: 2.3005 - val_accuracy: 0.5904\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1140 - accuracy: 0.9611 - val_loss: 2.3337 - val_accuracy: 0.5896\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1236 - accuracy: 0.9539 - val_loss: 2.3011 - val_accuracy: 0.5738\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1224 - accuracy: 0.9541 - val_loss: 2.3252 - val_accuracy: 0.5883\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1217 - accuracy: 0.9545 - val_loss: 2.3773 - val_accuracy: 0.5933\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1208 - accuracy: 0.9570 - val_loss: 2.3065 - val_accuracy: 0.5858\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1104 - accuracy: 0.9616 - val_loss: 2.3008 - val_accuracy: 0.5929\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1133 - accuracy: 0.9593 - val_loss: 2.3852 - val_accuracy: 0.5917\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1139 - accuracy: 0.9595 - val_loss: 2.3464 - val_accuracy: 0.5946\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1134 - accuracy: 0.9614 - val_loss: 2.2850 - val_accuracy: 0.5875\n",
      "Epoch 649/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1218 - accuracy: 0.9548 - val_loss: 2.4299 - val_accuracy: 0.6083\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1212 - accuracy: 0.9541 - val_loss: 2.3165 - val_accuracy: 0.5962\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1185 - accuracy: 0.9598 - val_loss: 2.3632 - val_accuracy: 0.5933\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1125 - accuracy: 0.9614 - val_loss: 2.3492 - val_accuracy: 0.5913\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1224 - accuracy: 0.9554 - val_loss: 2.3856 - val_accuracy: 0.6042\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1155 - accuracy: 0.9575 - val_loss: 2.3452 - val_accuracy: 0.6037\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1108 - accuracy: 0.9638 - val_loss: 2.3643 - val_accuracy: 0.5896\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1128 - accuracy: 0.9570 - val_loss: 2.2962 - val_accuracy: 0.5883\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1339 - accuracy: 0.9546 - val_loss: 2.4417 - val_accuracy: 0.5946\n",
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1488 - accuracy: 0.9402 - val_loss: 2.3766 - val_accuracy: 0.5917\n",
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1378 - accuracy: 0.9480 - val_loss: 2.4197 - val_accuracy: 0.5821\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1370 - accuracy: 0.9446 - val_loss: 2.3651 - val_accuracy: 0.5825\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1332 - accuracy: 0.9507 - val_loss: 2.3525 - val_accuracy: 0.5850\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1152 - accuracy: 0.9577 - val_loss: 2.3698 - val_accuracy: 0.5854\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1078 - accuracy: 0.9616 - val_loss: 2.3998 - val_accuracy: 0.5908\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1186 - accuracy: 0.9568 - val_loss: 2.4188 - val_accuracy: 0.5879\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1088 - accuracy: 0.9598 - val_loss: 2.4207 - val_accuracy: 0.5979\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1218 - accuracy: 0.9532 - val_loss: 2.3917 - val_accuracy: 0.5854\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1150 - accuracy: 0.9614 - val_loss: 2.4071 - val_accuracy: 0.6004\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1232 - accuracy: 0.9561 - val_loss: 2.3503 - val_accuracy: 0.5867\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1355 - accuracy: 0.9464 - val_loss: 2.3676 - val_accuracy: 0.5817\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1198 - accuracy: 0.9573 - val_loss: 2.3083 - val_accuracy: 0.5888\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1121 - accuracy: 0.9641 - val_loss: 2.3418 - val_accuracy: 0.5867\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1110 - accuracy: 0.9616 - val_loss: 2.3754 - val_accuracy: 0.5954\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1095 - accuracy: 0.9634 - val_loss: 2.3902 - val_accuracy: 0.5888\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1095 - accuracy: 0.9611 - val_loss: 2.4037 - val_accuracy: 0.5967\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1069 - accuracy: 0.9655 - val_loss: 2.4025 - val_accuracy: 0.5871\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1063 - accuracy: 0.9634 - val_loss: 2.3716 - val_accuracy: 0.5950\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1141 - accuracy: 0.9577 - val_loss: 2.4447 - val_accuracy: 0.5987\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1071 - accuracy: 0.9602 - val_loss: 2.4011 - val_accuracy: 0.5888\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1118 - accuracy: 0.9577 - val_loss: 2.4247 - val_accuracy: 0.5875\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1098 - accuracy: 0.9613 - val_loss: 2.3977 - val_accuracy: 0.5929\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1118 - accuracy: 0.9593 - val_loss: 2.4007 - val_accuracy: 0.5929\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1067 - accuracy: 0.9638 - val_loss: 2.4459 - val_accuracy: 0.5875\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.1031 - accuracy: 0.9673 - val_loss: 2.3980 - val_accuracy: 0.5846\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.1248 - accuracy: 0.9523 - val_loss: 2.3931 - val_accuracy: 0.5971\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1216 - accuracy: 0.9534 - val_loss: 2.4098 - val_accuracy: 0.5892\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1276 - accuracy: 0.9573 - val_loss: 2.4162 - val_accuracy: 0.5921\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1210 - accuracy: 0.9552 - val_loss: 2.4275 - val_accuracy: 0.5971\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1245 - accuracy: 0.9564 - val_loss: 2.4045 - val_accuracy: 0.5900\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1202 - accuracy: 0.9566 - val_loss: 2.4328 - val_accuracy: 0.5729\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1527 - accuracy: 0.9454 - val_loss: 2.4597 - val_accuracy: 0.5788\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1501 - accuracy: 0.9416 - val_loss: 2.4001 - val_accuracy: 0.5850\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1454 - accuracy: 0.9496 - val_loss: 2.4979 - val_accuracy: 0.5871\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1522 - accuracy: 0.9405 - val_loss: 2.4335 - val_accuracy: 0.5975\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1513 - accuracy: 0.9432 - val_loss: 2.4205 - val_accuracy: 0.5858\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1458 - accuracy: 0.9407 - val_loss: 2.3520 - val_accuracy: 0.5942\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1241 - accuracy: 0.9548 - val_loss: 2.3671 - val_accuracy: 0.5808\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1053 - accuracy: 0.9629 - val_loss: 2.4022 - val_accuracy: 0.5879\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.1007 - accuracy: 0.9657 - val_loss: 2.3841 - val_accuracy: 0.5863\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0963 - accuracy: 0.9657 - val_loss: 2.3937 - val_accuracy: 0.5967\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0977 - accuracy: 0.9679 - val_loss: 2.4167 - val_accuracy: 0.5854\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1060 - accuracy: 0.9611 - val_loss: 2.4696 - val_accuracy: 0.5983\n",
      "Epoch 702/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1066 - accuracy: 0.9625 - val_loss: 2.4142 - val_accuracy: 0.5871\n",
      "Epoch 703/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1007 - accuracy: 0.9659 - val_loss: 2.4335 - val_accuracy: 0.5821\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0992 - accuracy: 0.9645 - val_loss: 2.4448 - val_accuracy: 0.5846\n",
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0986 - accuracy: 0.9689 - val_loss: 2.4323 - val_accuracy: 0.5929\n",
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0971 - accuracy: 0.9668 - val_loss: 2.4185 - val_accuracy: 0.5950\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0964 - accuracy: 0.9686 - val_loss: 2.4332 - val_accuracy: 0.5867\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1056 - accuracy: 0.9614 - val_loss: 2.4755 - val_accuracy: 0.5875\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1149 - accuracy: 0.9564 - val_loss: 2.4761 - val_accuracy: 0.5875\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1018 - accuracy: 0.9657 - val_loss: 2.4591 - val_accuracy: 0.5896\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1052 - accuracy: 0.9645 - val_loss: 2.4619 - val_accuracy: 0.6000\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1067 - accuracy: 0.9616 - val_loss: 2.4377 - val_accuracy: 0.5867\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0925 - accuracy: 0.9693 - val_loss: 2.4403 - val_accuracy: 0.5933\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0993 - accuracy: 0.9682 - val_loss: 2.4280 - val_accuracy: 0.5908\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1026 - accuracy: 0.9666 - val_loss: 2.4594 - val_accuracy: 0.5983\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0967 - accuracy: 0.9684 - val_loss: 2.4425 - val_accuracy: 0.5921\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0950 - accuracy: 0.9680 - val_loss: 2.4528 - val_accuracy: 0.5925\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0997 - accuracy: 0.9645 - val_loss: 2.4552 - val_accuracy: 0.5933\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1028 - accuracy: 0.9625 - val_loss: 2.5210 - val_accuracy: 0.5996\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1000 - accuracy: 0.9632 - val_loss: 2.4460 - val_accuracy: 0.5871\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0976 - accuracy: 0.9671 - val_loss: 2.4758 - val_accuracy: 0.5858\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1191 - accuracy: 0.9595 - val_loss: 2.4897 - val_accuracy: 0.5854\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1449 - accuracy: 0.9471 - val_loss: 2.4743 - val_accuracy: 0.5896\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1356 - accuracy: 0.9525 - val_loss: 2.4191 - val_accuracy: 0.6004\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1550 - accuracy: 0.9402 - val_loss: 2.5476 - val_accuracy: 0.5850\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1419 - accuracy: 0.9516 - val_loss: 2.4831 - val_accuracy: 0.5879\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1658 - accuracy: 0.9323 - val_loss: 2.5181 - val_accuracy: 0.5846\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1289 - accuracy: 0.9543 - val_loss: 2.4932 - val_accuracy: 0.5975\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1196 - accuracy: 0.9552 - val_loss: 2.4642 - val_accuracy: 0.5942\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0985 - accuracy: 0.9648 - val_loss: 2.4691 - val_accuracy: 0.5858\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1059 - accuracy: 0.9618 - val_loss: 2.5171 - val_accuracy: 0.5871\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1025 - accuracy: 0.9638 - val_loss: 2.4942 - val_accuracy: 0.5792\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0942 - accuracy: 0.9686 - val_loss: 2.4419 - val_accuracy: 0.5917\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0972 - accuracy: 0.9655 - val_loss: 2.4887 - val_accuracy: 0.5908\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0935 - accuracy: 0.9693 - val_loss: 2.4555 - val_accuracy: 0.5925\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0951 - accuracy: 0.9659 - val_loss: 2.4671 - val_accuracy: 0.5867\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0845 - accuracy: 0.9705 - val_loss: 2.5372 - val_accuracy: 0.5929\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0957 - accuracy: 0.9682 - val_loss: 2.5045 - val_accuracy: 0.5933\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0935 - accuracy: 0.9702 - val_loss: 2.4966 - val_accuracy: 0.5996\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1013 - accuracy: 0.9632 - val_loss: 2.5038 - val_accuracy: 0.5829\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0906 - accuracy: 0.9693 - val_loss: 2.4947 - val_accuracy: 0.5908\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0902 - accuracy: 0.9698 - val_loss: 2.5372 - val_accuracy: 0.5992\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0955 - accuracy: 0.9639 - val_loss: 2.5186 - val_accuracy: 0.5892\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0855 - accuracy: 0.9714 - val_loss: 2.5398 - val_accuracy: 0.5917\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0965 - accuracy: 0.9636 - val_loss: 2.5025 - val_accuracy: 0.5813\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0908 - accuracy: 0.9691 - val_loss: 2.5244 - val_accuracy: 0.5871\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0932 - accuracy: 0.9693 - val_loss: 2.5016 - val_accuracy: 0.5846\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0931 - accuracy: 0.9693 - val_loss: 2.5938 - val_accuracy: 0.5958\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0968 - accuracy: 0.9641 - val_loss: 2.5398 - val_accuracy: 0.5917\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0964 - accuracy: 0.9641 - val_loss: 2.5428 - val_accuracy: 0.6042\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0943 - accuracy: 0.9668 - val_loss: 2.5459 - val_accuracy: 0.5892\n",
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0989 - accuracy: 0.9625 - val_loss: 2.5479 - val_accuracy: 0.5925\n",
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1010 - accuracy: 0.9639 - val_loss: 2.5426 - val_accuracy: 0.5850\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0959 - accuracy: 0.9661 - val_loss: 2.5387 - val_accuracy: 0.5962\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0934 - accuracy: 0.9673 - val_loss: 2.5562 - val_accuracy: 0.5846\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0961 - accuracy: 0.9663 - val_loss: 2.5134 - val_accuracy: 0.5904\n",
      "Epoch 757/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0979 - accuracy: 0.9679 - val_loss: 2.5005 - val_accuracy: 0.5850\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1182 - accuracy: 0.9577 - val_loss: 2.4831 - val_accuracy: 0.6021\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0979 - accuracy: 0.9652 - val_loss: 2.5630 - val_accuracy: 0.5888\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0943 - accuracy: 0.9673 - val_loss: 2.5506 - val_accuracy: 0.5863\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0991 - accuracy: 0.9695 - val_loss: 2.5057 - val_accuracy: 0.5904\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0935 - accuracy: 0.9688 - val_loss: 2.5623 - val_accuracy: 0.5875\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0873 - accuracy: 0.9709 - val_loss: 2.6069 - val_accuracy: 0.5854\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1028 - accuracy: 0.9625 - val_loss: 2.5583 - val_accuracy: 0.5925\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1089 - accuracy: 0.9605 - val_loss: 2.5541 - val_accuracy: 0.5850\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0929 - accuracy: 0.9696 - val_loss: 2.5491 - val_accuracy: 0.6008\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1042 - accuracy: 0.9605 - val_loss: 2.5477 - val_accuracy: 0.5908\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0982 - accuracy: 0.9639 - val_loss: 2.6069 - val_accuracy: 0.5875\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0979 - accuracy: 0.9661 - val_loss: 2.5656 - val_accuracy: 0.5913\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1009 - accuracy: 0.9655 - val_loss: 2.5448 - val_accuracy: 0.5946\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1160 - accuracy: 0.9575 - val_loss: 2.5792 - val_accuracy: 0.5917\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1238 - accuracy: 0.9571 - val_loss: 2.5641 - val_accuracy: 0.5954\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1204 - accuracy: 0.9563 - val_loss: 2.5582 - val_accuracy: 0.5850\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1104 - accuracy: 0.9607 - val_loss: 2.5752 - val_accuracy: 0.5892\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0934 - accuracy: 0.9691 - val_loss: 2.5635 - val_accuracy: 0.5871\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0890 - accuracy: 0.9702 - val_loss: 2.5568 - val_accuracy: 0.5804\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0989 - accuracy: 0.9650 - val_loss: 2.6086 - val_accuracy: 0.5775\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0920 - accuracy: 0.9680 - val_loss: 2.6141 - val_accuracy: 0.5875\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0868 - accuracy: 0.9721 - val_loss: 2.5609 - val_accuracy: 0.5929\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0851 - accuracy: 0.9702 - val_loss: 2.5892 - val_accuracy: 0.5938\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0837 - accuracy: 0.9725 - val_loss: 2.5756 - val_accuracy: 0.5896\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0924 - accuracy: 0.9668 - val_loss: 2.5573 - val_accuracy: 0.5950\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0818 - accuracy: 0.9730 - val_loss: 2.5959 - val_accuracy: 0.5879\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0801 - accuracy: 0.9759 - val_loss: 2.5866 - val_accuracy: 0.5913\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0841 - accuracy: 0.9721 - val_loss: 2.5959 - val_accuracy: 0.5858\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0807 - accuracy: 0.9754 - val_loss: 2.5855 - val_accuracy: 0.5917\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0948 - accuracy: 0.9664 - val_loss: 2.5728 - val_accuracy: 0.5896\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1206 - accuracy: 0.9577 - val_loss: 2.6210 - val_accuracy: 0.5771\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1700 - accuracy: 0.9362 - val_loss: 2.5154 - val_accuracy: 0.5758\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1396 - accuracy: 0.9570 - val_loss: 2.4995 - val_accuracy: 0.5929\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1152 - accuracy: 0.9625 - val_loss: 2.6084 - val_accuracy: 0.5863\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1034 - accuracy: 0.9584 - val_loss: 2.5908 - val_accuracy: 0.5813\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0864 - accuracy: 0.9711 - val_loss: 2.5355 - val_accuracy: 0.5962\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0821 - accuracy: 0.9746 - val_loss: 2.5817 - val_accuracy: 0.5946\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0785 - accuracy: 0.9746 - val_loss: 2.5960 - val_accuracy: 0.5917\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0756 - accuracy: 0.9764 - val_loss: 2.5915 - val_accuracy: 0.5946\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0781 - accuracy: 0.9739 - val_loss: 2.6273 - val_accuracy: 0.5879\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0809 - accuracy: 0.9730 - val_loss: 2.6076 - val_accuracy: 0.5892\n",
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0798 - accuracy: 0.9734 - val_loss: 2.6623 - val_accuracy: 0.6062\n",
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0872 - accuracy: 0.9707 - val_loss: 2.6248 - val_accuracy: 0.5933\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0860 - accuracy: 0.9727 - val_loss: 2.6333 - val_accuracy: 0.5983\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0868 - accuracy: 0.9702 - val_loss: 2.6510 - val_accuracy: 0.5883\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0849 - accuracy: 0.9707 - val_loss: 2.5945 - val_accuracy: 0.5875\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0834 - accuracy: 0.9732 - val_loss: 2.6075 - val_accuracy: 0.5971\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0789 - accuracy: 0.9737 - val_loss: 2.6490 - val_accuracy: 0.5871\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0912 - accuracy: 0.9693 - val_loss: 2.5798 - val_accuracy: 0.5996\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0871 - accuracy: 0.9707 - val_loss: 2.5931 - val_accuracy: 0.5867\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0873 - accuracy: 0.9702 - val_loss: 2.5824 - val_accuracy: 0.5983\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0888 - accuracy: 0.9680 - val_loss: 2.6338 - val_accuracy: 0.5908\n",
      "Epoch 810/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0865 - accuracy: 0.9704 - val_loss: 2.6646 - val_accuracy: 0.5863\n",
      "Epoch 811/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1080 - accuracy: 0.9616 - val_loss: 2.7014 - val_accuracy: 0.6012\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1416 - accuracy: 0.9493 - val_loss: 2.6993 - val_accuracy: 0.6062\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1146 - accuracy: 0.9630 - val_loss: 2.6655 - val_accuracy: 0.5992\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1169 - accuracy: 0.9593 - val_loss: 2.6321 - val_accuracy: 0.5863\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1037 - accuracy: 0.9595 - val_loss: 2.6445 - val_accuracy: 0.5858\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.0881 - accuracy: 0.9727 - val_loss: 2.6258 - val_accuracy: 0.5875\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.1006 - accuracy: 0.9634 - val_loss: 2.6591 - val_accuracy: 0.5817\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0920 - accuracy: 0.9691 - val_loss: 2.6621 - val_accuracy: 0.6004\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0862 - accuracy: 0.9725 - val_loss: 2.6405 - val_accuracy: 0.5917\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0777 - accuracy: 0.9739 - val_loss: 2.6602 - val_accuracy: 0.5950\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0803 - accuracy: 0.9746 - val_loss: 2.6947 - val_accuracy: 0.5975\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0731 - accuracy: 0.9762 - val_loss: 2.6801 - val_accuracy: 0.5883\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0716 - accuracy: 0.9762 - val_loss: 2.6688 - val_accuracy: 0.5892\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0770 - accuracy: 0.9741 - val_loss: 2.6851 - val_accuracy: 0.5892\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0768 - accuracy: 0.9734 - val_loss: 2.6793 - val_accuracy: 0.5900\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0714 - accuracy: 0.9771 - val_loss: 2.6776 - val_accuracy: 0.5871\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0712 - accuracy: 0.9775 - val_loss: 2.6861 - val_accuracy: 0.5804\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0725 - accuracy: 0.9766 - val_loss: 2.7064 - val_accuracy: 0.5875\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0721 - accuracy: 0.9786 - val_loss: 2.6836 - val_accuracy: 0.5871\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0716 - accuracy: 0.9786 - val_loss: 2.6758 - val_accuracy: 0.5763\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0783 - accuracy: 0.9725 - val_loss: 2.6941 - val_accuracy: 0.6000\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0749 - accuracy: 0.9755 - val_loss: 2.6822 - val_accuracy: 0.5913\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0812 - accuracy: 0.9721 - val_loss: 2.7187 - val_accuracy: 0.5929\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0773 - accuracy: 0.9736 - val_loss: 2.6969 - val_accuracy: 0.5775\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0851 - accuracy: 0.9704 - val_loss: 2.6716 - val_accuracy: 0.5925\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0764 - accuracy: 0.9743 - val_loss: 2.6768 - val_accuracy: 0.5962\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0782 - accuracy: 0.9734 - val_loss: 2.6882 - val_accuracy: 0.5913\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0707 - accuracy: 0.9775 - val_loss: 2.6892 - val_accuracy: 0.5792\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0795 - accuracy: 0.9745 - val_loss: 2.6730 - val_accuracy: 0.5825\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0809 - accuracy: 0.9743 - val_loss: 2.7321 - val_accuracy: 0.5967\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0868 - accuracy: 0.9693 - val_loss: 2.6824 - val_accuracy: 0.5883\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0813 - accuracy: 0.9716 - val_loss: 2.6631 - val_accuracy: 0.5921\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0856 - accuracy: 0.9716 - val_loss: 2.7041 - val_accuracy: 0.5929\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0743 - accuracy: 0.9768 - val_loss: 2.7153 - val_accuracy: 0.5767\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0718 - accuracy: 0.9780 - val_loss: 2.7040 - val_accuracy: 0.5917\n",
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0740 - accuracy: 0.9761 - val_loss: 2.7150 - val_accuracy: 0.5958\n",
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.0871 - accuracy: 0.9734 - val_loss: 2.6934 - val_accuracy: 0.5958\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0986 - accuracy: 0.9668 - val_loss: 2.7744 - val_accuracy: 0.5896\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1123 - accuracy: 0.9584 - val_loss: 2.7127 - val_accuracy: 0.5850\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1160 - accuracy: 0.9514 - val_loss: 2.7256 - val_accuracy: 0.5821\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1613 - accuracy: 0.9418 - val_loss: 2.7011 - val_accuracy: 0.5713\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2090 - accuracy: 0.9309 - val_loss: 2.7052 - val_accuracy: 0.5975\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1798 - accuracy: 0.9396 - val_loss: 2.6820 - val_accuracy: 0.6025\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1406 - accuracy: 0.9534 - val_loss: 2.7141 - val_accuracy: 0.5987\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1232 - accuracy: 0.9532 - val_loss: 2.7289 - val_accuracy: 0.5908\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0976 - accuracy: 0.9657 - val_loss: 2.7104 - val_accuracy: 0.5888\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0799 - accuracy: 0.9721 - val_loss: 2.6867 - val_accuracy: 0.5825\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0713 - accuracy: 0.9768 - val_loss: 2.7046 - val_accuracy: 0.5900\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0721 - accuracy: 0.9752 - val_loss: 2.6841 - val_accuracy: 0.5888\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0756 - accuracy: 0.9739 - val_loss: 2.7081 - val_accuracy: 0.5758\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0747 - accuracy: 0.9782 - val_loss: 2.7100 - val_accuracy: 0.5867\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0675 - accuracy: 0.9796 - val_loss: 2.7365 - val_accuracy: 0.5996\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0694 - accuracy: 0.9807 - val_loss: 2.6996 - val_accuracy: 0.5938\n",
      "Epoch 864/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0707 - accuracy: 0.9771 - val_loss: 2.7220 - val_accuracy: 0.5913\n",
      "Epoch 865/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0725 - accuracy: 0.9771 - val_loss: 2.7486 - val_accuracy: 0.5729\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0757 - accuracy: 0.9761 - val_loss: 2.7133 - val_accuracy: 0.5896\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0643 - accuracy: 0.9802 - val_loss: 2.7338 - val_accuracy: 0.5933\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0645 - accuracy: 0.9796 - val_loss: 2.7227 - val_accuracy: 0.5925\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0724 - accuracy: 0.9732 - val_loss: 2.7199 - val_accuracy: 0.5929\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0649 - accuracy: 0.9816 - val_loss: 2.7642 - val_accuracy: 0.5950\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0717 - accuracy: 0.9784 - val_loss: 2.7463 - val_accuracy: 0.5817\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0636 - accuracy: 0.9820 - val_loss: 2.7470 - val_accuracy: 0.6000\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0763 - accuracy: 0.9752 - val_loss: 2.7283 - val_accuracy: 0.5863\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0766 - accuracy: 0.9729 - val_loss: 2.7410 - val_accuracy: 0.6058\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0737 - accuracy: 0.9761 - val_loss: 2.7175 - val_accuracy: 0.5913\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0700 - accuracy: 0.9757 - val_loss: 2.7411 - val_accuracy: 0.5962\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0704 - accuracy: 0.9770 - val_loss: 2.7518 - val_accuracy: 0.5783\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0688 - accuracy: 0.9789 - val_loss: 2.7595 - val_accuracy: 0.5783\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0902 - accuracy: 0.9664 - val_loss: 2.7408 - val_accuracy: 0.5896\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0762 - accuracy: 0.9752 - val_loss: 2.7289 - val_accuracy: 0.5921\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0693 - accuracy: 0.9770 - val_loss: 2.7086 - val_accuracy: 0.5871\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0645 - accuracy: 0.9805 - val_loss: 2.7945 - val_accuracy: 0.5938\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0726 - accuracy: 0.9757 - val_loss: 2.7457 - val_accuracy: 0.5854\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0676 - accuracy: 0.9791 - val_loss: 2.7906 - val_accuracy: 0.5842\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0702 - accuracy: 0.9784 - val_loss: 2.7253 - val_accuracy: 0.5921\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0682 - accuracy: 0.9754 - val_loss: 2.8186 - val_accuracy: 0.5896\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0717 - accuracy: 0.9787 - val_loss: 2.7463 - val_accuracy: 0.5821\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0685 - accuracy: 0.9775 - val_loss: 2.7288 - val_accuracy: 0.5804\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0699 - accuracy: 0.9766 - val_loss: 2.7495 - val_accuracy: 0.5904\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0745 - accuracy: 0.9757 - val_loss: 2.7455 - val_accuracy: 0.5883\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0718 - accuracy: 0.9750 - val_loss: 2.7658 - val_accuracy: 0.5796\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0750 - accuracy: 0.9761 - val_loss: 2.7432 - val_accuracy: 0.5888\n",
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0737 - accuracy: 0.9764 - val_loss: 2.7737 - val_accuracy: 0.5825\n",
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0738 - accuracy: 0.9759 - val_loss: 2.8016 - val_accuracy: 0.6008\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0717 - accuracy: 0.9761 - val_loss: 2.7983 - val_accuracy: 0.5996\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0911 - accuracy: 0.9680 - val_loss: 2.7734 - val_accuracy: 0.5900\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1198 - accuracy: 0.9621 - val_loss: 2.7826 - val_accuracy: 0.5888\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1664 - accuracy: 0.9425 - val_loss: 2.7325 - val_accuracy: 0.5875\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1825 - accuracy: 0.9321 - val_loss: 2.8942 - val_accuracy: 0.5875\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1500 - accuracy: 0.9455 - val_loss: 2.7676 - val_accuracy: 0.5867\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1161 - accuracy: 0.9575 - val_loss: 2.7886 - val_accuracy: 0.5950\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0949 - accuracy: 0.9673 - val_loss: 2.7867 - val_accuracy: 0.5792\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0891 - accuracy: 0.9695 - val_loss: 2.7486 - val_accuracy: 0.5883\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0819 - accuracy: 0.9727 - val_loss: 2.7836 - val_accuracy: 0.6004\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0800 - accuracy: 0.9768 - val_loss: 2.7748 - val_accuracy: 0.6033\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0785 - accuracy: 0.9730 - val_loss: 2.7570 - val_accuracy: 0.5817\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0636 - accuracy: 0.9812 - val_loss: 2.7787 - val_accuracy: 0.5813\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0628 - accuracy: 0.9812 - val_loss: 2.7832 - val_accuracy: 0.5962\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0654 - accuracy: 0.9786 - val_loss: 2.8345 - val_accuracy: 0.5879\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0631 - accuracy: 0.9816 - val_loss: 2.7860 - val_accuracy: 0.5908\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0604 - accuracy: 0.9818 - val_loss: 2.7928 - val_accuracy: 0.5925\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0628 - accuracy: 0.9804 - val_loss: 2.7966 - val_accuracy: 0.5821\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0601 - accuracy: 0.9807 - val_loss: 2.7808 - val_accuracy: 0.5846\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0626 - accuracy: 0.9814 - val_loss: 2.7803 - val_accuracy: 0.5921\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0632 - accuracy: 0.9786 - val_loss: 2.7960 - val_accuracy: 0.5875\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0630 - accuracy: 0.9789 - val_loss: 2.8152 - val_accuracy: 0.5879\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0674 - accuracy: 0.9752 - val_loss: 2.7981 - val_accuracy: 0.5888\n",
      "Epoch 918/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0656 - accuracy: 0.9777 - val_loss: 2.7935 - val_accuracy: 0.5942\n",
      "Epoch 919/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0588 - accuracy: 0.9809 - val_loss: 2.7996 - val_accuracy: 0.5938\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0625 - accuracy: 0.9814 - val_loss: 2.8334 - val_accuracy: 0.5842\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0612 - accuracy: 0.9814 - val_loss: 2.8098 - val_accuracy: 0.5888\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0644 - accuracy: 0.9787 - val_loss: 2.8052 - val_accuracy: 0.5946\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.0595 - accuracy: 0.9804 - val_loss: 2.8082 - val_accuracy: 0.5842\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0673 - accuracy: 0.9793 - val_loss: 2.8232 - val_accuracy: 0.5896\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0685 - accuracy: 0.9773 - val_loss: 2.8143 - val_accuracy: 0.5875\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0635 - accuracy: 0.9811 - val_loss: 2.8235 - val_accuracy: 0.5954\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0666 - accuracy: 0.9793 - val_loss: 2.8105 - val_accuracy: 0.5954\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0598 - accuracy: 0.9823 - val_loss: 2.8174 - val_accuracy: 0.5871\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0621 - accuracy: 0.9812 - val_loss: 2.8651 - val_accuracy: 0.5954\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0638 - accuracy: 0.9805 - val_loss: 2.8284 - val_accuracy: 0.5846\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0697 - accuracy: 0.9780 - val_loss: 2.8230 - val_accuracy: 0.5854\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0722 - accuracy: 0.9764 - val_loss: 2.8113 - val_accuracy: 0.5913\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0706 - accuracy: 0.9773 - val_loss: 2.8782 - val_accuracy: 0.5875\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0706 - accuracy: 0.9780 - val_loss: 2.9081 - val_accuracy: 0.5754\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1172 - accuracy: 0.9564 - val_loss: 2.8169 - val_accuracy: 0.5829\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1452 - accuracy: 0.9586 - val_loss: 2.8600 - val_accuracy: 0.5771\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2652 - accuracy: 0.9098 - val_loss: 2.7622 - val_accuracy: 0.6017\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1505 - accuracy: 0.9459 - val_loss: 2.8512 - val_accuracy: 0.5888\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1113 - accuracy: 0.9627 - val_loss: 2.8534 - val_accuracy: 0.5904\n",
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.1111 - accuracy: 0.9627 - val_loss: 2.8281 - val_accuracy: 0.5892\n",
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0829 - accuracy: 0.9718 - val_loss: 2.8520 - val_accuracy: 0.5813\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1033 - accuracy: 0.9679 - val_loss: 2.8346 - val_accuracy: 0.5971\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0751 - accuracy: 0.9737 - val_loss: 2.8456 - val_accuracy: 0.5854\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0690 - accuracy: 0.9773 - val_loss: 2.8205 - val_accuracy: 0.5867\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0650 - accuracy: 0.9796 - val_loss: 2.8568 - val_accuracy: 0.5950\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0565 - accuracy: 0.9820 - val_loss: 2.8716 - val_accuracy: 0.5871\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0515 - accuracy: 0.9870 - val_loss: 2.8423 - val_accuracy: 0.5942\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0574 - accuracy: 0.9818 - val_loss: 2.8328 - val_accuracy: 0.5825\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0561 - accuracy: 0.9832 - val_loss: 2.8800 - val_accuracy: 0.5904\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0572 - accuracy: 0.9830 - val_loss: 2.8327 - val_accuracy: 0.5879\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0577 - accuracy: 0.9823 - val_loss: 2.8460 - val_accuracy: 0.5875\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0616 - accuracy: 0.9814 - val_loss: 2.8301 - val_accuracy: 0.5904\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0540 - accuracy: 0.9850 - val_loss: 2.8715 - val_accuracy: 0.5821\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0612 - accuracy: 0.9807 - val_loss: 2.8817 - val_accuracy: 0.5804\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0576 - accuracy: 0.9829 - val_loss: 2.8727 - val_accuracy: 0.5817\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0605 - accuracy: 0.9809 - val_loss: 2.8420 - val_accuracy: 0.5867\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0635 - accuracy: 0.9795 - val_loss: 2.8636 - val_accuracy: 0.5917\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0607 - accuracy: 0.9807 - val_loss: 2.8706 - val_accuracy: 0.5908\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0578 - accuracy: 0.9820 - val_loss: 2.8591 - val_accuracy: 0.5942\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0605 - accuracy: 0.9829 - val_loss: 2.9296 - val_accuracy: 0.5892\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0767 - accuracy: 0.9732 - val_loss: 2.9023 - val_accuracy: 0.5842\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.0658 - accuracy: 0.9795 - val_loss: 2.8911 - val_accuracy: 0.5871\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.0622 - accuracy: 0.9802 - val_loss: 2.8622 - val_accuracy: 0.5971\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0670 - accuracy: 0.9793 - val_loss: 2.8584 - val_accuracy: 0.5967\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0676 - accuracy: 0.9779 - val_loss: 2.8783 - val_accuracy: 0.5888\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0534 - accuracy: 0.9848 - val_loss: 2.8745 - val_accuracy: 0.5850\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0599 - accuracy: 0.9821 - val_loss: 2.9312 - val_accuracy: 0.5813\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.0636 - accuracy: 0.9791 - val_loss: 2.9063 - val_accuracy: 0.5792\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0610 - accuracy: 0.9823 - val_loss: 2.9260 - val_accuracy: 0.5929\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0693 - accuracy: 0.9748 - val_loss: 2.9023 - val_accuracy: 0.5933\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0688 - accuracy: 0.9754 - val_loss: 2.9354 - val_accuracy: 0.5900\n",
      "Epoch 972/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0648 - accuracy: 0.9802 - val_loss: 2.9248 - val_accuracy: 0.5746\n",
      "Epoch 973/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0619 - accuracy: 0.9796 - val_loss: 2.8927 - val_accuracy: 0.5938\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0602 - accuracy: 0.9816 - val_loss: 2.9062 - val_accuracy: 0.5858\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0568 - accuracy: 0.9850 - val_loss: 2.8897 - val_accuracy: 0.5892\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0502 - accuracy: 0.9859 - val_loss: 2.9288 - val_accuracy: 0.5850\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0570 - accuracy: 0.9830 - val_loss: 2.9122 - val_accuracy: 0.5967\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0541 - accuracy: 0.9834 - val_loss: 2.9075 - val_accuracy: 0.5788\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0590 - accuracy: 0.9814 - val_loss: 2.9268 - val_accuracy: 0.5979\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0645 - accuracy: 0.9780 - val_loss: 2.9063 - val_accuracy: 0.5933\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0601 - accuracy: 0.9812 - val_loss: 2.9125 - val_accuracy: 0.5867\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0590 - accuracy: 0.9809 - val_loss: 2.9528 - val_accuracy: 0.5992\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0591 - accuracy: 0.9832 - val_loss: 2.8996 - val_accuracy: 0.5838\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0601 - accuracy: 0.9825 - val_loss: 2.9124 - val_accuracy: 0.5942\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0564 - accuracy: 0.9834 - val_loss: 2.9170 - val_accuracy: 0.5921\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0650 - accuracy: 0.9804 - val_loss: 2.9339 - val_accuracy: 0.5871\n",
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0555 - accuracy: 0.9841 - val_loss: 2.9279 - val_accuracy: 0.5904\n",
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0705 - accuracy: 0.9784 - val_loss: 2.9181 - val_accuracy: 0.5946\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0811 - accuracy: 0.9723 - val_loss: 2.8413 - val_accuracy: 0.5821\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0765 - accuracy: 0.9741 - val_loss: 2.9866 - val_accuracy: 0.5833\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1094 - accuracy: 0.9613 - val_loss: 2.9333 - val_accuracy: 0.5917\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1930 - accuracy: 0.9445 - val_loss: 2.9804 - val_accuracy: 0.5842\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1576 - accuracy: 0.9513 - val_loss: 2.9231 - val_accuracy: 0.5896\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1062 - accuracy: 0.9643 - val_loss: 2.9096 - val_accuracy: 0.5879\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1127 - accuracy: 0.9604 - val_loss: 2.8685 - val_accuracy: 0.5838\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0940 - accuracy: 0.9684 - val_loss: 2.8973 - val_accuracy: 0.5962\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0625 - accuracy: 0.9809 - val_loss: 2.9088 - val_accuracy: 0.5888\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0670 - accuracy: 0.9791 - val_loss: 2.9054 - val_accuracy: 0.5946\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0642 - accuracy: 0.9805 - val_loss: 2.9496 - val_accuracy: 0.5896\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0537 - accuracy: 0.9861 - val_loss: 2.8965 - val_accuracy: 0.5971\n"
     ]
    }
   ],
   "source": [
    "# specify network layers\n",
    "binary_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (13, )),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# compile and fit network\n",
    "binary_ann.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "history = binary_ann.fit(X_train, y_train, epochs = 1000, batch_size = 128, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs. Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 0.7570 - accuracy: 0.5245 - val_loss: 0.6523 - val_accuracy: 0.6454\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6573 - accuracy: 0.6402 - val_loss: 0.6477 - val_accuracy: 0.6454\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6531 - accuracy: 0.6407 - val_loss: 0.6456 - val_accuracy: 0.6454\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6518 - accuracy: 0.6416 - val_loss: 0.6441 - val_accuracy: 0.6454\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6508 - accuracy: 0.6414 - val_loss: 0.6438 - val_accuracy: 0.6454\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6501 - accuracy: 0.6414 - val_loss: 0.6424 - val_accuracy: 0.6454\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6499 - accuracy: 0.6418 - val_loss: 0.6419 - val_accuracy: 0.6454\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 0s 41us/sample - loss: 0.6492 - accuracy: 0.6414 - val_loss: 0.6413 - val_accuracy: 0.6454\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6471 - accuracy: 0.6418 - val_loss: 0.6405 - val_accuracy: 0.6454\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6479 - accuracy: 0.6418 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6481 - accuracy: 0.6418 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6497 - accuracy: 0.6418 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6485 - accuracy: 0.6414 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6479 - accuracy: 0.6418 - val_loss: 0.6424 - val_accuracy: 0.6454\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6483 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6410 - val_accuracy: 0.6454\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6481 - accuracy: 0.6416 - val_loss: 0.6403 - val_accuracy: 0.6454\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6471 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6477 - accuracy: 0.6416 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6473 - accuracy: 0.6416 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6480 - accuracy: 0.6416 - val_loss: 0.6430 - val_accuracy: 0.6454\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6466 - accuracy: 0.6413 - val_loss: 0.6437 - val_accuracy: 0.6454\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6499 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6456 - accuracy: 0.6413 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6486 - accuracy: 0.6418 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6478 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6476 - accuracy: 0.6414 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 0s 44us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6480 - accuracy: 0.6416 - val_loss: 0.6422 - val_accuracy: 0.6454\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6468 - accuracy: 0.6418 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6473 - accuracy: 0.6414 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6479 - accuracy: 0.6416 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6437 - val_accuracy: 0.6454\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6449 - accuracy: 0.6418 - val_loss: 0.6501 - val_accuracy: 0.6408\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6464 - accuracy: 0.6420 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6484 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6468 - accuracy: 0.6413 - val_loss: 0.6429 - val_accuracy: 0.6454\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6418 - val_accuracy: 0.6454\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6488 - accuracy: 0.6416 - val_loss: 0.6417 - val_accuracy: 0.6454\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6475 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6473 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6467 - accuracy: 0.6416 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6463 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 54/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6463 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6428 - val_accuracy: 0.6454\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 0s 54us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6455 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6447 - accuracy: 0.6416 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6457 - accuracy: 0.6414 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6458 - accuracy: 0.6414 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6441 - val_accuracy: 0.6454\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6461 - accuracy: 0.6418 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6410 - val_accuracy: 0.6454\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6464 - accuracy: 0.6418 - val_loss: 0.6403 - val_accuracy: 0.6454\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6448 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6468 - accuracy: 0.6418 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6418 - val_accuracy: 0.6454\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6455 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6450 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6462 - accuracy: 0.6409 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6484 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6451 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 94/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 95/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6458 - accuracy: 0.6414 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6475 - accuracy: 0.6411 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6414 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6452 - accuracy: 0.6414 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6442 - accuracy: 0.6414 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6450 - accuracy: 0.6413 - val_loss: 0.6456 - val_accuracy: 0.6446\n",
      "Epoch 108/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6475 - accuracy: 0.6411 - val_loss: 0.6384 - val_accuracy: 0.6454\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6451 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6443 - accuracy: 0.6418 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6453 - accuracy: 0.6416 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6445 - accuracy: 0.6418 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6449 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6468 - accuracy: 0.6414 - val_loss: 0.6411 - val_accuracy: 0.6454\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - 0s 36us/sample - loss: 0.6437 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6471 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6440 - accuracy: 0.6420 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6471 - accuracy: 0.6420 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6475 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6433 - val_accuracy: 0.6454\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6466 - accuracy: 0.6423 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6434 - accuracy: 0.6405 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6449 - accuracy: 0.6416 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6451 - accuracy: 0.6423 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6379 - val_accuracy: 0.6454\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6435 - accuracy: 0.6414 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6441 - accuracy: 0.6418 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6454 - accuracy: 0.6414 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6429 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6444 - accuracy: 0.6418 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6439 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6433 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6441 - accuracy: 0.6413 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6411 - val_accuracy: 0.6454\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6431 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 162/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 163/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6422 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6426 - accuracy: 0.6414 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6450 - accuracy: 0.6418 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6445 - accuracy: 0.6414 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6439 - accuracy: 0.6418 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6435 - accuracy: 0.6420 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6435 - accuracy: 0.6416 - val_loss: 0.6379 - val_accuracy: 0.6454\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6436 - accuracy: 0.6416 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6444 - accuracy: 0.6409 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6384 - val_accuracy: 0.6454\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6422 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6450 - accuracy: 0.6416 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6430 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6456 - accuracy: 0.6418 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6425 - accuracy: 0.6409 - val_loss: 0.6450 - val_accuracy: 0.6442\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6418 - accuracy: 0.6421 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6427 - accuracy: 0.6418 - val_loss: 0.6412 - val_accuracy: 0.6454\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6420 - accuracy: 0.6414 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6420 - accuracy: 0.6420 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6435 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6438 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6416 - accuracy: 0.6413 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6438 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6425 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6406 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6426 - accuracy: 0.6416 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6439 - accuracy: 0.6418 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6420 - accuracy: 0.6423 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6428 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6428 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6447 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6409 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6430 - accuracy: 0.6404 - val_loss: 0.6424 - val_accuracy: 0.6463\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6417 - accuracy: 0.6413 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6419 - accuracy: 0.6418 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6406 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6420 - accuracy: 0.6423 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 216/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6425 - accuracy: 0.6411 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 217/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6415 - accuracy: 0.6413 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6426 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6417 - accuracy: 0.6418 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6442 - accuracy: 0.6414 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6428 - accuracy: 0.6420 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6421 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6419 - accuracy: 0.6414 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6404 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6429 - accuracy: 0.6413 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6420 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6425 - accuracy: 0.6418 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6413 - accuracy: 0.6414 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6421 - accuracy: 0.6414 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6409 - accuracy: 0.6420 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6418 - accuracy: 0.6418 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6428 - accuracy: 0.6414 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6421 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6412 - accuracy: 0.6416 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6417 - accuracy: 0.6414 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6433 - accuracy: 0.6409 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6410 - accuracy: 0.6420 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6401 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6415 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6412 - accuracy: 0.6414 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6398 - accuracy: 0.6414 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6424 - accuracy: 0.6416 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6407 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6390 - accuracy: 0.6420 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6413 - accuracy: 0.6416 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6405 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6417 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6390 - accuracy: 0.6418 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6398 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6408 - accuracy: 0.6416 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6414 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6409 - accuracy: 0.6421 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6421 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6420 - accuracy: 0.6418 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6407 - accuracy: 0.6413 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6412 - accuracy: 0.6416 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6386 - accuracy: 0.6413 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6395 - accuracy: 0.6414 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6389 - accuracy: 0.6413 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6405 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6373 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6399 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 270/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6407 - accuracy: 0.6411 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 271/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6400 - accuracy: 0.6418 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6401 - accuracy: 0.6427 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6379 - accuracy: 0.6421 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6378 - accuracy: 0.6418 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6394 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6394 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6394 - accuracy: 0.6421 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6402 - accuracy: 0.6418 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6410 - accuracy: 0.6416 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6400 - accuracy: 0.6416 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6403 - accuracy: 0.6416 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6384 - accuracy: 0.6413 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6414 - accuracy: 0.6418 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6401 - accuracy: 0.6430 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6407 - accuracy: 0.6416 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6394 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6450\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6410 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6400 - accuracy: 0.6416 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6404 - accuracy: 0.6407 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6401 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6396 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6408 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6399 - accuracy: 0.6411 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6416 - accuracy: 0.6421 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6392 - accuracy: 0.6418 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6407 - accuracy: 0.6396 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6403 - accuracy: 0.6411 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6423 - accuracy: 0.6409 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6404 - accuracy: 0.6416 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6423 - accuracy: 0.6413 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6384 - accuracy: 0.6421 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6404 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6387 - accuracy: 0.6414 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6391 - accuracy: 0.6413 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6380 - accuracy: 0.6421 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6400 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6393 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 53us/sample - loss: 0.6404 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6406 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6397 - accuracy: 0.6414 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6378 - accuracy: 0.6427 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 51us/sample - loss: 0.6413 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6384 - accuracy: 0.6407 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6355 - accuracy: 0.6407 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6402 - accuracy: 0.6411 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6410 - accuracy: 0.6373 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6391 - accuracy: 0.6413 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6401 - accuracy: 0.6407 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6401 - accuracy: 0.6409 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 324/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6385 - accuracy: 0.6421 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 325/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6384 - accuracy: 0.6413 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6389 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6380 - accuracy: 0.6420 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6374 - accuracy: 0.6418 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6391 - accuracy: 0.6434 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6398 - accuracy: 0.6432 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6395 - accuracy: 0.6409 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6414 - accuracy: 0.6407 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6386 - accuracy: 0.6425 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6388 - accuracy: 0.6423 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6383 - accuracy: 0.6413 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6400 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6395 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6391 - accuracy: 0.6421 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6358 - accuracy: 0.6405 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6394 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6401 - accuracy: 0.6407 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6397 - accuracy: 0.6414 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6391 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6388 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6369 - accuracy: 0.6432 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6412 - accuracy: 0.6402 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6384 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6376 - accuracy: 0.6418 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6380 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6381 - accuracy: 0.6413 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6376 - accuracy: 0.6416 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6374 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6375 - accuracy: 0.6425 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6363 - accuracy: 0.6432 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6372 - accuracy: 0.6404 - val_loss: 0.6346 - val_accuracy: 0.6450\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6384 - accuracy: 0.6409 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6396 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6394 - accuracy: 0.6409 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6371 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6379 - accuracy: 0.6416 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6388 - accuracy: 0.6416 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6371 - accuracy: 0.6423 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6374 - accuracy: 0.6427 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6383 - accuracy: 0.6421 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6367 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6369 - accuracy: 0.6425 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6401 - accuracy: 0.6402 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6385 - accuracy: 0.6423 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6376 - accuracy: 0.6427 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6375 - accuracy: 0.6407 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6393 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6411 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6386 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 378/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6370 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6389 - accuracy: 0.6407 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6390 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6358 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6382 - accuracy: 0.6414 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6370 - accuracy: 0.6427 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6392 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6378 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6384 - accuracy: 0.6418 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6377 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6379 - accuracy: 0.6411 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6371 - accuracy: 0.6427 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6387 - accuracy: 0.6413 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6393 - accuracy: 0.6413 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6413 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6368 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6398 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6389 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6349 - accuracy: 0.6420 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6367 - accuracy: 0.6414 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6377 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6383 - accuracy: 0.6414 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6373 - accuracy: 0.6425 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6371 - accuracy: 0.6416 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6375 - accuracy: 0.6429 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6376 - accuracy: 0.6421 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6378 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6371 - accuracy: 0.6418 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6395 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6350 - accuracy: 0.6420 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6383 - accuracy: 0.6423 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 0.6348 - accuracy: 0.6407 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6380 - accuracy: 0.6421 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6371 - accuracy: 0.6398 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6367 - accuracy: 0.6400 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6369 - accuracy: 0.6391 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6356 - accuracy: 0.6438 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 0.6375 - accuracy: 0.6391 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 0.6352 - accuracy: 0.6411 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6362 - accuracy: 0.6402 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6398 - accuracy: 0.6414 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6396 - accuracy: 0.6420 - val_loss: 0.6322 - val_accuracy: 0.6454\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6392 - accuracy: 0.6429 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6379 - accuracy: 0.6418 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6377 - accuracy: 0.6416 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6357 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6340 - accuracy: 0.6434 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6357 - accuracy: 0.6400 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6373 - accuracy: 0.6418 - val_loss: 0.6321 - val_accuracy: 0.6454\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 0.6374 - accuracy: 0.6438 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6391 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 0s 46us/sample - loss: 0.6367 - accuracy: 0.6413 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 432/1000\n",
      "5600/5600 [==============================] - 0s 50us/sample - loss: 0.6334 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 433/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 62us/sample - loss: 0.6365 - accuracy: 0.6420 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6399 - accuracy: 0.6400 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6384 - accuracy: 0.6416 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6359 - accuracy: 0.6413 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6404 - accuracy: 0.6402 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 46us/sample - loss: 0.6365 - accuracy: 0.6421 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6366 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 0s 50us/sample - loss: 0.6337 - accuracy: 0.6429 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6360 - accuracy: 0.6438 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6364 - accuracy: 0.6407 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6374 - accuracy: 0.6409 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6373 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6378 - accuracy: 0.6407 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6377 - accuracy: 0.6411 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6372 - accuracy: 0.6395 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6361 - accuracy: 0.6416 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6350 - accuracy: 0.6414 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6393 - accuracy: 0.6418 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6367 - accuracy: 0.6413 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6370 - accuracy: 0.6400 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6370 - accuracy: 0.6423 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6380 - accuracy: 0.6436 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6351 - accuracy: 0.6429 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6368 - accuracy: 0.6411 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6326 - accuracy: 0.6434 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6371 - accuracy: 0.6396 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6363 - accuracy: 0.6425 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6365 - accuracy: 0.6452 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6347 - accuracy: 0.6420 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 0.6396 - accuracy: 0.6427 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6369 - accuracy: 0.6432 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6352 - accuracy: 0.6405 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6386 - accuracy: 0.6420 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6373 - accuracy: 0.6421 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6327 - accuracy: 0.6430 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6395 - accuracy: 0.6421 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6362 - accuracy: 0.6405 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6340 - accuracy: 0.6398 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6358 - accuracy: 0.6421 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6368 - accuracy: 0.6421 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6371 - accuracy: 0.6400 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6379 - accuracy: 0.6413 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6348 - accuracy: 0.6430 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6390 - accuracy: 0.6414 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6362 - accuracy: 0.6420 - val_loss: 0.6346 - val_accuracy: 0.6458\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6356 - accuracy: 0.6389 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6414 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6363 - accuracy: 0.6423 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6367 - accuracy: 0.6398 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6368 - accuracy: 0.6423 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 486/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6379 - accuracy: 0.6411 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 487/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6366 - accuracy: 0.6407 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6346 - accuracy: 0.6416 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6390 - accuracy: 0.6416 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6361 - accuracy: 0.6434 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6388 - accuracy: 0.6405 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6375 - accuracy: 0.6416 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6365 - accuracy: 0.6427 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6402 - val_loss: 0.6319 - val_accuracy: 0.6454\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6362 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6356 - accuracy: 0.6414 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6353 - accuracy: 0.6405 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6413 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6418 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6434 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6365 - accuracy: 0.6405 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6436 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6407 - val_loss: 0.6340 - val_accuracy: 0.6463\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6379 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6348 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6357 - accuracy: 0.6423 - val_loss: 0.6333 - val_accuracy: 0.6458\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6348 - accuracy: 0.6414 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6346 - accuracy: 0.6441 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6367 - accuracy: 0.6427 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6378 - accuracy: 0.6421 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6369 - accuracy: 0.6414 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6361 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6409 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6414 - val_loss: 0.6414 - val_accuracy: 0.6454\n",
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6420 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6420 - val_loss: 0.6320 - val_accuracy: 0.6454\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6375 - accuracy: 0.6405 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6378 - accuracy: 0.6405 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6423 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6383 - accuracy: 0.6407 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6432 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6372 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6432 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6425 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6418 - val_loss: 0.6368 - val_accuracy: 0.6454\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6425 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6429 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6409 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6425 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6414 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6375 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6413 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6360 - accuracy: 0.6423 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6418 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 540/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6365 - accuracy: 0.6404 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 541/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6349 - accuracy: 0.6421 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6445 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6404 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6420 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6404 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6381 - accuracy: 0.6405 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6360 - accuracy: 0.6407 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6349 - accuracy: 0.6432 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6414 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6356 - accuracy: 0.6436 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6421 - val_loss: 0.6317 - val_accuracy: 0.6454\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6405 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6416 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6357 - accuracy: 0.6404 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6421 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6358 - accuracy: 0.6409 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6355 - accuracy: 0.6396 - val_loss: 0.6328 - val_accuracy: 0.6450\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6438 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6407 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6373 - accuracy: 0.6382 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6351 - accuracy: 0.6413 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6416 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6372 - accuracy: 0.6439 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6354 - accuracy: 0.6425 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6420 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6427 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6328 - accuracy: 0.6413 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6414 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6445 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6381 - accuracy: 0.6436 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6414 - val_loss: 0.6336 - val_accuracy: 0.6458\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6377 - accuracy: 0.6407 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6356 - accuracy: 0.6420 - val_loss: 0.6316 - val_accuracy: 0.6454\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6421 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6370 - accuracy: 0.6405 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6349 - accuracy: 0.6429 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6421 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6432 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6438 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6365 - accuracy: 0.6427 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6421 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6393 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6358 - accuracy: 0.6423 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6413 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6330 - accuracy: 0.6441 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6339 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6420 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6427 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6382 - val_loss: 0.6318 - val_accuracy: 0.6458\n",
      "Epoch 594/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6425 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6438 - val_loss: 0.6339 - val_accuracy: 0.6450\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6430 - val_loss: 0.6354 - val_accuracy: 0.6450\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6420 - val_loss: 0.6378 - val_accuracy: 0.6450\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6411 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6352 - accuracy: 0.6421 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6367 - accuracy: 0.6402 - val_loss: 0.6331 - val_accuracy: 0.6450\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6425 - val_loss: 0.6326 - val_accuracy: 0.6450\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6421 - val_loss: 0.6389 - val_accuracy: 0.6450\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6429 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6325 - val_accuracy: 0.6450\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6402 - val_loss: 0.6335 - val_accuracy: 0.6450\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6355 - accuracy: 0.6443 - val_loss: 0.6337 - val_accuracy: 0.6458\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6452 - val_loss: 0.6318 - val_accuracy: 0.6454\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6427 - val_loss: 0.6342 - val_accuracy: 0.6450\n",
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6409 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6416 - val_loss: 0.6348 - val_accuracy: 0.6450\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6405 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6434 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6400 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6393 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6423 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6425 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6400 - val_loss: 0.6347 - val_accuracy: 0.6450\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6340 - accuracy: 0.6411 - val_loss: 0.6358 - val_accuracy: 0.6450\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6418 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6421 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6359 - accuracy: 0.6379 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6427 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6317 - val_accuracy: 0.6454\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6427 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6423 - val_loss: 0.6320 - val_accuracy: 0.6450\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6343 - accuracy: 0.6402 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6396 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6330 - accuracy: 0.6421 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6441 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6425 - val_loss: 0.6347 - val_accuracy: 0.6450\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6441 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6441 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6441 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6370 - accuracy: 0.6418 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6446 - val_loss: 0.6349 - val_accuracy: 0.6450\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6425 - val_loss: 0.6330 - val_accuracy: 0.6446\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6395 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6404 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6366 - accuracy: 0.6416 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6446 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6405 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6409 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 648/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6356 - accuracy: 0.6429 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6320 - accuracy: 0.6421 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6411 - val_loss: 0.6363 - val_accuracy: 0.6450\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6450\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6429 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6393 - val_loss: 0.6350 - val_accuracy: 0.6450\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6398 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6436 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6423 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6314 - accuracy: 0.6429 - val_loss: 0.6350 - val_accuracy: 0.6450\n",
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6330 - accuracy: 0.6405 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6405 - val_loss: 0.6326 - val_accuracy: 0.6442\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6407 - val_loss: 0.6364 - val_accuracy: 0.6450\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6413 - val_loss: 0.6327 - val_accuracy: 0.6450\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6405 - val_loss: 0.6329 - val_accuracy: 0.6450\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6386 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6427 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6380 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6353 - accuracy: 0.6413 - val_loss: 0.6324 - val_accuracy: 0.6450\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6334 - accuracy: 0.6427 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6416 - val_loss: 0.6327 - val_accuracy: 0.6450\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6393 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6346 - accuracy: 0.6425 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6404 - val_loss: 0.6324 - val_accuracy: 0.6450\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6418 - val_loss: 0.6319 - val_accuracy: 0.6454\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6425 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6438 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6430 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6356 - accuracy: 0.6379 - val_loss: 0.6378 - val_accuracy: 0.6450\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6423 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6423 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6427 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6353 - accuracy: 0.6429 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6350 - accuracy: 0.6425 - val_loss: 0.6348 - val_accuracy: 0.6450\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6342 - accuracy: 0.6438 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6342 - accuracy: 0.6416 - val_loss: 0.6320 - val_accuracy: 0.6446\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6331 - accuracy: 0.6434 - val_loss: 0.6340 - val_accuracy: 0.6450\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6351 - accuracy: 0.6425 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6324 - accuracy: 0.6457 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6343 - accuracy: 0.6405 - val_loss: 0.6351 - val_accuracy: 0.6446\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6450 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6338 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6413 - val_loss: 0.6368 - val_accuracy: 0.6454\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6359 - accuracy: 0.6438 - val_loss: 0.6411 - val_accuracy: 0.6450\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6396 - val_loss: 0.6338 - val_accuracy: 0.6450\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6413 - val_loss: 0.6331 - val_accuracy: 0.6450\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6423 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6439 - val_loss: 0.6335 - val_accuracy: 0.6450\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6420 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6321 - val_accuracy: 0.6446\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6332 - accuracy: 0.6425 - val_loss: 0.6456 - val_accuracy: 0.6450\n",
      "Epoch 702/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6353 - accuracy: 0.6427 - val_loss: 0.6390 - val_accuracy: 0.6450\n",
      "Epoch 703/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6352 - accuracy: 0.6421 - val_loss: 0.6403 - val_accuracy: 0.6450\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6354 - accuracy: 0.6418 - val_loss: 0.6359 - val_accuracy: 0.6450\n",
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6368 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6436 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6420 - val_loss: 0.6336 - val_accuracy: 0.6446\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6402 - val_loss: 0.6332 - val_accuracy: 0.6442\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6404 - val_loss: 0.6378 - val_accuracy: 0.6446\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6379 - accuracy: 0.6382 - val_loss: 0.6337 - val_accuracy: 0.6450\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6439 - val_loss: 0.6359 - val_accuracy: 0.6446\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6427 - val_loss: 0.6354 - val_accuracy: 0.6450\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6352 - accuracy: 0.6411 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6436 - val_loss: 0.6319 - val_accuracy: 0.6446\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6334 - accuracy: 0.6405 - val_loss: 0.6317 - val_accuracy: 0.6446\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6464 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6427 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6365 - accuracy: 0.6407 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6347 - accuracy: 0.6409 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6328 - accuracy: 0.6409 - val_loss: 0.6309 - val_accuracy: 0.6446\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6366 - accuracy: 0.6382 - val_loss: 0.6313 - val_accuracy: 0.6446\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6452 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6357 - accuracy: 0.6405 - val_loss: 0.6321 - val_accuracy: 0.6446\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6429 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6357 - accuracy: 0.6430 - val_loss: 0.6350 - val_accuracy: 0.6446\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6404 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6347 - accuracy: 0.6421 - val_loss: 0.6352 - val_accuracy: 0.6450\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6429 - val_loss: 0.6340 - val_accuracy: 0.6446\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6303 - accuracy: 0.6425 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6348 - accuracy: 0.6425 - val_loss: 0.6374 - val_accuracy: 0.6450\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6425 - val_loss: 0.6323 - val_accuracy: 0.6446\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6425 - val_loss: 0.6328 - val_accuracy: 0.6446\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6332 - accuracy: 0.6448 - val_loss: 0.6322 - val_accuracy: 0.6446\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6434 - val_loss: 0.6318 - val_accuracy: 0.6446\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6405 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6425 - val_loss: 0.6365 - val_accuracy: 0.6450\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6441 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6356 - accuracy: 0.6423 - val_loss: 0.6327 - val_accuracy: 0.6446\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6322 - accuracy: 0.6438 - val_loss: 0.6372 - val_accuracy: 0.6450\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6420 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6315 - accuracy: 0.6446 - val_loss: 0.6366 - val_accuracy: 0.6450\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6404 - val_loss: 0.6323 - val_accuracy: 0.6450\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6320 - accuracy: 0.6423 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6404 - val_loss: 0.6307 - val_accuracy: 0.6442\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6345 - accuracy: 0.6418 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6311 - accuracy: 0.6427 - val_loss: 0.6387 - val_accuracy: 0.6450\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6329 - accuracy: 0.6448 - val_loss: 0.6394 - val_accuracy: 0.6450\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6414 - val_loss: 0.6334 - val_accuracy: 0.6450\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6409 - val_loss: 0.6320 - val_accuracy: 0.6450\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6430 - val_loss: 0.6336 - val_accuracy: 0.6446\n",
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6436 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6317 - accuracy: 0.6430 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6398 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6427 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 756/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6389 - val_loss: 0.6358 - val_accuracy: 0.6450\n",
      "Epoch 757/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6320 - accuracy: 0.6411 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6425 - val_loss: 0.6360 - val_accuracy: 0.6442\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6326 - accuracy: 0.6446 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6429 - val_loss: 0.6357 - val_accuracy: 0.6442\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6434 - val_loss: 0.6353 - val_accuracy: 0.6442\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6420 - val_loss: 0.6334 - val_accuracy: 0.6442\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6457 - val_loss: 0.6333 - val_accuracy: 0.6446\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6452 - val_loss: 0.6397 - val_accuracy: 0.6450\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6357 - accuracy: 0.6450 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6405 - val_loss: 0.6322 - val_accuracy: 0.6454\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6441 - val_loss: 0.6343 - val_accuracy: 0.6450\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6389 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6420 - val_loss: 0.6377 - val_accuracy: 0.6450\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6328 - accuracy: 0.6418 - val_loss: 0.6317 - val_accuracy: 0.6446\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6434 - val_loss: 0.6383 - val_accuracy: 0.6446\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6400 - val_loss: 0.6357 - val_accuracy: 0.6446\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6420 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6443 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6463 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6407 - val_loss: 0.6347 - val_accuracy: 0.6446\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6421 - val_loss: 0.6343 - val_accuracy: 0.6446\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6445 - val_loss: 0.6327 - val_accuracy: 0.6446\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6411 - val_loss: 0.6440 - val_accuracy: 0.6450\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6436 - val_loss: 0.6330 - val_accuracy: 0.6442\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6427 - val_loss: 0.6355 - val_accuracy: 0.6446\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6296 - accuracy: 0.6450 - val_loss: 0.6322 - val_accuracy: 0.6442\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6438 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6321 - accuracy: 0.6420 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6334 - accuracy: 0.6418 - val_loss: 0.6325 - val_accuracy: 0.6442\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6409 - val_loss: 0.6370 - val_accuracy: 0.6446\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6356 - accuracy: 0.6430 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6327 - accuracy: 0.6400 - val_loss: 0.6366 - val_accuracy: 0.6450\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6350 - accuracy: 0.6398 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6379 - val_accuracy: 0.6446\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6353 - accuracy: 0.6405 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6425 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6386 - val_loss: 0.6376 - val_accuracy: 0.6450\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6423 - val_loss: 0.6355 - val_accuracy: 0.6450\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6441 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6327 - accuracy: 0.6416 - val_loss: 0.6373 - val_accuracy: 0.6442\n",
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6441 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6421 - val_loss: 0.6328 - val_accuracy: 0.6442\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6446\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6438 - val_loss: 0.6328 - val_accuracy: 0.6442\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6318 - accuracy: 0.6445 - val_loss: 0.6371 - val_accuracy: 0.6446\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6309 - accuracy: 0.6411 - val_loss: 0.6334 - val_accuracy: 0.6442\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6427 - val_loss: 0.6379 - val_accuracy: 0.6446\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6418 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6353 - accuracy: 0.6448 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6398 - val_loss: 0.6374 - val_accuracy: 0.6450\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6448 - val_loss: 0.6425 - val_accuracy: 0.6446\n",
      "Epoch 810/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6434 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 811/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6322 - accuracy: 0.6441 - val_loss: 0.6322 - val_accuracy: 0.6442\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6470 - val_loss: 0.6417 - val_accuracy: 0.6450\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6429 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6432 - val_loss: 0.6340 - val_accuracy: 0.6438\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6438 - val_loss: 0.6360 - val_accuracy: 0.6446\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6434 - val_loss: 0.6458 - val_accuracy: 0.6450\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6413 - val_loss: 0.6321 - val_accuracy: 0.6442\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6396 - val_loss: 0.6381 - val_accuracy: 0.6446\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6398 - val_loss: 0.6350 - val_accuracy: 0.6446\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6310 - accuracy: 0.6421 - val_loss: 0.6368 - val_accuracy: 0.6446\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6331 - accuracy: 0.6432 - val_loss: 0.6393 - val_accuracy: 0.6450\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6395 - val_loss: 0.6417 - val_accuracy: 0.6442\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6363 - accuracy: 0.6430 - val_loss: 0.6409 - val_accuracy: 0.6446\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6429 - val_loss: 0.6364 - val_accuracy: 0.6446\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6418 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6414 - val_loss: 0.6417 - val_accuracy: 0.6446\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6312 - accuracy: 0.6432 - val_loss: 0.6357 - val_accuracy: 0.6446\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6393 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6398 - val_loss: 0.6354 - val_accuracy: 0.6438\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6320 - accuracy: 0.6395 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6414 - val_loss: 0.6369 - val_accuracy: 0.6438\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6434 - val_loss: 0.6332 - val_accuracy: 0.6438\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6420 - val_loss: 0.6384 - val_accuracy: 0.6442\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6306 - accuracy: 0.6438 - val_loss: 0.6320 - val_accuracy: 0.6442\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6318 - accuracy: 0.6420 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6308 - accuracy: 0.6445 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6306 - accuracy: 0.6452 - val_loss: 0.6339 - val_accuracy: 0.6442\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6423 - val_loss: 0.6359 - val_accuracy: 0.6446\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6404 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6404 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6454 - val_loss: 0.6365 - val_accuracy: 0.6438\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6430 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6326 - accuracy: 0.6382 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6373 - val_loss: 0.6390 - val_accuracy: 0.6446\n",
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6414 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6285 - accuracy: 0.6436 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6413 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6413 - val_loss: 0.6395 - val_accuracy: 0.6446\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6430 - val_loss: 0.6403 - val_accuracy: 0.6446\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6322 - accuracy: 0.6421 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6427 - val_loss: 0.6429 - val_accuracy: 0.6446\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6438 - val_loss: 0.6312 - val_accuracy: 0.6442\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6395 - val_loss: 0.6337 - val_accuracy: 0.6442\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6327 - accuracy: 0.6480 - val_loss: 0.6482 - val_accuracy: 0.6450\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6353 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6446\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6409 - val_loss: 0.6379 - val_accuracy: 0.6442\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6329 - accuracy: 0.6430 - val_loss: 0.6352 - val_accuracy: 0.6450\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6432 - val_loss: 0.6348 - val_accuracy: 0.6438\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6425 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 864/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6421 - val_loss: 0.6390 - val_accuracy: 0.6442\n",
      "Epoch 865/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6322 - accuracy: 0.6416 - val_loss: 0.6353 - val_accuracy: 0.6442\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6313 - accuracy: 0.6432 - val_loss: 0.6356 - val_accuracy: 0.6438\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6423 - val_loss: 0.6325 - val_accuracy: 0.6438\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6454 - val_loss: 0.6341 - val_accuracy: 0.6438\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6432 - val_loss: 0.6409 - val_accuracy: 0.6446\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6409 - val_loss: 0.6375 - val_accuracy: 0.6442\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6363 - accuracy: 0.6418 - val_loss: 0.6378 - val_accuracy: 0.6442\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6336 - accuracy: 0.6423 - val_loss: 0.6352 - val_accuracy: 0.6442\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6409 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6438 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6325 - accuracy: 0.6414 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6430 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6328 - accuracy: 0.6439 - val_loss: 0.6370 - val_accuracy: 0.6446\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6442\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6438 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6404 - val_loss: 0.6374 - val_accuracy: 0.6442\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6334 - accuracy: 0.6420 - val_loss: 0.6341 - val_accuracy: 0.6442\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6321 - accuracy: 0.6436 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6434 - val_loss: 0.6426 - val_accuracy: 0.6442\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6322 - accuracy: 0.6443 - val_loss: 0.6331 - val_accuracy: 0.6446\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6438 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6441 - val_loss: 0.6333 - val_accuracy: 0.6446\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6313 - accuracy: 0.6452 - val_loss: 0.6364 - val_accuracy: 0.6442\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6315 - accuracy: 0.6471 - val_loss: 0.6349 - val_accuracy: 0.6450\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6405 - val_loss: 0.6325 - val_accuracy: 0.6438\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6319 - accuracy: 0.6434 - val_loss: 0.6364 - val_accuracy: 0.6442\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6302 - accuracy: 0.6464 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6427 - val_loss: 0.6377 - val_accuracy: 0.6446\n",
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6316 - accuracy: 0.6427 - val_loss: 0.6362 - val_accuracy: 0.6442\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6314 - accuracy: 0.6421 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6340 - accuracy: 0.6421 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6326 - accuracy: 0.6425 - val_loss: 0.6365 - val_accuracy: 0.6446\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6416 - val_loss: 0.6373 - val_accuracy: 0.6450\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6452 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6325 - accuracy: 0.6409 - val_loss: 0.6412 - val_accuracy: 0.6446\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6343 - accuracy: 0.6396 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6446\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6420 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6311 - accuracy: 0.6411 - val_loss: 0.6413 - val_accuracy: 0.6446\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6308 - accuracy: 0.6427 - val_loss: 0.6365 - val_accuracy: 0.6442\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6429 - val_loss: 0.6415 - val_accuracy: 0.6446\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6427 - val_loss: 0.6359 - val_accuracy: 0.6450\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6295 - accuracy: 0.6418 - val_loss: 0.6396 - val_accuracy: 0.6446\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6305 - accuracy: 0.6429 - val_loss: 0.6382 - val_accuracy: 0.6446\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6330 - accuracy: 0.6411 - val_loss: 0.6367 - val_accuracy: 0.6446\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6307 - accuracy: 0.6464 - val_loss: 0.6385 - val_accuracy: 0.6446\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6300 - accuracy: 0.6432 - val_loss: 0.6372 - val_accuracy: 0.6442\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6314 - accuracy: 0.6438 - val_loss: 0.6400 - val_accuracy: 0.6446\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6345 - val_accuracy: 0.6442\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6423 - val_loss: 0.6394 - val_accuracy: 0.6442\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6430 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 918/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6445 - val_loss: 0.6413 - val_accuracy: 0.6446\n",
      "Epoch 919/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6323 - accuracy: 0.6427 - val_loss: 0.6420 - val_accuracy: 0.6446\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6307 - accuracy: 0.6423 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6407 - val_loss: 0.6381 - val_accuracy: 0.6446\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6330 - accuracy: 0.6450 - val_loss: 0.6436 - val_accuracy: 0.6446\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6413 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6309 - accuracy: 0.6452 - val_loss: 0.6397 - val_accuracy: 0.6438\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6300 - accuracy: 0.6434 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6402 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6443 - val_loss: 0.6340 - val_accuracy: 0.6438\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6306 - accuracy: 0.6414 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6434 - val_loss: 0.6406 - val_accuracy: 0.6442\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6438 - val_loss: 0.6354 - val_accuracy: 0.6446\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6404 - val_loss: 0.6366 - val_accuracy: 0.6442\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6443 - val_loss: 0.6351 - val_accuracy: 0.6438\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6302 - accuracy: 0.6450 - val_loss: 0.6355 - val_accuracy: 0.6442\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6457 - val_loss: 0.6428 - val_accuracy: 0.6446\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6442\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6404 - val_loss: 0.6388 - val_accuracy: 0.6442\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6427 - val_loss: 0.6415 - val_accuracy: 0.6442\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6438 - val_loss: 0.6356 - val_accuracy: 0.6442\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6423 - val_loss: 0.6421 - val_accuracy: 0.6446\n",
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6389 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6325 - accuracy: 0.6427 - val_loss: 0.6323 - val_accuracy: 0.6442\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6445 - val_loss: 0.6446 - val_accuracy: 0.6446\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6421 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6416 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6434 - val_loss: 0.6417 - val_accuracy: 0.6446\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6427 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6413 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6284 - accuracy: 0.6432 - val_loss: 0.6335 - val_accuracy: 0.6442\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6343 - accuracy: 0.6420 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6308 - accuracy: 0.6432 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6505 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6288 - accuracy: 0.6455 - val_loss: 0.6364 - val_accuracy: 0.6446\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6324 - accuracy: 0.6457 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6310 - accuracy: 0.6430 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6318 - accuracy: 0.6420 - val_loss: 0.6360 - val_accuracy: 0.6446\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6327 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6446\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6301 - accuracy: 0.6430 - val_loss: 0.6383 - val_accuracy: 0.6446\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6344 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6438\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6318 - accuracy: 0.6430 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6316 - accuracy: 0.6429 - val_loss: 0.6401 - val_accuracy: 0.6450\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6315 - accuracy: 0.6423 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6432 - val_loss: 0.6409 - val_accuracy: 0.6450\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6439 - val_loss: 0.6416 - val_accuracy: 0.6442\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6333 - accuracy: 0.6414 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6461 - val_loss: 0.6372 - val_accuracy: 0.6442\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6425 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6321 - accuracy: 0.6448 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6461 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6304 - accuracy: 0.6441 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6304 - accuracy: 0.6441 - val_loss: 0.6330 - val_accuracy: 0.6442\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6411 - val_loss: 0.6357 - val_accuracy: 0.6442\n",
      "Epoch 972/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6298 - accuracy: 0.6427 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 973/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6425 - val_loss: 0.6441 - val_accuracy: 0.6446\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6299 - accuracy: 0.6466 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6407 - val_loss: 0.6394 - val_accuracy: 0.6446\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6282 - accuracy: 0.6457 - val_loss: 0.6408 - val_accuracy: 0.6446\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6438 - val_loss: 0.6381 - val_accuracy: 0.6442\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6311 - accuracy: 0.6407 - val_loss: 0.6396 - val_accuracy: 0.6446\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6309 - accuracy: 0.6425 - val_loss: 0.6374 - val_accuracy: 0.6442\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6446 - val_loss: 0.6390 - val_accuracy: 0.6446\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6324 - accuracy: 0.6413 - val_loss: 0.6401 - val_accuracy: 0.6446\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6332 - accuracy: 0.6427 - val_loss: 0.6403 - val_accuracy: 0.6450\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6331 - accuracy: 0.6429 - val_loss: 0.6398 - val_accuracy: 0.6446\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6348 - accuracy: 0.6427 - val_loss: 0.6362 - val_accuracy: 0.6438\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6330 - accuracy: 0.6436 - val_loss: 0.6399 - val_accuracy: 0.6442\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6327 - accuracy: 0.6413 - val_loss: 0.6382 - val_accuracy: 0.6446\n",
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6324 - accuracy: 0.6439 - val_loss: 0.6418 - val_accuracy: 0.6446\n",
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6302 - accuracy: 0.6425 - val_loss: 0.6344 - val_accuracy: 0.6442\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6301 - accuracy: 0.6482 - val_loss: 0.6367 - val_accuracy: 0.6446\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6317 - accuracy: 0.6446 - val_loss: 0.6504 - val_accuracy: 0.6446\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.64 - 0s 20us/sample - loss: 0.6299 - accuracy: 0.6459 - val_loss: 0.6439 - val_accuracy: 0.6442\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6332 - accuracy: 0.6414 - val_loss: 0.6516 - val_accuracy: 0.6446\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6303 - accuracy: 0.6434 - val_loss: 0.6329 - val_accuracy: 0.6433\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6301 - accuracy: 0.6443 - val_loss: 0.6404 - val_accuracy: 0.6438\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6292 - accuracy: 0.6432 - val_loss: 0.6359 - val_accuracy: 0.6438\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6322 - accuracy: 0.6411 - val_loss: 0.6445 - val_accuracy: 0.6442\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6316 - accuracy: 0.6409 - val_loss: 0.6392 - val_accuracy: 0.6446\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6296 - accuracy: 0.6466 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6307 - accuracy: 0.6425 - val_loss: 0.6408 - val_accuracy: 0.6446\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6319 - accuracy: 0.6438 - val_loss: 0.6414 - val_accuracy: 0.6446\n"
     ]
    }
   ],
   "source": [
    "# specify network layers\n",
    "binary_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (13, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# compile and fit network\n",
    "binary_ann.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "history = binary_ann.fit(X_train, y_train, epochs = 1000, batch_size = 128, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FPX5wPHPk5uQhAAhHOEIyA1igIiCqByCoAiKeKAtgreVoq1WsR5Vqq1t1Z9arReCt9RbVNQqooIHEOSQ04Q7gJBwhjPX8/tjJssm2c1uQpYAed6v177YmfnOzHd2wjzzPeY7oqoYY4wxFQmr6QwYY4w59lmwMMYYE5AFC2OMMQFZsDDGGBOQBQtjjDEBWbAwxhgTkAULU2NEJFVEVEQigkg7VkTmHI18mdATkX4ikl3T+TDBs2BhgiIi60QkX0SSysxf5F7wU2smZ6XyUldE9orIjJrOy/HEK2jvLfO5rKbzZo4dFixMZawFRpdMiMjJQJ2ay045o4BDwGARaXo0dxxM6eg4kKiqcV6f/9Z0hsyxw4KFqYxXgTFe01cBr3gnEJF6IvKKiOSIyHoRuUdEwtxl4SLyiIjkisga4Hwf674oIltEZJOIPCgi4ZXI31XAs8AS4Moy224hIu+5+douIk95LbtORFaISJ6ILBeRHu58FZG2XuleEpEH3e/9RCRbRO4UkV+BqSJSX0Q+dvex0/3e3Gv9BiIyVUQ2u8s/cOcvFZELvNJFur9RWtkDdPM5zGs6wk3bQ0RiROQ19/h2ich8EWlcid/PJ/e4nxWRL9zf6BsRaeW1vI+7r93uv30CHbPX8ttEZJt7zsd5zT/PPRd57t/C7Ud6HObIWLAwlfEjkCAindyL+GXAa2XS/BuoB7QBzsYJLiUXgeuAYUB3IB2nJODtZaAQaOumGQxcG0zGRKQl0A943f2M8VoWDnwMrAdSgRRgmrvsEuB+N30CMBzYHsw+gSZAA6AVcD3O/6ep7nRL4ADwlFf6V4FYoAuQDPyfO/8V4Dde6c4DtqjqIh/7fBOv0h1wLpCrqj/hBMt6QAugIXCjm4fqcCXwVyAJWITzGyMiDYBPgCfdfT4GfCIiDd31/B0zOL9fPZzzcQ3wtIjUd5e9CNygqvFAV+CrajoOU1Wqah/7BPwA64BzgHuAvwNDgC+ACEBxLsLhONVAnb3WuwH42v3+FXCj17LB7roRQGN33Tpey0cDs9zvY4E5FeTvHmCR+70ZUAR0d6d7AzlAhI/1Pgdu8bNNBdp6Tb8EPOh+7wfkAzEV5CkN2Ol+bwoUA/V9pGsG5AEJ7vQ7wB1+ttnWTRvrTr8O3Od+vxr4HuhWyXOb6h7rrjKfTl7HPc0rfZz7+7YAfgvMK7O9H9zzVdEx98MJZBFe87YBp7vfN7h/Owk1/bdvH+djJQtTWa8CV+BcDF4psywJiMK5gy+xHufOEZyL4sYyy0q0AiKBLW4Vyi7gOZy70WCMwb3bVdXNwDc4d9rgXNTWq2qhj/VaAKuD3EdZOap6sGRCRGJF5Dm3+m0P8C2Q6JZsWgA7VHVn2Y24+f0OuFhEEoGhJcfiI20WsAK4QERicUpCb7iLX8UJftPcap9/ikhkJY4nSVUTvT4rvJZ5zpuq7gV24JzPZpQ+j3D4nPs9Ztf2MudkP04gArgYp4S13q326l2J4zAhYMHCVIqqrsdp6D4PeK/M4lygAOfCX6IlsMn9vgXnAuK9rMRGnJKF9wUrQVW7BMqTW0feDrhLRH512xBOA0a7Dc8bgZZ+GqE3Aif52fR+nCqUEk3KLC87ZPNtQAfgNFVNAM4qyaK7nwZuMPDlZZyqqEuAH1R1k590cLgqagSw3A0gqGqBqj6gqp2BPjhVfmP8b6ZSPOdNROJwqt82u59WZdKWnPNAx+yXqs5X1RE4NwsfAG9VMd+mmliwMFVxDTBAVfd5z1TVIpz/1A+JSLzbCPpHDrdrvAVMEJHmbt30RK91twD/Ax4VkQQRCRORk0Tk7CDycxVOlVhnnKqfNJx67licu/R5OIHqYXG618aIyBnuupOB20WkpzjaejXeLgKuEKdhfghOG0xF4nGqVna5dfl/KXN8nwL/cRvCI0XkLK91PwB6ALdQvsRW1jScKrybOFyqQET6i8jJbklmD07gLgqwrWCdJyJ9RSQKp+1irqpuBGYA7UXkCrex/TKc8/BxEMfsk4hEiciVIlJPVQvcY6mu4zBVZMHCVJqqrlbVDD+Lfw/sA9YAc3AuZlPcZS/gVJMsBn6ifMlkDE411nJgJ07dfYVdYEUkBrgU+Leq/ur1WYtTLXOVG8QuwKnv3wBk4zTOo6pvAw+5+czDuWg3cDd/i7veLpwG3lI9eXx4HKcrcS5OZ4DPyiz/Lc4FfCVO/fytJQtU9QDwLtDax+9SinsR/gGn9ODdvbUJzm+2B6eq6hvcQO32Zno2QP53SennLP7otewNnOC3A+iJ29tMVbfjlGBuw+kYcAcwTFVzAx1zAL8F1rnVeTdSugOAqQGiai8/MuZYICL3Ae1V9Zi6MIrIS0C2qt5T03kxNedEeJDImOOeW211Dc4dtTHHHKuGMqaGich1OI3Bn6rqtzWdH2N8sWooY4wxAVnJwhhjTEAnTJtFUlKSpqam1nQ2jDHmuLJgwYJcVW0UKN0JEyxSU1PJyPDXm9MYY4wvIlL2CXyfrBrKGGNMQBYsjDHGBGTBwhhjTEAWLIwxxgRkwcIYY0xAFiyMMcYEZMHCGGNMQBYsjDGmhmVty+PHNcG++r1mnDAP5RljzPHqnMec8SPXPXx+DefEPytZGGOMCciChTHGmIAsWBhjjJeDBUU8+81qCouKazorPuUXFnPRf77jh9VHt43DgoUxxnh5elYWD3+6kncWZFeYrqComD0HC6p13wVBBKgtuw+wcMMubn97cbXuOxALFsaYE5Kqct4Ts/lw0aZKrbfvUBEAew8VVphu/Bs/0e3+/1U5f74cLCgqNy937yHW5e7zTAsCQGFxMUXFyk8bdlZrHvyxYFHb/boU3r0WCg/VdE6Ob4WH/P+G+fvh7bGwZTEUVXwBqpSCA1CY7zsvBQdLzzu0t/r2W0WpEz/h7vd/PuLtTF+8mWe+Xh0w3YGCIpZv2cMt0xZVavthzrWY4gBvEf182VYA2t/9abllf5+xgtSJn1Rqv+DkGSDvYAGTZ69hxs9b6PP3r+j3yNeeNIcKnTRb9xzi2W9WM/I/3zN/3Y5K76uyrOts/j748T/Bp9+5DuqnVm8e9m2H8AiIqVe9283bCtFxEFXXf5pZfwMthm0rof1giKxTvXnYsRYSUiAiqvq2uXMdxDeFiOgj207OKmjUAfZsgahY379/USHsWg/FhU5aoFgPX1A85jwOCPS6DvZvh8QWh5flZsGy951Pw7ZwyuXl95O/H/bnQmJL33lVIHeVJw8o8M3DUKc+nHZD6e3MeQyS2kP6NfDrz85vnzEFjYxF2g9x8rBnMzRIhdxMZzr3F2fbqs5xiEDTNMjbDJsXQftznc+ONbD8Q+e327sVmp4Cderz7M8QGymM6dseWvSC1V/ByaNKHUIsBzmU8SrFF/wNCQ9HpOyP6NvL36/j54zZPHJhO2h5OhPeXAjATf1OqnC9XfudKqLIcP/7KSgqZm3uPhLCC2iS/Tl0u5Rw9+SW1Agt3bSbpvViaBjn++8t301YVKyedZ/7do2zrLCYqIgwioudwBMWJlz90nzSWiQyYWC7ctv6ZMkWxp3Rmns+WMqHizaXWvb+wmwu6t6cgwWHq6qWZO8CIHvnfk5NbVDh73GkTph3cKenp2uVXn60Lxf+VfEfnTGmCsIi4cw/ws71sGRaqUUbGw+gxdav2NV8AJEHc6mbu8SzbE98WxJOuQCyM9ja8bfc/GE270RPchb2GMPgH7rQIyyTO5sv5z/1/sDdyd87Nzt7t0KP38JJA9m2YSVbPvorkr+XMBG6duoMh/bA2RMhtgFEJ5AjDbno728yIGwhkyJfdrZ/xi28tKcH7Rf9gzaJYTQZ/gDtX9xLq3oRfHHHENieBQ1aQ96v3PvIY3QPy+Kxwkt475x9rPz6TSKHPkTvvV/y6rfLmVI0lHdvG8769av5wzvLaNn2ZJ66sBVn/Gs2+USw6p6+kJ8HkbFM+8cNZGkKjdKGckP4R6xasYQr8yZQhBAvB4iigIbk8Xq7r8jqMoHxH6zngEbTqV1b1mSt4E9DuzD0rD5VOk0iskBV0wOmq/XBQtW5awzG4mkwfTykpMPVn1V+X/78Ncn5997c6tumFsODyUFuV+CvDSGxFfx+QfXlAWD2Y9DydGhVtT9kn757HFJ6QuqZVd/G/h3w7T/h7DudO+XYhtDpgvLpNv0EC191SjH9/syT323lya8yublfW/4wqL1XQvfuNetL5y799JtKb0fCKCgq5ptVWxnYMbn8XfXqWbBtOfS+meVb9jD8qTl8dstZtE2Oc5bvWAsZL1LU/x7yNZLvVucy/s2FzLv7HBKiD1cQ5D96MlH7t0D61U4pY83XsPdXlhe34iTZRHSzrk711f7tcO2XMH8yu1sMYP3sN3k9pw3/KPyHc9jShBT91f/vFx7l/L/R0g2yRYQhyR0J27a8gh//+LJL65Io+yC5C2xbdtT2m6/hREn5NgxfdiR0psEff6jSfoINFlYNJQLhkcGljY531wkLfp1g3PqzcxdWndsEaHUGNOse3HZ//xPEJFZ/HvrdWelV1uXuIy4mgiQ/xX7O+tMRZgqIb8yUeuNZ+dkW/jnqOnYfKODRj1cRGR7GvcM6U1hUzLiX5nNTv5PoM+IpVJWp363jy192UEiE8zv5+q06DIEOQ1j1ax5tGtUlMjyMOZm5rNqaR0FRMQ9/upLnf9uTwV2alF6v/WDnA7y7aCuFRPBV5g7W78onMTaSk1PaMnrdCLY+MZ/snQfclcJYsCGP3ic15LpXMpgwsB037PgLTWPyqf9rR+4Y0oFuIxO59uX5fLliGwCfDj+TTk0TAMhYt4PclN/zyGeryNp2EQDL5EGSZDc/F7dhwcQzmZOZQ4OwfXT+aJgTIC6eDJ1HkLFuB//+KovnLm7N/73+ATnZWazRZizStoRnC101k6m3XMS2/z1K0/XTuf/AZewknixNoU/YUhrWb8id/Zrwx/dWspN46pNHE9nBqPBvGVMwkX+1nEvvrW96fp5/F17IkuI23Nw3hefmbKCB5PFQ5JTDv1/nETy8MpmJxS94Zs0vbs+pYb8E/Scxo9u/GbLiTsIK9peanyhO43LBtpX4+9+RE9eBJ3b24cHIqQC8XDSEq8Ird0NZpML+iHrEFzlVSyWBYos2oFDDaRGW43fdh3LP4tFK7a3yLFhURkndf5B1rUHzV099pMbNCD5tw+qpisvJO0SjeOci/31WLj+s2c5tgzv4Tb9rfz7xMZGeul6Ai5/5nu378j1DH6gqre+awR8Htad/h2QueGoOn916Jh2bJLAt7yDJ8THs3JdPQh1nO6rKzv0FxEVHcLCwiOwdByhWJT4mgi27D3J6m4YATPrYufsd3aslF/3ne8/+7x3WmdU5+5idmcva3H3MuXMAG3cc8KQHeGJmJtMXb+aWge3o2ao+T32VxaQLuxAdEc6G7fs59/FvGdWzOX8+rxO/eXEuAOef3BSANV49W6bN28DE934mtWEsf7voZPq0TWJ/vlPS/XTpryzc4Fw4vr69HwvWl+/1snHnfj79cAuzM3PZe6iQHSSw4yCQlcuaV/cy584BnkABMPSJ2bx7U296tmrAqGfL34ku1TZOewgwa2s0497dAsDsO7bQMC6KomKl+EAB93+0jKWb9vD49wk8t6EZ0MyzjaJiZTFtufrdjSzaOBgYXGofbxX1h1yIy+vAe8XJpZb9p+hC55ysv4CWDS5jw47SF+4vvgVwfsfXi84BoElCDM/27smzP33Hs/Qvlb4xOxgd8RVPFo6kmDAaxUfTYG8mz0c+xstF5zKlaCgXhc1mkyYxb15DYLJn3UFhGXQLW0MTdvBG0UAWauk2hhgO8fuI93mucBgj0rrw2o/rKexxNb1Pashfpi3iLwVjPGkvCf+aIWHzmV7UmzZhv/J44UjUR/+ijvkbOCtsMc8XDcNTWvXSQ36hc9h6XisaxLqYKwDodvB59hAX8mBh1VCVse47eOk8tPlpyLXV22XOl6JipaComP98vZqIMGFAx2S6plTcCF5crIS5F95d+/PZvOsgHZvEe+aB0+j21FeZ/LZ3qufC/nbGRto0qkuPlvUBEHEuuqpOo1xxsSICHyzaRP3YKBRYvnkPI3uk0LSe0yi+JHsXw5/6jj+d24HzTm5Kf7cHx3O/7cneg4V0aBJPhybxLN20m4Ii5b4Pl7Ly1zyuPqM1913QmW17DrL3UCEDHv0GgF6tG/DHQe1pmxxH+oNfAjD8lGZMX7yZBnWjeOzSUxg7dX6p48+45xzezsjmH5+t5JQWiSzeuKvcb/S3i05mw479PPuN7x41308cwNTv1vLC7LX0btOQN68/nVmrtjGuzL5KiDi1mXWjwklPbUDjhGjeynD66P+u30n8p0zPnavPaM2fz+vI3LU7uHLy3FLLerVuwLy15Xu2XNQ9hfcXlu8Cell6C/6bsdFnvgAa1o1i+77yPaYmj0nn2leC///SoXE8q7bmBZ2+Nnj8sjRu/W/pnlZJcVHk7vXRQy0EmrKdfUSzhzgaxUcz/+5zqrQda7OoBFXlmW9W88/PVhEbFc7+/CKu6dsagIUbdpIYG0V+YTHRm+fyYvG9LA3rwJsnv0jDur57+OzPL2JN7j7aJseRuTWPIoXWDWOpVyeSjTsPsPLXPFISY4iJDOekRnEk1Ikkc2seq7bm0Tg+BhGIDA9j+uLN5bZdPzaSgZ0as33vITo2TSAn7xBN68Ww91AhU79bB0CDulFc0rO5p0cGwJjercg7WEij+GjW5OzjyxVbaZscx3ldm/Dj2h0+L1AlUhLrsGnXAdomx5G1rXwXzJE9UhjQMZkX56z13AlX1kfj+3L58z+wLz+4Olp/oiLCCBNK9Rg5UrcMbMcTMzOrbXvHoqjwME+vHl9OalSX1Tn7/C6vquT4aLblha7b9tg+qagqL/+wvtq3/c+Lu3HHu0sCJzwKZt/RnxYNYqu07jERLERkCPAEEA5MVtWHfaS5FLgfp/C7WFWv8FqWAKwA3lfV8RXt60iCxdw127ns+R8DpkuXlbwTPYkFxe24OP8BN4+l03j/nCV3nP6ma8qxkg9TfSYMbMeTRxDQXr66F9/+ksOLc9ZWet1RPZuXe9p56rhT/ZbEvL12zWn85sW5AYOVP/ExEeQd9N9BZWyfVJITovnnZ6t8Lm+SEMOvew76XAbQuWkCy7fs8bls0ogu3PfhMq44rSUJMZF+S6rBWvyXwZz64JfkFxUTESYUFgf/n/RIRqut8QZuEQkHngYGAdnAfBGZrqrLvdK0A+4CzlDVnSKSXGYzfwW+CVUeS5TcVU8ek86Z7ZPIyTtE8/qxLNywkxdmr+H+C7qwY38+cQfbwkuTaNX7Yp5p3oOhbh20t7yDBWRt20tai0Sf/cjHTJnHii17eP3a03jumzWkJMZwoKCIrXsO8dilp1CkysYd+2mbHO9Zp+Thnp/uHcTmXQdISaxD/bpOaeftBRsZdnIz6kSFs2zzblon1SVt0hfc1O8kpsxZS+ukurx7Ux8WrN/J96u3c1H3FDo0ObztYf+ezdJNe7jytJbszy9iWLemfJe1ndiocM5om0Tz+nV4Y94Gnvl6NS9f3Yvk+GhPAynA6py93PHOEqc9YNdB/jGqGy98u4aLuqewNncf6an1S7UHlDizXRKLNuwiIlzYuf/wkAlhAlPH9eKqKfN8nquS/6AAN5zVplTpqazGCdFEhodxz/mdaF4/ltiocG797yKWZO8ulzbQtqrLnDv7M+Kp78pVDf3m9Ja89uMG6sdGlvo9fHlydHfW5OwlY91O5mQ5Pd0aJxzZMycN60Zx77DOVQoWj1xyCnMyc0tddPt3KPtf2Td1G0hOa9OA2Zm+e+19P3EAfR7+qtz8+rGRPHjhydz8xk+eecO6NeW6M9sw4unvPPOaJMT43O6Ege1YvW0vn/y8xTPvpXGneqo246MjmHHLmXy1citXv+TciM687WwGutWkJc9O1KsTyZ1DOnqCxaiezWkYF8Vz3wT+ewoTmHxVOk0S6lCvTiRL7h9MTt4hEmIiKSguZvHGXVzz8uGb4DuHdCSlfh26t0gkv6iY617JID6mmjul+BHKBu5eQJaqrgEQkWnACMC7T911wNOquhNAVT0tcSLSE2gMfAYEjHpHYnZWLt2a1+Oczo0BaF7fKc51b1mf/1zZE4DkhBggAW7PIim2IUPDfD/8Hh8TSXe33t+XqWNPRVWJCA/j0UtPKbc8AkoFCoC/jzyZlg1iaVA3igZeVV9REWFceVorz3TJflc9OITIsDBuG9SeMBHCwoSz2jfirPaNyu1v4pBOTHxvCROHdvT80Q3s1LhUmjuHdOSWge2IiQwvt/5JjeJ496bS3WKfvrJHqel3buxNu8bxnPKA087jfRe091AhV06e62lbWHjfYOrVieStG3rzy9Y8Zq7YSssGsXy5Yhubdh1gTO9UZq7YRlqLRG49px2DuzTh4me+p21yHOd2acy8tTtoUDeKJy7v7jO//romJNRxjn1kjxTe+8lpG7j/gs6881M2SzeVv7OcPCadzbsPeAJXWX8d0YV73WXe7QrN68cy988D2XWggIc/XUmbRnVplxzP2e0bcd+wLkSGC6Nf+JEf1zg3ML7u0JvVi2H4Kc2YtWobc7Jyadkglot7NOfu95eWSvfUFd0Z/8ZCT/Whrzr2EvXLVKneNqg9j37h9CQ6v1tTvl65rcIqwteuPY3/+/IXPlmyxW+aMb1b8YpXddCt57Sjd5uG3HBWG67p25pef5sJwKL7BpE26QsAwsOEZol1uGNIh1KlgzeuPY0+bZ0u5ze/UXo/p7RI5L5hnT0dEhr7CRaN4qPp3DTeEywaJ0TTzyvILbxvEAADOjbmlweHeqqH2yTVZewZqVya3oLsnQe4uX/bUtv9+8iTiQwP47usXJZu2sPM285m1sptPPjJCpLiomnRoI6nulbd7ZeIiQwvVZ00sFNjsh4ayje/5BAfE0mv1qUfvPvyD2dTUHx0BjwMZbBIAbxb3rKB08qkaQ8gIt/hVFXdr6qfiUgY8CjwW2Cgvx2IyPXA9QAtW1a9R1F+YTGJsUE+YRxX/oJbGU6vn8r1phrdq3LHFh3hXCTDgthP33ZJzLlzQMB0vi68wUp3nyyNCBOGpzUrtSwuOoJRPZuzeOMurjitJfXci3av1g3o1boBvzndCYZ3DCn0DLL28tW9POv3bFWf+XefQ0KdCM9xV6RHq/oszt7NNX1be+6i77+gM6NPa0nDulFckt6C01s3pEerRNomx9MqqS7jps5ndK+WXJjWjGtezuDU1PqeG4stuw+ybc8h3v3JqYYZkdaMC9NS6N8xmfO7NSMmMozI8LBSjdAR4WEkxUXzyCXlbxYApl3fu9T0uofP590F2dzmDhxX8hv175DMnDv7k5JYx2cpdli3ZnRvWZ/G8dHs2JdPckKMJ1h0aprAii17PA3gSXGl//5/P7Admdv2Mn3xZi5Lb8EVvVqWaowPDxOKvKpJ2ibH8fQVPfhkySdceZrvv9dJI7ry4aLN7D5QwNe39yM1yeldeNd5nTzHWWLi0I48/OlK6kY553RAx2RPsPjiD2fRrvHhG6o3rj2NK9y8lfwOo9Kb8+Oa7fyu/0moOk9xFxSVrtZpFBfFkK5NefqKHtz8xk+EuesmxUWTu/cQEeGHbwijIg5//+r2fp7v9wzr7Pk+ulcL3py3kUh3vbdv6MOBgiIa1I3ipEZxnHdyU5olOp1B8g4WcPL9/wuqOjgiPKzcDVyJsDAhOqzq/zcrI5TBwteVquxPEwG0A/oBzYHZItIV+A0wQ1U3VjQkgKo+DzwPTptFVTNq1fdHR9bfzvM5v+QMV/Qfp260/z/Vkh5dwbhraCeGn9KMtBaJvPbjeg4VFjM8LYXoiHAud4PypaceHqqjf4dk3rmxNz1a1icsTFj6wLmltnfnkI6AUyUyec5a/nbRyZ68NvDTAaIqLu7ZnPs+XMq+/CJPKQgOl4IBnrg8jWaJdbjEq0tsintxSnbvrqeOO5VDBUXMXbuDFVv2cFO/k7j6jNae3nLf/qk/kRHO9z+d24EOTeLp2zaJpZsPV93dPrg9Azs1ZugTs8vlM1DdeWxUOLsPFJTqKu3Lb05vxcOfrvSck5Lf8o+D2pcKFAB92iZ5Sk0lI3skxETy/JjDFRIrJg1hx/589h4sZPmWPazcksegzs6zLnWinIt7SY6++MNZPnuQBfL3kd34+8hunuk6UeHUiTp8IS8JFAB1o5y/kUt6Nq/0fmpKKINFNuA1QA7NgbLde7KBH1W1AFgrIqtwgkdv4EwR+R0QB0SJyF5VnRiqzFbzkxOmEqr7sZWKREWEeVXXDQ1qnfQgxty567xOjB/QtsKgdqT6dUjmk5+3kOCnjnpEWkrAbZS0Jcxb6zyzUdI1ukTLhoeDT4sGsZ4qli7NDnfZvrpva2Kjqnac/To04s15GwOWVOOiI1h47yBPYEyOj2HhvYM8paqyhnRtwsWZzblziO9neiLCw0iOjyE5Hto0imPY4Wu658Jd4JaU6teNKlctV93CwoTF9w2mbvTRKRVUh1AGi/lAOxFpDWwCLgeuKJPmA2A08JKIJOFUS61R1StLEojIWCA9lIHC1KwI92IVEeBu81gWHiYVVmWO7tWSNkkVDOgYhEcvPYU/DGpf6m7Vl/l3n1OqisiXcWek8s0v2xhRplrQn/AwYfYd/ZmTlesJFFec1tJTcqnI3D8P9NwQPDC8K+POaB1UabDsBbuiC3hMZLjPNsBglHT42FtBr6pQqBd7dBqmq0vIgoWqForIeOBznPaIKaq6TEQmARmqOt1dNlhElgNFwJ9U9ei+/snJ7FHfpTlsRFoKP2/azR9LjbV0Yvn7yJOPeBsxkeGHx4qqQDAX4hYNYpl5W79K7b9Fg9hS7Wd/uyi4Y/JuYI580x5SAAAdoUlEQVSKCKN9mWqkmpYYG8Xtg9t7nuw3vtlDecCIp+ZQv24UL43rFTixMSYomVvzWLd9P4M6+26cNceGGn/O4nhyYoRLY44t7RrHl2uMNscve1Oe6/itLTfGmNCzYIE1WRhjTCAWLFzBvuLRGGNqIwsWxhhjArJgweHBzIwxxvhmwcJllVDGGOOfBQusgdsYYwKxYOGy9m1jjPHPggVWsjDGmEAsWHhY0cIYY/yxYGGMMSYgCxbY2FDGGBOIBQuXNXAbY4x/FiyAE2WYdmOMCRULFi4rWBhjjH8WLIwxxgRkwcJlbRbGGOOfBQtjjDEBWbDAnuA2xphALFi4xJq4jTHGLwsW2PssjDEmkJAGCxEZIiKrRCRLRCb6SXOpiCwXkWUi8oY7L01EfnDnLRGRy0KZT2efod6DMcYcvyJCtWERCQeeBgYB2cB8EZmuqsu90rQD7gLOUNWdIpLsLtoPjFHVTBFpBiwQkc9VdVco8mptFsYYU7FQlix6AVmqukZV84FpwIgyaa4DnlbVnQCqus399xdVzXS/bwa2AY1CmFcrWRhjTAVCGSxSgI1e09nuPG/tgfYi8p2I/CgiQ8puRER6AVHAah/LrheRDBHJyMnJqcasG2OM8RbKYOHrXr1shU8E0A7oB4wGJotIomcDIk2BV4FxqlpcbmOqz6tquqqmN2pU9YKH1UIZY0zFQhkssoEWXtPNgc0+0nyoqgWquhZYhRM8EJEE4BPgHlX9MYT5BKzrrDHGVCSUwWI+0E5EWotIFHA5ML1Mmg+A/gAikoRTLbXGTf8+8Iqqvh3CPAI26qwxxgQSsmChqoXAeOBzYAXwlqouE5FJIjLcTfY5sF1ElgOzgD+p6nbgUuAsYKyILHI/aaHKK2DDzhpjTAVC1nUWQFVnADPKzLvP67sCf3Q/3mleA14LZd5K7e9o7cgYY45T9gS3ywoWxhjjnwULY4wxAVmwAKuHMsaYACxYuMQe4TbGGL8sWGAFC2OMCcSChcvKFcYY458FC+yhPGOMCcSChcuaLIwxxj8LFsYYYwKyYIE1cBtjTCAWLFxWC2WMMf5ZsMBeq2qMMYFYsHDZQ3nGGOOfBQtArdXCGGMqZMHCZeUKY4zxz4KFMcaYgCxYYA3cxhgTiAWLElYPZYwxflmwwEoWxhgTiAULl1jRwhhj/LJgYYwxJiALFi57Js8YY/wLabAQkSEiskpEskRkop80l4rIchFZJiJveM2/SkQy3c9VocynMcaYikWEasMiEg48DQwCsoH5IjJdVZd7pWkH3AWcoao7RSTZnd8A+AuQjjMo7AJ33Z2hyKu9/MgYYyoWypJFLyBLVdeoaj4wDRhRJs11wNMlQUBVt7nzzwW+UNUd7rIvgCEhzKs1bxtjTAUCBgsRGS8i9auw7RRgo9d0tjvPW3ugvYh8JyI/isiQSqyLiFwvIhkikpGTk1OFLDqsXGGMMRULpmTRBKcK6S23DSLYm3Bf6cpelyOAdkA/YDQwWUQSg1wXVX1eVdNVNb1Ro0ZBZstPZq1oYYwxfgUMFqp6D84F/UVgLJApIn8TkZMCrJoNtPCabg5s9pHmQ1UtUNW1wCp3X8GsW22sycIYYyoWVJuFOi3Av7qfQqA+8I6I/LOC1eYD7USktYhEAZcD08uk+QDoDyAiSTjVUmuAz4HBIlLfrQIb7M4LGXsozxhj/AvYG0pEJgBXAbnAZOBPqlogImFAJnCHr/VUtVBExuNc5MOBKaq6TEQmARmqOp3DQWE5UORue7u737/iBByASaq640gO1BhjTNUF03U2CRipquu9Z6pqsYgMq2hFVZ0BzCgz7z6v7wr80f2UXXcKMCWI/B0xe/mRMcZULJhqqBmA565eROJF5DQAVV0RqowdbdbAbYwx/gUTLJ4B9npN73PnnTCsgdsYYyoWTLAQ9XrEWVWLCeGT3zXFShbGGONfMMFijYhMEJFI93MLTo+lE4YVLIwxpmLBBIsbgT7AJpznH04Drg9lpmqGFS2MMcafgNVJ7nhNlx+FvBhjjDlGBfOcRQxwDdAFiCmZr6pXhzBfR5U1cBtjTMWCqYZ6FWd8qHOBb3CG3sgLZaZqgjVwG2OMf8EEi7aqei+wT1VfBs4HTg5tto42K1oYY0xFggkWBe6/u0SkK1APSA1ZjmqIFSyMMca/YJ6XeN4dzO8enIEA44B7Q5qro8zaLIwxpmIVBgt3sMA97tvqvgXaHJVc1QBrszDGGP8qrIZyn9Yef5TyYowx5hgVTJvFFyJyu4i0EJEGJZ+Q5+woslooY4ypWDBtFiXPU9zsNU85waqk7OVHxhjjXzBPcLc+GhmpSWot3MYYU6FgnuAe42u+qr5S/dmpOdbAbYwx/gVTDXWq1/cYYCDwE3DCBAsrVxhjTMWCqYb6vfe0iNTDGQLkhGIFC2OM8S+Y3lBl7QfaVXdGjDHGHLuCabP4iMM1NWFAZ+CtUGbqaLP2bWOMqVgwbRaPeH0vBNaranaI8lNjxFq4jTHGr2CCxQZgi6oeBBCROiKSqqrrQpqzo8i6zhpjTMWCabN4Gyj2mi5y5wUkIkNEZJWIZInIRB/Lx4pIjogscj/Xei37p4gsE5EVIvKk2K2/McbUmGBKFhGqml8yoar5IhIVaCURCQeeBgbhvLt7vohMV9XlZZL+V1XHl1m3D3AG0M2dNQc4G/g6iPxWmpUrjDGmYsGULHJEZHjJhIiMAHKDWK8XkKWqa9xgMw0YEWS+FOeZjiggGogEtga5bpVYucUYY/wLJljcCPxZRDaIyAbgTuCGINZLATZ6TWe788q6WESWiMg7ItICQFV/AGYBW9zP56q6ouyKInK9iGSISEZOTk4QWTLGGFMVAYOFqq5W1dNxusx2UdU+qpoVxLZ93auXrfH5CEhV1W7Al8DLACLSFuiE877vFGCAiJzlI2/Pq2q6qqY3atQoiCz5YfVQxhhToYDBQkT+JiKJqrpXVfNEpL6IPBjEtrOBFl7TzYHN3glUdbuqHnInXwB6ut8vAn5097kX+BQ4PYh9VpmNOmuMMf4FUw01VFV3lUy4b807L4j15gPtRKS12yB+Oc5rWT1EpKnX5HCgpKppA3C2iESISCRO43a5aqjqYgULY4ypWDC9ocJFJLqkBCAidXAanSukqoUiMh74HAgHpqjqMhGZBGSo6nRggtt4XgjsAMa6q78DDAB+xrmWf6aqH1Xu0CrHGriNMca/YILFa8BMEZnqTo/DbVsIRFVnADPKzLvP6/tdwF0+1isiuEb0amEP5RljTMWCGXX2nyKyBDgHp9H6M6BVqDN2tFnBwhhj/At21NlfcZ7ivhjnfRYhaz8wxhhz7PFbshCR9jiN0qOB7cB/AVHV/kcpb0eNVUIZY0zFKqqGWgnMBi4oea5CRP5wVHJVA6yB2xhj/KuoGupinOqnWSLygogM5ASt2rf2bWOMqZjfYKGq76vqZUBHnAH8/gA0FpFnRGTwUcrfUWOD2hpjjH/BDPexT1VfV9VhOE9hLwLKDTd+PFNrtTDGmApV6h3cqrpDVZ9T1QGhylBNsXKFMcb4V6lgYYwxpnayYIE1cBtjTCAWLEpYPZQxxvhlwQJ7KM8YYwKxYOGy91kYY4x/FizAihbGGBOABQuXPZNnjDH+WbAwxhgTkAUL7AluY4wJxIKFy2qhjDHGPwsW2EN5xhgTiAULlzVwG2OMfxYssJ6zxhgTiAULlz2UZ4wx/oU0WIjIEBFZJSJZIlLuHRgiMlZEckRkkfu51mtZSxH5n4isEJHlIpIayrwaY4zxr6J3cB8REQkHngYGAdnAfBGZrqrLyyT9r6qO97GJV4CHVPULEYkDikOVV7UWbmOMqVAoSxa9gCxVXaOq+cA0YEQwK4pIZyBCVb8AUNW9qro/dFm1Bm5jjKlIKINFCrDRazrbnVfWxSKyRETeEZEW7rz2wC4ReU9EForIv9ySSikicr2IZIhIRk5OTpUzauUKY4ypWCiDha979bLX5Y+AVFXtBnwJvOzOjwDOBG4HTgXaAGPLbUz1eVVNV9X0Ro0aVXtmjTHGOEIZLLKBFl7TzYHN3glUdbuqHnInXwB6eq270K3CKgQ+AHqEKqPWZGGMMRULZbCYD7QTkdYiEgVcDkz3TiAiTb0mhwMrvNatLyIlxYUBQNmG8epljRbGGONXyHpDqWqhiIwHPgfCgSmqukxEJgEZqjodmCAiw4FCYAduVZOqFonI7cBMERFgAU7JwxhjTA0IWbAAUNUZwIwy8+7z+n4XcJefdb8AuoUyf8YYY4JjT3C7rBLKGGP8q/XBwh7IM8aYwGp9sChh7dvGGONfrQ8WVrAwxpjAan2wKGGjzhpjjH8WLIwxxgRU64OF1UIZY0xgtT5YlLAGbmOM8a/WBwvrOmuMMYHV+mBRwgoWxhjjX60PFlauMMaYwGp9sChhbRbGGOOfBQtjjDEB1fpgYe3bxhgTWK0PFiXE6qGMMcavWh8s1Jq4jTEmoFofLIwxxgRW64OFtVkYY0xgtT5YlLAmC2OM8c+ChTHGmIAsWBhjjAnIgoXLXn5kjDH+RYRy4yIyBHgCCAcmq+rDZZaPBf4FbHJnPaWqk72WJwArgPdVdXwo8mgN3MYcewoKCsjOzubgwYM1nZUTRkxMDM2bNycyMrJK64csWIhIOPA0MAjIBuaLyHRVXV4m6X8rCAR/Bb4JVR69WQO3MceO7Oxs4uPjSU1NtQdmq4Gqsn37drKzs2ndunWVthHKaqheQJaqrlHVfGAaMCLYlUWkJ9AY+F+I8gfYQ3nGHIsOHjxIw4YNLVBUExGhYcOGR1RSC2WwSAE2ek1nu/PKulhElojIOyLSAkBEwoBHgT+FMH+l2J+kMccWCxTV60h/z1AGC185K3sb/xGQqqrdgC+Bl935vwNmqOpGKiAi14tIhohk5OTkHHGGjTHG+BbKYJENtPCabg5s9k6gqttV9ZA7+QLQ0/3eGxgvIuuAR4AxIlKqcdxd/3lVTVfV9EaNGlUpk9bAbYwpa/v27aSlpZGWlkaTJk1ISUnxTOfn5we1jXHjxrFq1aoK0zz99NO8/vrr1ZHlkAtlb6j5QDsRaY3T2+ly4ArvBCLSVFW3uJPDcXo+oapXeqUZC6Sr6sQQ5tUauI0xHg0bNmTRokUA3H///cTFxXH77beXSqOqqCphYb7vuadOnRpwPzfffPORZ/YoCVmwUNVCERkPfI7TdXaKqi4TkUlAhqpOByaIyHCgENgBjA1Vfvzm82jv0BhTKQ98tIzlm/dU6zY7N0vgLxd0qfR6WVlZXHjhhfTt25e5c+fy8ccf88ADD/DTTz9x4MABLrvsMu677z4A+vbty1NPPUXXrl1JSkrixhtv5NNPPyU2NpYPP/yQ5ORk7rnnHpKSkrj11lvp27cvffv25auvvmL37t1MnTqVPn36sG/fPsaMGUNWVhadO3cmMzOTyZMnk5aWVq2/SSAhfShPVWeoantVPUlVH3Ln3ecGClT1LlXtoqqnqGp/VV3pYxsvheoZC2/2UJ4xJhjLly/nmmuuYeHChaSkpPDwww+TkZHB4sWL+eKLL1i+vOzTAbB7927OPvtsFi9eTO/evZkyZYrPbasq8+bN41//+heTJk0C4N///jdNmjRh8eLFTJw4kYULF4b0+PwJ6UN5xwO1RgtjjmlVKQGE0kknncSpp57qmX7zzTd58cUXKSwsZPPmzSxfvpzOnTuXWqdOnToMHToUgJ49ezJ79myf2x45cqQnzbp16wCYM2cOd955JwCnnHIKXbrUzO9R64NFCWuzMMYEo27dup7vmZmZPPHEE8ybN4/ExER+85vf+HyWISoqyvM9PDycwsJCn9uOjo4ul+ZYuaG1saGMMaaK9uzZQ3x8PAkJCWzZsoXPP/+82vfRt29f3nrrLQB+/vlnn9VcR0OtL1kcGzHbGHM86tGjB507d6Zr1660adOGM844o9r38fvf/54xY8bQrVs3evToQdeuXalXr1617ycQOVaKOEcqPT1dMzIyKr3enoMFdLv/f9xzfieuPbNNCHJmjKmsFStW0KlTp5rOxjGhsLCQwsJCYmJiyMzMZPDgwWRmZhIRUfl7fV+/q4gsUNX0QOtayeLEiJXGmBPU3r17GThwIIWFhagqzz33XJUCxZGq9cGihI1DY4w5FiUmJrJgwYKazoY1cFujhTHGBGbBwmXlCmOM8c+ChTHGmIBqfbCwlx8ZY0xgtT5YlLD2bWNMiX79+pV7wO7xxx/nd7/7nd914uLiANi8eTOjRo3yu91AXfwff/xx9u/f75k+77zz2LVrV7BZD5laHyys66wxpqzRo0czbdq0UvOmTZvG6NGjA67brFkz3nnnnSrvu2ywmDFjBomJiVXeXnWxrrMuK1gYc4z6dCL8+nP1brPJyTC03PvUPEaNGsU999zDoUOHiI6OZt26dWzevJm0tDQGDhzIzp07KSgo4MEHH2TEiBGl1l23bh3Dhg1j6dKlHDhwgHHjxrF8+XI6derEgQMHPOluuukm5s+fz4EDBxg1ahQPPPAATz75JJs3b6Z///4kJSUxa9YsUlNTycjIICkpiccee8wzYu21117Lrbfeyrp16xg6dCh9+/bl+++/JyUlhQ8//JA6depU609mJYuazoAx5pjTsGFDevXqxWeffQY4pYrLLruMOnXq8P777/PTTz8xa9YsbrvttgoH+nvmmWeIjY1lyZIl3H333aWel3jooYfIyMhgyZIlfPPNNyxZsoQJEybQrFkzZs2axaxZs0pta8GCBUydOpW5c+fy448/8sILL3iGK8/MzOTmm29m2bJlJCYm8u6771b7b2IlC2PMsa2CEkAolVRFjRgxgmnTpjFlyhRUlT//+c98++23hIWFsWnTJrZu3UqTJk18buPbb79lwoQJAHTr1o1u3bp5lr311ls8//zzFBYWsmXLFpYvX15qeVlz5szhoosu8ox6O3LkSGbPns3w4cNp3bq152VI3sObV6daX7IoYU9wG2O8XXjhhcycOdPzFrwePXrw+uuvk5OTw4IFC1i0aBGNGzf2OSS5N1/XlrVr1/LII48wc+ZMlixZwvnnnx9wOxWVYEqGNoeKh0A/ErU+WJwoAykaY6pXXFwc/fr14+qrr/Y0bO/evZvk5GQiIyOZNWsW69evr3AbZ511Fq+//joAS5cuZcmSJYAztHndunWpV68eW7du5dNPP/WsEx8fT15ens9tffDBB+zfv599+/bx/vvvc+aZZ1bX4QZk1VAuK1gYY8oaPXo0I0eO9PSMuvLKK7ngggtIT08nLS2Njh07Vrj+TTfdxLhx4+jWrRtpaWn06tULcN541717d7p06VJuaPPrr7+eoUOH0rRp01LtFj169GDs2LGebVx77bV07949JFVOvtT6IcrzDhYw8d2fuSS9Of06JIcgZ8aYyrIhykPDhig/AvExkTx9ZY+azoYxxhzTan2bhTHGmMAsWBhjjkknShX5seJIf8+QBgsRGSIiq0QkS0Qm+lg+VkRyRGSR+7nWnZ8mIj+IyDIRWSIil4Uyn8aYY0tMTAzbt2+3gFFNVJXt27cTExNT5W2ErM1CRMKBp4FBQDYwX0Smq+ryMkn/q6rjy8zbD4xR1UwRaQYsEJHPVbXmR9MyxoRc8+bNyc7OJicnp6azcsKIiYmhefPmVV4/lA3cvYAsVV0DICLTgBFA2WBRjqr+4vV9s4hsAxoBFiyMqQUiIyNp3bp1TWfDeAllNVQKsNFrOtudV9bFblXTOyLSouxCEekFRAGrfSy7XkQyRCTD7kCMMSZ0QhksfD3mVrYC8iMgVVW7AV8CL5fagEhT4FVgnKoWl9uY6vOqmq6q6Y0aNaqmbBtjjCkrlMEiG/AuKTQHNnsnUNXtqnrInXwB6FmyTEQSgE+Ae1T1xxDm0xhjTAChbLOYD7QTkdbAJuBy4ArvBCLSVFW3uJPDgRXu/CjgfeAVVX07mJ0tWLAgV0QqHqilYklA7hGsfzyyYz7x1bbjBTvmymoVTKKQBQtVLRSR8cDnQDgwRVWXicgkIENVpwMTRGQ4UAjsAMa6q18KnAU0FJGSeWNVdVEF+zuieigRyQjmkfcTiR3zia+2HS/YMYdsH9aP2WF/YLVDbTvm2na8YMccKvYEtzHGmIAsWBz2fE1noAbYMZ/4atvxgh1zSFg1lDHGmICsZGGMMSYgCxbGGGMCqvXBItDIuMcrEWkhIrNEZIU7eu8t7vwGIvKFiGS6/9Z354uIPOn+DktE5Lh9I5SIhIvIQhH52J1uLSJz3WP+r/scDyIS7U5nuctTazLfVSUiie5wOSvd8937RD/PIvIH9+96qYi8KSIxJ9p5FpEpIrJNRJZ6zav0eRWRq9z0mSJyVVXzU6uDhdfIuEOBzsBoEelcs7mqNoXAbaraCTgduNk9tonATFVtB8x0p8H5Ddq5n+uBZ45+lqvNLbgPeLr+Afyfe8w7gWvc+dcAO1W1LfB/brrj0RPAZ6raETgF59hP2PMsIinABCBdVbviPMd1OSfeeX4JGFJmXqXOq4g0AP4CnIYzuOtfSgJMpalqrf0AvYHPvabvAu6q6XyF6Fg/xBkufhXQ1J3XFFjlfn8OGO2V3pPuePrgDCszExgAfIwzRlkuEFH2nOM8MNrb/R7hppOaPoZKHm8CsLZsvk/k88zhQUobuOftY+DcE/E8A6nA0qqeV2A08JzX/FLpKvOp1SULgh8Z97jmFru7A3OBxuoOseL+m+wmO1F+i8eBO4CSgScbArtUtdCd9j4uzzG7y3e76Y8nbYAcYKpb9TZZROpyAp9nVd0EPAJsALbgnLcFnNjnuURlz2u1ne/aHiyCGRn3uCYiccC7wK2quqeipD7mHVe/hYgMA7ap6gLv2T6SahDLjhcRQA/gGVXtDuzjcNWEL8f9MbvVKCOA1kAzoC5ONUxZJ9J5DsTfMVbbsdf2YBFwZNzjmYhE4gSK11X1PXf2Vnfo95Ih4Le580+E3+IMYLiIrAOm4VRFPQ4kikjJOGjex+U5Znd5PZwxyo4n2UC2qs51p9/BCR4n8nk+B1irqjmqWgC8B/ThxD7PJSp7XqvtfNf2YOEZGdftOXE5ML2G81QtRESAF4EVqvqY16LpQEmPiKtw2jJK5o9xe1WcDuzWwyMCHxdU9S5Vba6qqTjn8itVvRKYBYxyk5U95pLfYpSb/ri641TVX4GNItLBnTUQ522UJ+x5xql+Ol1EYt2/85JjPmHPs5fKntfPgcEiUt8tkQ1251VeTTfg1PQHOA/4BedNfHfXdH6q8bj64hQ3lwCL3M95OHW1M4FM998GbnrB6Rm2GvgZp6dJjR/HERx/P+Bj93sbYB6QBbwNRLvzY9zpLHd5m5rOdxWPNQ3IcM/1B0D9E/08Aw8AK4GlOC9Iiz7RzjPwJk6bTAFOCeGaqpxX4Gr32LNwXiRXpfzYcB/GGGMCqu3VUMYYY4JgwcIYY0xAFiyMMcYEZMHCGGNMQBYsjDHGBGTBwpgARKRIRBZ5faptdGIRSfUeVdSYY1VE4CTG1HoHVDWtpjNhTE2ykoUxVSQi60TkHyIyz/20dee3EpGZ7nsFZopIS3d+YxF5X0QWu58+7qbCReQF9/0M/xOROm76CSKy3N3OtBo6TGMACxbGBKNOmWqoy7yW7VHVXsBTOONQ4X5/RVW7Aa8DT7rznwS+UdVTcMZvWubObwc8rapdgF3Axe78iUB3dzs3hurgjAmGPcFtTAAisldV43zMXwcMUNU17qCNv6pqQxHJxXnnQIE7f4uqJolIDtBcVQ95bSMV+EKdl9kgIncCkar6oIh8BuzFGcLjA1XdG+JDNcYvK1kYc2TUz3d/aXw55PW9iMNtiefjjPfTE1jgNaKqMUedBQtjjsxlXv/+4H7/HmfUW4ArgTnu95nATeB5T3iCv42KSBjQQlVn4bzMKREoV7ox5mixOxVjAqsjIou8pj9T1ZLus9EiMhfnxmu0O28CMEVE/oTzFrtx7vxbgOdF5BqcEsRNOKOK+hIOvCYi9XBGFP0/Vd1VbUdkTCVZm4UxVeS2WaSram5N58WYULNqKGOMMQFZycIYY0xAVrIwxhgTkAULY4wxAVmwMMYYE5AFC2OMMQFZsDDGGBPQ/wMhKrCCzt4/yQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs. Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = pd.read_csv('data/songs_10000.csv')\n",
    "\n",
    "# drop additional index column\n",
    "songs_df = songs_df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "songs_df_clean = songs_df.drop(columns = ['Artist', 'Track Name', 'Track ID'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(songs_df_clean.loc[:, songs_df_clean.columns != 'Popularity'], \n",
    "                                                    songs_df_clean.Popularity, test_size = 0.2, \n",
    "                                                    random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# cat_cols = ['Key', 'Time Signature']\n",
    "# X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "# X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "# num_features = X_train_num.columns.tolist()\n",
    "# num_index_train = X_train.index.tolist()\n",
    "# num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "\n",
    "# scaler = MinMaxScaler().fit(X_train_num)\n",
    "# X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "# X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "# X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)\n",
    "\n",
    "cat_cols = ['Key', 'Time Signature', 'Mode']\n",
    "X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "num_features = X_train_num.columns.tolist()\n",
    "num_index_train = X_train.index.tolist()\n",
    "num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "X_train_dum = X_train[cat_cols]\n",
    "X_test_dum = X_test[cat_cols]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.239920</td>\n",
       "      <td>0.485962</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.833175</td>\n",
       "      <td>0.265010</td>\n",
       "      <td>0.568203</td>\n",
       "      <td>0.432074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724899</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.105684</td>\n",
       "      <td>0.462915</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.061393</td>\n",
       "      <td>0.821867</td>\n",
       "      <td>0.283644</td>\n",
       "      <td>0.430502</td>\n",
       "      <td>0.222676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.626396</td>\n",
       "      <td>0.301020</td>\n",
       "      <td>0.248482</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.098395</td>\n",
       "      <td>0.670088</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>0.632278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045681</td>\n",
       "      <td>0.829442</td>\n",
       "      <td>0.268965</td>\n",
       "      <td>0.665324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048676</td>\n",
       "      <td>0.825085</td>\n",
       "      <td>0.059627</td>\n",
       "      <td>0.445949</td>\n",
       "      <td>0.208376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462850</td>\n",
       "      <td>0.948223</td>\n",
       "      <td>0.246726</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.066046</td>\n",
       "      <td>0.522910</td>\n",
       "      <td>0.554648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027909</td>\n",
       "      <td>0.626396</td>\n",
       "      <td>0.270068</td>\n",
       "      <td>0.733462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109860</td>\n",
       "      <td>0.857285</td>\n",
       "      <td>0.028778</td>\n",
       "      <td>0.386085</td>\n",
       "      <td>0.738509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.788832</td>\n",
       "      <td>0.296175</td>\n",
       "      <td>0.658310</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.076715</td>\n",
       "      <td>0.864151</td>\n",
       "      <td>0.245342</td>\n",
       "      <td>0.863212</td>\n",
       "      <td>0.250255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086243</td>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.360517</td>\n",
       "      <td>0.834666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.857444</td>\n",
       "      <td>0.124224</td>\n",
       "      <td>0.495577</td>\n",
       "      <td>0.718080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099195</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.193043</td>\n",
       "      <td>0.659312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>0.893950</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>0.635977</td>\n",
       "      <td>0.331971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.890355</td>\n",
       "      <td>0.296823</td>\n",
       "      <td>0.559109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057223</td>\n",
       "      <td>0.899003</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.713438</td>\n",
       "      <td>0.503575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.494416</td>\n",
       "      <td>0.314166</td>\n",
       "      <td>0.953907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147384</td>\n",
       "      <td>0.909585</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>0.572438</td>\n",
       "      <td>0.336057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.422335</td>\n",
       "      <td>0.328477</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.089014</td>\n",
       "      <td>0.849241</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.680230</td>\n",
       "      <td>0.134831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.113452</td>\n",
       "      <td>0.861929</td>\n",
       "      <td>0.190791</td>\n",
       "      <td>0.583158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138003</td>\n",
       "      <td>0.864763</td>\n",
       "      <td>0.221532</td>\n",
       "      <td>0.454564</td>\n",
       "      <td>0.527068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026905</td>\n",
       "      <td>0.748223</td>\n",
       "      <td>0.220057</td>\n",
       "      <td>0.908816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027621</td>\n",
       "      <td>0.911240</td>\n",
       "      <td>0.042961</td>\n",
       "      <td>0.581516</td>\n",
       "      <td>0.849847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385541</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.335932</td>\n",
       "      <td>0.655304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379821</td>\n",
       "      <td>0.855404</td>\n",
       "      <td>0.308489</td>\n",
       "      <td>0.681280</td>\n",
       "      <td>0.689479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507027</td>\n",
       "      <td>0.498477</td>\n",
       "      <td>0.340111</td>\n",
       "      <td>0.384757</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.836370</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.445645</td>\n",
       "      <td>0.113381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145580</td>\n",
       "      <td>0.699492</td>\n",
       "      <td>0.226768</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.457995</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.070186</td>\n",
       "      <td>0.568449</td>\n",
       "      <td>0.664964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511043</td>\n",
       "      <td>0.536041</td>\n",
       "      <td>0.341775</td>\n",
       "      <td>0.751498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.912848</td>\n",
       "      <td>0.112836</td>\n",
       "      <td>0.798391</td>\n",
       "      <td>0.546476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.846701</td>\n",
       "      <td>0.324166</td>\n",
       "      <td>0.622237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058683</td>\n",
       "      <td>0.870451</td>\n",
       "      <td>0.084990</td>\n",
       "      <td>0.636305</td>\n",
       "      <td>0.626149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112448</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.294249</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119241</td>\n",
       "      <td>0.930093</td>\n",
       "      <td>0.079089</td>\n",
       "      <td>0.636459</td>\n",
       "      <td>0.402451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266063</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.268431</td>\n",
       "      <td>0.438867</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.065145</td>\n",
       "      <td>0.823182</td>\n",
       "      <td>0.122153</td>\n",
       "      <td>0.654333</td>\n",
       "      <td>0.157303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655622</td>\n",
       "      <td>0.746193</td>\n",
       "      <td>0.192221</td>\n",
       "      <td>0.526043</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.943349</td>\n",
       "      <td>0.045652</td>\n",
       "      <td>0.727613</td>\n",
       "      <td>0.376915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752007</td>\n",
       "      <td>0.339086</td>\n",
       "      <td>0.279566</td>\n",
       "      <td>0.440871</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.225558</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.044928</td>\n",
       "      <td>0.365227</td>\n",
       "      <td>0.384065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112448</td>\n",
       "      <td>0.703553</td>\n",
       "      <td>0.352779</td>\n",
       "      <td>0.489970</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.787627</td>\n",
       "      <td>0.093271</td>\n",
       "      <td>0.399416</td>\n",
       "      <td>0.444331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530119</td>\n",
       "      <td>0.760406</td>\n",
       "      <td>0.244590</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>0.074431</td>\n",
       "      <td>0.545963</td>\n",
       "      <td>0.284985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895582</td>\n",
       "      <td>0.450761</td>\n",
       "      <td>0.438794</td>\n",
       "      <td>0.210405</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.854075</td>\n",
       "      <td>0.704034</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.640344</td>\n",
       "      <td>0.251277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.162437</td>\n",
       "      <td>0.048646</td>\n",
       "      <td>0.407804</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.596623</td>\n",
       "      <td>0.221867</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.597395</td>\n",
       "      <td>0.028703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946787</td>\n",
       "      <td>0.189848</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.820638</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.365942</td>\n",
       "      <td>0.049068</td>\n",
       "      <td>0.600189</td>\n",
       "      <td>0.029213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.470882</td>\n",
       "      <td>0.785787</td>\n",
       "      <td>0.189842</td>\n",
       "      <td>0.514018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246404</td>\n",
       "      <td>0.841650</td>\n",
       "      <td>0.091304</td>\n",
       "      <td>0.363468</td>\n",
       "      <td>0.365679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.717766</td>\n",
       "      <td>0.248573</td>\n",
       "      <td>0.138259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>0.781033</td>\n",
       "      <td>0.044410</td>\n",
       "      <td>0.385845</td>\n",
       "      <td>0.603677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047689</td>\n",
       "      <td>0.669036</td>\n",
       "      <td>0.297758</td>\n",
       "      <td>0.498988</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.789452</td>\n",
       "      <td>0.821845</td>\n",
       "      <td>0.061801</td>\n",
       "      <td>0.375436</td>\n",
       "      <td>0.813075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097990</td>\n",
       "      <td>0.502538</td>\n",
       "      <td>0.300819</td>\n",
       "      <td>0.751498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067855</td>\n",
       "      <td>0.906322</td>\n",
       "      <td>0.068944</td>\n",
       "      <td>0.718422</td>\n",
       "      <td>0.517875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851405</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.179819</td>\n",
       "      <td>0.283553</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.182823</td>\n",
       "      <td>0.791072</td>\n",
       "      <td>0.034369</td>\n",
       "      <td>0.349970</td>\n",
       "      <td>0.280899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142568</td>\n",
       "      <td>0.747208</td>\n",
       "      <td>0.237775</td>\n",
       "      <td>0.484960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426725</td>\n",
       "      <td>0.815613</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.470243</td>\n",
       "      <td>0.218590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.842640</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.356890</td>\n",
       "      <td>0.682257</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.476831</td>\n",
       "      <td>0.193054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.273739</td>\n",
       "      <td>0.737470</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.883141</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>0.536177</td>\n",
       "      <td>0.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968875</td>\n",
       "      <td>0.414213</td>\n",
       "      <td>0.161808</td>\n",
       "      <td>0.197379</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.602674</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.608726</td>\n",
       "      <td>0.639428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.522087</td>\n",
       "      <td>0.581726</td>\n",
       "      <td>0.280920</td>\n",
       "      <td>0.398786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.797575</td>\n",
       "      <td>0.118012</td>\n",
       "      <td>0.563715</td>\n",
       "      <td>0.445352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588352</td>\n",
       "      <td>0.658883</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.493978</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.049093</td>\n",
       "      <td>0.836710</td>\n",
       "      <td>0.113872</td>\n",
       "      <td>0.474977</td>\n",
       "      <td>0.282942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893574</td>\n",
       "      <td>0.696447</td>\n",
       "      <td>0.312803</td>\n",
       "      <td>0.332652</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.743485</td>\n",
       "      <td>0.046998</td>\n",
       "      <td>0.367989</td>\n",
       "      <td>0.076098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.617259</td>\n",
       "      <td>0.300821</td>\n",
       "      <td>0.157298</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.114030</td>\n",
       "      <td>0.739089</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.704665</td>\n",
       "      <td>0.153218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.372589</td>\n",
       "      <td>0.445712</td>\n",
       "      <td>0.272531</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.685565</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.544087</td>\n",
       "      <td>0.088662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.589848</td>\n",
       "      <td>0.278201</td>\n",
       "      <td>0.197379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.835531</td>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.592311</td>\n",
       "      <td>0.356486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072488</td>\n",
       "      <td>0.649746</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>0.676346</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.194288</td>\n",
       "      <td>0.896397</td>\n",
       "      <td>0.096894</td>\n",
       "      <td>0.367657</td>\n",
       "      <td>0.712972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.780711</td>\n",
       "      <td>0.164941</td>\n",
       "      <td>0.808613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>0.942737</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.374477</td>\n",
       "      <td>0.371808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.414213</td>\n",
       "      <td>0.101696</td>\n",
       "      <td>0.506002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151553</td>\n",
       "      <td>0.698754</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.819817</td>\n",
       "      <td>0.650664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056022</td>\n",
       "      <td>0.504569</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.210965</td>\n",
       "      <td>0.730138</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.793316</td>\n",
       "      <td>0.527068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405621</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.237512</td>\n",
       "      <td>0.510010</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.164061</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.063665</td>\n",
       "      <td>0.455695</td>\n",
       "      <td>0.330950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056122</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.265895</td>\n",
       "      <td>0.552095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147384</td>\n",
       "      <td>0.846862</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>0.476908</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.248994</td>\n",
       "      <td>0.672081</td>\n",
       "      <td>0.144127</td>\n",
       "      <td>0.606205</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.130707</td>\n",
       "      <td>0.779968</td>\n",
       "      <td>0.671843</td>\n",
       "      <td>0.346998</td>\n",
       "      <td>0.708887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.281757</td>\n",
       "      <td>0.736468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.196687</td>\n",
       "      <td>0.685964</td>\n",
       "      <td>0.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.267324</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415260</td>\n",
       "      <td>0.896012</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.818023</td>\n",
       "      <td>0.876404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.089255</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.253191</td>\n",
       "      <td>0.547085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056077</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.156315</td>\n",
       "      <td>0.817119</td>\n",
       "      <td>0.410623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107428</td>\n",
       "      <td>0.705584</td>\n",
       "      <td>0.240472</td>\n",
       "      <td>0.640273</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.853660</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.409030</td>\n",
       "      <td>0.351379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964859</td>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.265317</td>\n",
       "      <td>0.396781</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.065875</td>\n",
       "      <td>0.771697</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>0.527176</td>\n",
       "      <td>0.686415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.531980</td>\n",
       "      <td>0.201019</td>\n",
       "      <td>0.140263</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.148426</td>\n",
       "      <td>0.506979</td>\n",
       "      <td>0.141822</td>\n",
       "      <td>0.612802</td>\n",
       "      <td>0.265577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.219878</td>\n",
       "      <td>0.467005</td>\n",
       "      <td>0.245797</td>\n",
       "      <td>0.498988</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.082760</td>\n",
       "      <td>0.866916</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.349397</td>\n",
       "      <td>0.157303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7960</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045881</td>\n",
       "      <td>0.722843</td>\n",
       "      <td>0.321652</td>\n",
       "      <td>0.661316</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.067125</td>\n",
       "      <td>0.810605</td>\n",
       "      <td>0.052795</td>\n",
       "      <td>0.620593</td>\n",
       "      <td>0.407559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.970558</td>\n",
       "      <td>0.253548</td>\n",
       "      <td>0.545081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>0.864695</td>\n",
       "      <td>0.085093</td>\n",
       "      <td>0.472565</td>\n",
       "      <td>0.129724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.405076</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.162308</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.086929</td>\n",
       "      <td>0.475074</td>\n",
       "      <td>0.046170</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.318693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Key  Time Signature  Mode  Acousticness  Danceability  Duration_ms    Energy  Instrumentalness  Liveness  Loudness  Speechiness     Tempo   Valence\n",
       "8018   11               4     0      0.010138      0.800000     0.239920  0.485962          0.000524  0.009589  0.833175     0.265010  0.568203  0.432074\n",
       "9225    6               4     1      0.724899      0.680203     0.105684  0.462915          0.000009  0.061393  0.821867     0.283644  0.430502  0.222676\n",
       "3854   11               4     1      0.764056      0.626396     0.301020  0.248482          0.599000  0.098395  0.670088     0.039130  0.735592  0.632278\n",
       "2029    1               4     1      0.045681      0.829442     0.268965  0.665324          0.000000  0.048676  0.825085     0.059627  0.445949  0.208376\n",
       "3539   11               4     1      0.462850      0.948223     0.246726  0.565122          0.000018  0.084845  0.901042     0.066046  0.522910  0.554648\n",
       "1942   11               4     0      0.027909      0.626396     0.270068  0.733462          0.000000  0.109860  0.857285     0.028778  0.386085  0.738509\n",
       "1250    1               4     1      0.000739      0.788832     0.296175  0.658310          0.000003  0.076715  0.864151     0.245342  0.863212  0.250255\n",
       "2817    1               4     0      0.086243      0.817259     0.360517  0.834666          0.000000  0.039817  0.857444     0.124224  0.495577  0.718080\n",
       "4211    2               4     1      0.099195      0.786802     0.193043  0.659312          0.000000  0.095268  0.893950     0.181159  0.635977  0.331971\n",
       "477     1               4     1      0.066263      0.890355     0.296823  0.559109          0.000000  0.057223  0.899003     0.354037  0.713438  0.503575\n",
       "7904    1               4     0      0.000227      0.494416     0.314166  0.953907          0.000000  0.147384  0.909585     0.063458  0.572438  0.336057\n",
       "4322    6               4     0      0.013853      0.422335     0.328477  0.565122          0.005670  0.089014  0.849241     0.033126  0.680230  0.134831\n",
       "770     8               4     1      0.113452      0.861929     0.190791  0.583158          0.000000  0.138003  0.864763     0.221532  0.454564  0.527068\n",
       "8272    8               4     1      0.026905      0.748223     0.220057  0.908816          0.000000  0.027621  0.911240     0.042961  0.581516  0.849847\n",
       "540     0               4     1      0.385541      0.734010     0.335932  0.655304          0.000000  0.379821  0.855404     0.308489  0.681280  0.689479\n",
       "1519    9               4     1      0.507027      0.498477     0.340111  0.384757          0.000051  0.096310  0.836370     0.026708  0.445645  0.113381\n",
       "5315    7               4     1      0.145580      0.699492     0.226768  0.902804          0.000010  0.457995  0.977521     0.070186  0.568449  0.664964\n",
       "7728    2               4     1      0.511043      0.536041     0.341775  0.751498          0.000000  0.268293  0.912848     0.112836  0.798391  0.546476\n",
       "61      7               4     1      0.092167      0.846701     0.324166  0.622237          0.000000  0.058683  0.870451     0.084990  0.636305  0.626149\n",
       "9851    9               4     0      0.112448      0.667005     0.294249  0.753502          0.000000  0.119241  0.930093     0.079089  0.636459  0.402451\n",
       "2987    1               4     0      0.266063      0.769543     0.268431  0.438867          0.002980  0.065145  0.823182     0.122153  0.654333  0.157303\n",
       "258     6               4     0      0.655622      0.746193     0.192221  0.526043          0.918000  0.084845  0.943349     0.045652  0.727613  0.376915\n",
       "2933    0               4     1      0.752007      0.339086     0.279566  0.440871          0.000607  0.225558  0.786585     0.044928  0.365227  0.384065\n",
       "6996    4               4     0      0.112448      0.703553     0.352779  0.489970          0.000465  0.064936  0.787627     0.093271  0.399416  0.444331\n",
       "3308    5               4     1      0.530119      0.760406     0.244590  0.419828          0.000008  0.097353  0.796714     0.074431  0.545963  0.284985\n",
       "3149    3               3     1      0.895582      0.450761     0.438794  0.210405          0.000021  0.854075  0.704034     0.036439  0.640344  0.251277\n",
       "8571    5               4     1      0.001383      0.162437     0.048646  0.407804          0.778000  0.596623  0.221867     0.053313  0.597395  0.028703\n",
       "4751    9               5     1      0.946787      0.189848     0.051920  0.820638          0.989000  0.096310  0.365942     0.049068  0.600189  0.029213\n",
       "4564   11               4     0      0.470882      0.785787     0.189842  0.514018          0.000000  0.246404  0.841650     0.091304  0.363468  0.365679\n",
       "7694    4               4     1      0.823293      0.717766     0.248573  0.138259          0.000000  0.120284  0.781033     0.044410  0.385845  0.603677\n",
       "...   ...             ...   ...           ...           ...          ...       ...               ...       ...       ...          ...       ...       ...\n",
       "9057    4               4     0      0.047689      0.669036     0.297758  0.498988          0.855000  0.789452  0.821845     0.061801  0.375436  0.813075\n",
       "8302    6               4     0      0.097990      0.502538     0.300819  0.751498          0.000000  0.067855  0.906322     0.068944  0.718422  0.517875\n",
       "8550    9               4     1      0.851405      0.659898     0.179819  0.283553          0.912000  0.182823  0.791072     0.034369  0.349970  0.280899\n",
       "9890    7               4     1      0.142568      0.747208     0.237775  0.484960          0.000000  0.426725  0.815613     0.452381  0.470243  0.218590\n",
       "2131    1               4     1      0.949799      0.842640     0.267715  0.159302          0.000020  0.356890  0.682257     0.039648  0.476831  0.193054\n",
       "1093   10               4     0      0.006102      0.613198     0.273739  0.737470          0.109000  0.094226  0.883141     0.057453  0.536177  0.352400\n",
       "8719   11               4     1      0.968875      0.414213     0.161808  0.197379          0.791000  0.094226  0.602674     0.054141  0.608726  0.639428\n",
       "5624    5               5     0      0.522087      0.581726     0.280920  0.398786          0.000000  0.097353  0.797575     0.118012  0.563715  0.445352\n",
       "303    11               4     0      0.588352      0.658883     0.305085  0.493978          0.000167  0.049093  0.836710     0.113872  0.474977  0.282942\n",
       "4066    9               4     1      0.893574      0.696447     0.312803  0.332652          0.342000  0.091099  0.743485     0.046998  0.367989  0.076098\n",
       "5056    0               3     1      0.843373      0.617259     0.300821  0.157298          0.000060  0.114030  0.739089     0.035300  0.704665  0.153218\n",
       "9664    7               3     0      0.925703      0.372589     0.445712  0.272531          0.580000  0.096310  0.685565     0.054141  0.544087  0.088662\n",
       "244     3               4     1      0.883534      0.589848     0.278201  0.197379          0.000000  0.091099  0.835531     0.034058  0.592311  0.356486\n",
       "3266    9               4     1      0.072488      0.649746     0.246092  0.676346          0.000001  0.194288  0.896397     0.096894  0.367657  0.712972\n",
       "707    11               4     1      0.051705      0.780711     0.164941  0.808613          0.000000  0.095268  0.942737     0.316770  0.374477  0.371808\n",
       "5358    2               4     1      0.262047      0.414213     0.101696  0.506002          0.000000  0.151553  0.698754     0.219462  0.819817  0.650664\n",
       "6980    9               3     0      0.056022      0.504569     0.268810  0.499990          0.730000  0.210965  0.730138     0.057971  0.793316  0.527068\n",
       "6408    5               4     0      0.405621      0.560406     0.237512  0.510010          0.000016  0.164061  0.805031     0.063665  0.455695  0.330950\n",
       "1664    0               4     1      0.056122      0.862944     0.265895  0.552095          0.000000  0.147384  0.846862     0.059213  0.476908  0.647600\n",
       "5655    6               4     1      0.248994      0.672081     0.144127  0.606205          0.013200  0.130707  0.779968     0.671843  0.346998  0.708887\n",
       "8628   11               4     0      0.008612      0.898477     0.281757  0.736468          0.000000  0.081301  0.854294     0.196687  0.685964  0.911134\n",
       "3053    8               4     0      0.100400      0.707614     0.267324  0.773543          0.000000  0.415260  0.896012     0.319876  0.818023  0.876404\n",
       "1275    8               4     1      0.089255      0.613198     0.253191  0.547085          0.000000  0.056077  0.880988     0.156315  0.817119  0.410623\n",
       "2602    7               4     0      0.107428      0.705584     0.240472  0.640273          0.000657  0.102564  0.853660     0.158385  0.409030  0.351379\n",
       "6654    0               4     1      0.964859      0.750254     0.265317  0.396781          0.320000  0.065875  0.771697     0.045342  0.527176  0.686415\n",
       "6923    4               4     0      0.897590      0.531980     0.201019  0.140263          0.000238  0.148426  0.506979     0.141822  0.612802  0.265577\n",
       "1207    5               4     1      0.219878      0.467005     0.245797  0.498988          0.003290  0.082760  0.866916     0.029193  0.349397  0.157303\n",
       "7960    1               3     1      0.045881      0.722843     0.321652  0.661316          0.000293  0.067125  0.810605     0.052795  0.620593  0.407559\n",
       "2339    1               4     1      0.002950      0.970558     0.253548  0.545081          0.000000  0.087972  0.864695     0.085093  0.472565  0.129724\n",
       "6637    7               4     0      0.993976      0.405076     0.248009  0.162308          0.903000  0.086929  0.475074     0.046170  0.891363  0.318693\n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 1s 114us/sample - loss: 1087.3272 - mean_squared_error: 1087.3274 - val_loss: 64.0599 - val_mean_squared_error: 64.0599\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 65.0369 - mean_squared_error: 65.0369 - val_loss: 60.3037 - val_mean_squared_error: 60.3037\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 62.3946 - mean_squared_error: 62.3946 - val_loss: 59.2946 - val_mean_squared_error: 59.2946\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 61.6495 - mean_squared_error: 61.6495 - val_loss: 58.6100 - val_mean_squared_error: 58.6100\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 1s 89us/sample - loss: 61.2329 - mean_squared_error: 61.2329 - val_loss: 58.2909 - val_mean_squared_error: 58.2909\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 1s 95us/sample - loss: 60.8856 - mean_squared_error: 60.8856 - val_loss: 59.2073 - val_mean_squared_error: 59.2073\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 60.6082 - mean_squared_error: 60.6082 - val_loss: 57.9259 - val_mean_squared_error: 57.9259\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 60.5623 - mean_squared_error: 60.5623 - val_loss: 57.7302 - val_mean_squared_error: 57.7302\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 1s 89us/sample - loss: 60.4981 - mean_squared_error: 60.4981 - val_loss: 58.9537 - val_mean_squared_error: 58.9537\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 60.2564 - mean_squared_error: 60.2565 - val_loss: 59.1692 - val_mean_squared_error: 59.1692\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 60.3358 - mean_squared_error: 60.3358 - val_loss: 57.7124 - val_mean_squared_error: 57.7124\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 60.2613 - mean_squared_error: 60.2613 - val_loss: 59.3650 - val_mean_squared_error: 59.3650\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 1s 143us/sample - loss: 60.1096 - mean_squared_error: 60.1096 - val_loss: 57.3274 - val_mean_squared_error: 57.3274\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 60.1184 - mean_squared_error: 60.1184 - val_loss: 57.2096 - val_mean_squared_error: 57.2096\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 59.9568 - mean_squared_error: 59.9568 - val_loss: 57.5732 - val_mean_squared_error: 57.5732\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 59.9957 - mean_squared_error: 59.9957 - val_loss: 57.2055 - val_mean_squared_error: 57.2055\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 59.9102 - mean_squared_error: 59.9102 - val_loss: 57.1423 - val_mean_squared_error: 57.1423\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 59.8239 - mean_squared_error: 59.8239 - val_loss: 58.0767 - val_mean_squared_error: 58.0767\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 60.0289 - mean_squared_error: 60.0289 - val_loss: 56.9725 - val_mean_squared_error: 56.9725\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 59.7849 - mean_squared_error: 59.7849 - val_loss: 57.0457 - val_mean_squared_error: 57.0457\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 60.2190 - mean_squared_error: 60.2190 - val_loss: 56.9261 - val_mean_squared_error: 56.9261\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 60.0196 - mean_squared_error: 60.0196 - val_loss: 57.2707 - val_mean_squared_error: 57.2707\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 59.6805 - mean_squared_error: 59.6805 - val_loss: 56.8919 - val_mean_squared_error: 56.8919\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 1s 96us/sample - loss: 59.9550 - mean_squared_error: 59.9550 - val_loss: 57.7344 - val_mean_squared_error: 57.7344\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 59.6679 - mean_squared_error: 59.6679 - val_loss: 56.9914 - val_mean_squared_error: 56.9914\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 59.7919 - mean_squared_error: 59.7920 - val_loss: 58.0130 - val_mean_squared_error: 58.0130\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 59.8697 - mean_squared_error: 59.8697 - val_loss: 56.8420 - val_mean_squared_error: 56.8420\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 59.6958 - mean_squared_error: 59.6958 - val_loss: 57.8994 - val_mean_squared_error: 57.8994\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 59.5875 - mean_squared_error: 59.5875 - val_loss: 56.7730 - val_mean_squared_error: 56.7730\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 1s 98us/sample - loss: 59.7290 - mean_squared_error: 59.7289 - val_loss: 56.9983 - val_mean_squared_error: 56.9983\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 1s 112us/sample - loss: 59.7844 - mean_squared_error: 59.7844 - val_loss: 57.0003 - val_mean_squared_error: 57.0003\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 59.5805 - mean_squared_error: 59.5805 - val_loss: 57.5455 - val_mean_squared_error: 57.5455\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 59.7934 - mean_squared_error: 59.7934 - val_loss: 57.4566 - val_mean_squared_error: 57.4566\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 1s 101us/sample - loss: 60.0071 - mean_squared_error: 60.0071 - val_loss: 59.6872 - val_mean_squared_error: 59.6872\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 60.2619 - mean_squared_error: 60.2619 - val_loss: 57.0845 - val_mean_squared_error: 57.0845\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 59.8152 - mean_squared_error: 59.8152 - val_loss: 56.7252 - val_mean_squared_error: 56.7252\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 1s 146us/sample - loss: 59.5987 - mean_squared_error: 59.5987 - val_loss: 56.6622 - val_mean_squared_error: 56.6622\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 1s 144us/sample - loss: 59.7004 - mean_squared_error: 59.7004 - val_loss: 57.1387 - val_mean_squared_error: 57.1387\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 1s 95us/sample - loss: 60.1489 - mean_squared_error: 60.1489 - val_loss: 56.8510 - val_mean_squared_error: 56.8510\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 1s 96us/sample - loss: 59.7194 - mean_squared_error: 59.7194 - val_loss: 57.0448 - val_mean_squared_error: 57.0448\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 1s 146us/sample - loss: 59.6815 - mean_squared_error: 59.6815 - val_loss: 56.6795 - val_mean_squared_error: 56.6795\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 1s 224us/sample - loss: 59.6351 - mean_squared_error: 59.6351 - val_loss: 56.8406 - val_mean_squared_error: 56.8406\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 1s 235us/sample - loss: 59.7333 - mean_squared_error: 59.7333 - val_loss: 57.5547 - val_mean_squared_error: 57.5547\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 1s 238us/sample - loss: 59.4536 - mean_squared_error: 59.4535 - val_loss: 57.2848 - val_mean_squared_error: 57.2848\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 1s 108us/sample - loss: 59.8312 - mean_squared_error: 59.8312 - val_loss: 56.8238 - val_mean_squared_error: 56.8238\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 1s 136us/sample - loss: 59.5155 - mean_squared_error: 59.5155 - val_loss: 59.7390 - val_mean_squared_error: 59.7390\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 1s 136us/sample - loss: 59.5312 - mean_squared_error: 59.5312 - val_loss: 57.6690 - val_mean_squared_error: 57.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 1s 138us/sample - loss: 59.5451 - mean_squared_error: 59.5451 - val_loss: 58.1125 - val_mean_squared_error: 58.1125\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 1s 145us/sample - loss: 59.4567 - mean_squared_error: 59.4567 - val_loss: 56.6045 - val_mean_squared_error: 56.6045\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 1s 176us/sample - loss: 59.5434 - mean_squared_error: 59.5434 - val_loss: 57.3567 - val_mean_squared_error: 57.3567\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 1s 147us/sample - loss: 59.3023 - mean_squared_error: 59.3023 - val_loss: 57.1210 - val_mean_squared_error: 57.1210\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 1s 113us/sample - loss: 59.6055 - mean_squared_error: 59.6055 - val_loss: 57.0171 - val_mean_squared_error: 57.0171\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 1s 107us/sample - loss: 59.4689 - mean_squared_error: 59.4689 - val_loss: 56.6193 - val_mean_squared_error: 56.6193\n",
      "Epoch 54/1000\n",
      "5600/5600 [==============================] - 1s 125us/sample - loss: 59.6438 - mean_squared_error: 59.6438 - val_loss: 56.8149 - val_mean_squared_error: 56.8149\n",
      "Epoch 55/1000\n",
      "5600/5600 [==============================] - 1s 155us/sample - loss: 59.6314 - mean_squared_error: 59.6314 - val_loss: 56.5812 - val_mean_squared_error: 56.5812\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 1s 139us/sample - loss: 59.5629 - mean_squared_error: 59.5629 - val_loss: 56.6472 - val_mean_squared_error: 56.6472\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 1s 154us/sample - loss: 59.8980 - mean_squared_error: 59.8980 - val_loss: 58.9793 - val_mean_squared_error: 58.9793\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 1s 128us/sample - loss: 59.6761 - mean_squared_error: 59.6761 - val_loss: 56.7556 - val_mean_squared_error: 56.7556\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 1s 130us/sample - loss: 59.6909 - mean_squared_error: 59.6910 - val_loss: 60.5370 - val_mean_squared_error: 60.5370\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 1s 157us/sample - loss: 59.4931 - mean_squared_error: 59.4931 - val_loss: 56.5980 - val_mean_squared_error: 56.5980\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 1s 127us/sample - loss: 59.7566 - mean_squared_error: 59.7566 - val_loss: 57.4763 - val_mean_squared_error: 57.4763\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 1s 149us/sample - loss: 59.3871 - mean_squared_error: 59.3871 - val_loss: 56.9580 - val_mean_squared_error: 56.9580\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - ETA: 0s - loss: 59.6883 - mean_squared_error: 59.68 - 1s 139us/sample - loss: 59.5329 - mean_squared_error: 59.5329 - val_loss: 56.6237 - val_mean_squared_error: 56.6237\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 1s 130us/sample - loss: 59.4918 - mean_squared_error: 59.4918 - val_loss: 57.0436 - val_mean_squared_error: 57.0436\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 1s 109us/sample - loss: 59.7993 - mean_squared_error: 59.7993 - val_loss: 56.8998 - val_mean_squared_error: 56.8998\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 1s 104us/sample - loss: 59.5691 - mean_squared_error: 59.5690 - val_loss: 57.5281 - val_mean_squared_error: 57.5281\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 1s 140us/sample - loss: 59.5532 - mean_squared_error: 59.5532 - val_loss: 56.6451 - val_mean_squared_error: 56.6451\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 1s 121us/sample - loss: 59.3952 - mean_squared_error: 59.3952 - val_loss: 56.8135 - val_mean_squared_error: 56.8135\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 1s 124us/sample - loss: 59.2730 - mean_squared_error: 59.2730 - val_loss: 56.6876 - val_mean_squared_error: 56.6876\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 1s 118us/sample - loss: 59.5364 - mean_squared_error: 59.5364 - val_loss: 56.6073 - val_mean_squared_error: 56.6073\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 1s 106us/sample - loss: 59.4092 - mean_squared_error: 59.4092 - val_loss: 56.5419 - val_mean_squared_error: 56.5419\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 1s 133us/sample - loss: 59.9429 - mean_squared_error: 59.9428 - val_loss: 56.8696 - val_mean_squared_error: 56.8696\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 59.5231 - mean_squared_error: 59.5230 - val_loss: 56.7135 - val_mean_squared_error: 56.7135\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 1s 129us/sample - loss: 59.2794 - mean_squared_error: 59.2794 - val_loss: 56.6847 - val_mean_squared_error: 56.6847\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 59.3564 - mean_squared_error: 59.3563 - val_loss: 56.5862 - val_mean_squared_error: 56.5862\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 59.7177 - mean_squared_error: 59.7177 - val_loss: 57.7392 - val_mean_squared_error: 57.7392\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 1s 97us/sample - loss: 59.4110 - mean_squared_error: 59.4110 - val_loss: 56.5159 - val_mean_squared_error: 56.5159\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 1s 100us/sample - loss: 59.2003 - mean_squared_error: 59.2003 - val_loss: 56.5805 - val_mean_squared_error: 56.5805\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 59.5335 - mean_squared_error: 59.5335 - val_loss: 56.4858 - val_mean_squared_error: 56.4858\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 59.2755 - mean_squared_error: 59.2755 - val_loss: 56.6970 - val_mean_squared_error: 56.6970\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 59.6648 - mean_squared_error: 59.6648 - val_loss: 56.4736 - val_mean_squared_error: 56.4736\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 59.3806 - mean_squared_error: 59.3805 - val_loss: 56.9128 - val_mean_squared_error: 56.9128\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.5321 - mean_squared_error: 59.5320 - val_loss: 57.3854 - val_mean_squared_error: 57.3854\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.3034 - mean_squared_error: 59.3034 - val_loss: 56.9940 - val_mean_squared_error: 56.9940\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.5046 - mean_squared_error: 59.5046 - val_loss: 56.4226 - val_mean_squared_error: 56.4226\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 59.2885 - mean_squared_error: 59.2885 - val_loss: 56.4084 - val_mean_squared_error: 56.4084\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 59.1682 - mean_squared_error: 59.1682 - val_loss: 57.7751 - val_mean_squared_error: 57.7751\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 59.1868 - mean_squared_error: 59.1868 - val_loss: 56.5362 - val_mean_squared_error: 56.5362\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 59.3411 - mean_squared_error: 59.3411 - val_loss: 56.5009 - val_mean_squared_error: 56.5009\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 1s 108us/sample - loss: 59.6091 - mean_squared_error: 59.6091 - val_loss: 57.0360 - val_mean_squared_error: 57.0360\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 1s 115us/sample - loss: 59.3506 - mean_squared_error: 59.3506 - val_loss: 56.7150 - val_mean_squared_error: 56.7150\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 1s 120us/sample - loss: 59.2419 - mean_squared_error: 59.2419 - val_loss: 56.6443 - val_mean_squared_error: 56.6443\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 59.3288 - mean_squared_error: 59.3288 - val_loss: 56.6188 - val_mean_squared_error: 56.6188\n",
      "Epoch 94/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 1s 112us/sample - loss: 59.1945 - mean_squared_error: 59.1945 - val_loss: 56.5264 - val_mean_squared_error: 56.5264\n",
      "Epoch 95/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.1774 - mean_squared_error: 59.1774 - val_loss: 57.0434 - val_mean_squared_error: 57.0434\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.5983 - mean_squared_error: 59.5983 - val_loss: 57.6252 - val_mean_squared_error: 57.6252\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.5601 - mean_squared_error: 59.5601 - val_loss: 56.9291 - val_mean_squared_error: 56.9291\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 59.3148 - mean_squared_error: 59.3148 - val_loss: 58.3983 - val_mean_squared_error: 58.3984\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.3688 - mean_squared_error: 59.3688 - val_loss: 57.5287 - val_mean_squared_error: 57.5287\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 59.5473 - mean_squared_error: 59.5473 - val_loss: 57.0786 - val_mean_squared_error: 57.0786\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 59.5553 - mean_squared_error: 59.5553 - val_loss: 56.4721 - val_mean_squared_error: 56.4721\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 59.6092 - mean_squared_error: 59.6092 - val_loss: 56.5942 - val_mean_squared_error: 56.5942\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 59.1181 - mean_squared_error: 59.1181 - val_loss: 56.6655 - val_mean_squared_error: 56.6655\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 59.0499 - mean_squared_error: 59.0499 - val_loss: 57.8830 - val_mean_squared_error: 57.8830\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 59.4367 - mean_squared_error: 59.4367 - val_loss: 56.5003 - val_mean_squared_error: 56.5004\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.0822 - mean_squared_error: 59.0822 - val_loss: 56.6950 - val_mean_squared_error: 56.6950\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 59.0583 - mean_squared_error: 59.0583 - val_loss: 57.1467 - val_mean_squared_error: 57.1467\n",
      "Epoch 108/1000\n",
      "5600/5600 [==============================] - 1s 109us/sample - loss: 59.1033 - mean_squared_error: 59.1034 - val_loss: 56.3258 - val_mean_squared_error: 56.3258\n",
      "Epoch 109/1000\n",
      "5600/5600 [==============================] - 1s 122us/sample - loss: 58.9473 - mean_squared_error: 58.9473 - val_loss: 56.3774 - val_mean_squared_error: 56.3774\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 1s 104us/sample - loss: 58.8470 - mean_squared_error: 58.8471 - val_loss: 56.4763 - val_mean_squared_error: 56.4763\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 59.7835 - mean_squared_error: 59.7835 - val_loss: 57.0979 - val_mean_squared_error: 57.0979\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 59.4080 - mean_squared_error: 59.4080 - val_loss: 56.3385 - val_mean_squared_error: 56.3385\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 59.3026 - mean_squared_error: 59.3026 - val_loss: 56.6168 - val_mean_squared_error: 56.6168\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 1s 116us/sample - loss: 59.3187 - mean_squared_error: 59.3187 - val_loss: 56.3637 - val_mean_squared_error: 56.3637\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 1s 106us/sample - loss: 58.9964 - mean_squared_error: 58.9965 - val_loss: 56.2953 - val_mean_squared_error: 56.2953\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 58.9357 - mean_squared_error: 58.9357 - val_loss: 57.0438 - val_mean_squared_error: 57.0438\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 59.1794 - mean_squared_error: 59.1794 - val_loss: 56.8559 - val_mean_squared_error: 56.8559\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 58.8134 - mean_squared_error: 58.8134 - val_loss: 56.2871 - val_mean_squared_error: 56.2871\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 58.9740 - mean_squared_error: 58.9740 - val_loss: 58.5476 - val_mean_squared_error: 58.5476\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 58.8567 - mean_squared_error: 58.8567 - val_loss: 57.1149 - val_mean_squared_error: 57.1150\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 59.1519 - mean_squared_error: 59.1519 - val_loss: 56.5170 - val_mean_squared_error: 56.5170\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 59.1218 - mean_squared_error: 59.1218 - val_loss: 56.6016 - val_mean_squared_error: 56.6016\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.9020 - mean_squared_error: 58.9020 - val_loss: 56.3135 - val_mean_squared_error: 56.3135\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 59.1459 - mean_squared_error: 59.1459 - val_loss: 56.5784 - val_mean_squared_error: 56.5784\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 59.0231 - mean_squared_error: 59.0231 - val_loss: 56.2185 - val_mean_squared_error: 56.2185\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 58.8708 - mean_squared_error: 58.8708 - val_loss: 56.5706 - val_mean_squared_error: 56.5706\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.0379 - mean_squared_error: 59.0379 - val_loss: 57.0352 - val_mean_squared_error: 57.0352\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 58.9314 - mean_squared_error: 58.9314 - val_loss: 56.2625 - val_mean_squared_error: 56.2625\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 59.1011 - mean_squared_error: 59.1011 - val_loss: 56.6255 - val_mean_squared_error: 56.6255\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 58.9123 - mean_squared_error: 58.9123 - val_loss: 56.1888 - val_mean_squared_error: 56.1888\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 1s 109us/sample - loss: 58.7762 - mean_squared_error: 58.7762 - val_loss: 57.9401 - val_mean_squared_error: 57.9401\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 58.9751 - mean_squared_error: 58.9751 - val_loss: 56.2120 - val_mean_squared_error: 56.2120\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 58.7590 - mean_squared_error: 58.7590 - val_loss: 56.9795 - val_mean_squared_error: 56.9795\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 59.0123 - mean_squared_error: 59.0123 - val_loss: 56.4393 - val_mean_squared_error: 56.4393\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.0612 - mean_squared_error: 59.0612 - val_loss: 56.3936 - val_mean_squared_error: 56.3936\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 59.0835 - mean_squared_error: 59.0835 - val_loss: 56.7248 - val_mean_squared_error: 56.7248\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 59.0174 - mean_squared_error: 59.0174 - val_loss: 56.3685 - val_mean_squared_error: 56.3685\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 59.0840 - mean_squared_error: 59.0839 - val_loss: 56.0639 - val_mean_squared_error: 56.0640\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 58.7755 - mean_squared_error: 58.7755 - val_loss: 56.1156 - val_mean_squared_error: 56.1156\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 1s 101us/sample - loss: 59.1835 - mean_squared_error: 59.1835 - val_loss: 56.0054 - val_mean_squared_error: 56.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 59.1223 - mean_squared_error: 59.1223 - val_loss: 56.6651 - val_mean_squared_error: 56.6651\n",
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 58.8696 - mean_squared_error: 58.8696 - val_loss: 56.0590 - val_mean_squared_error: 56.0590\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 58.7457 - mean_squared_error: 58.7457 - val_loss: 56.1363 - val_mean_squared_error: 56.1363\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 1s 103us/sample - loss: 58.8101 - mean_squared_error: 58.8101 - val_loss: 56.6538 - val_mean_squared_error: 56.6538\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 58.8631 - mean_squared_error: 58.8631 - val_loss: 57.2484 - val_mean_squared_error: 57.2484\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 58.9654 - mean_squared_error: 58.9654 - val_loss: 56.1794 - val_mean_squared_error: 56.1794\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 58.8606 - mean_squared_error: 58.8606 - val_loss: 56.7739 - val_mean_squared_error: 56.7739\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 1s 103us/sample - loss: 58.7012 - mean_squared_error: 58.7012 - val_loss: 55.9970 - val_mean_squared_error: 55.9970\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 58.7143 - mean_squared_error: 58.7143 - val_loss: 56.0646 - val_mean_squared_error: 56.0646\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 58.4901 - mean_squared_error: 58.4901 - val_loss: 56.7241 - val_mean_squared_error: 56.7241\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 58.7358 - mean_squared_error: 58.7358 - val_loss: 56.5766 - val_mean_squared_error: 56.5766\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 58.8596 - mean_squared_error: 58.8596 - val_loss: 56.0023 - val_mean_squared_error: 56.0023\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 58.8825 - mean_squared_error: 58.8825 - val_loss: 56.3041 - val_mean_squared_error: 56.3041\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 1s 89us/sample - loss: 58.5818 - mean_squared_error: 58.5818 - val_loss: 55.9117 - val_mean_squared_error: 55.9117\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 1s 114us/sample - loss: 58.5182 - mean_squared_error: 58.5182 - val_loss: 56.0209 - val_mean_squared_error: 56.0210\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 58.6787 - mean_squared_error: 58.6787 - val_loss: 55.8639 - val_mean_squared_error: 55.8639\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 58.6973 - mean_squared_error: 58.6973 - val_loss: 56.0237 - val_mean_squared_error: 56.0237\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 58.7918 - mean_squared_error: 58.7918 - val_loss: 56.5875 - val_mean_squared_error: 56.5875\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.6345 - mean_squared_error: 58.6345 - val_loss: 56.0337 - val_mean_squared_error: 56.0337\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.8475 - mean_squared_error: 58.8475 - val_loss: 57.6303 - val_mean_squared_error: 57.6303\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 59.2340 - mean_squared_error: 59.2340 - val_loss: 56.4818 - val_mean_squared_error: 56.4818\n",
      "Epoch 162/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.5597 - mean_squared_error: 58.5596 - val_loss: 55.8416 - val_mean_squared_error: 55.8416\n",
      "Epoch 163/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.6443 - mean_squared_error: 58.6443 - val_loss: 56.1188 - val_mean_squared_error: 56.1188\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.7663 - mean_squared_error: 58.7663 - val_loss: 56.5209 - val_mean_squared_error: 56.5209\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.6160 - mean_squared_error: 58.6160 - val_loss: 57.1217 - val_mean_squared_error: 57.1217\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.8831 - mean_squared_error: 58.8831 - val_loss: 56.3094 - val_mean_squared_error: 56.3094\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 58.6537 - mean_squared_error: 58.6537 - val_loss: 55.7604 - val_mean_squared_error: 55.7604\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 58.6808 - mean_squared_error: 58.6808 - val_loss: 56.2623 - val_mean_squared_error: 56.2623\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 58.8371 - mean_squared_error: 58.8371 - val_loss: 57.7547 - val_mean_squared_error: 57.7547\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.8331 - mean_squared_error: 58.8331 - val_loss: 56.0121 - val_mean_squared_error: 56.0121\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.4248 - mean_squared_error: 58.4249 - val_loss: 55.7714 - val_mean_squared_error: 55.7714\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.3976 - mean_squared_error: 58.3976 - val_loss: 55.7592 - val_mean_squared_error: 55.7592\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.2415 - mean_squared_error: 58.2415 - val_loss: 55.7697 - val_mean_squared_error: 55.7697\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.7022 - mean_squared_error: 58.7022 - val_loss: 55.7614 - val_mean_squared_error: 55.7614\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.7361 - mean_squared_error: 58.7361 - val_loss: 55.7251 - val_mean_squared_error: 55.7251\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.6419 - mean_squared_error: 58.6419 - val_loss: 55.7085 - val_mean_squared_error: 55.7085\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.5520 - mean_squared_error: 58.5520 - val_loss: 57.3298 - val_mean_squared_error: 57.3298\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.8086 - mean_squared_error: 58.8086 - val_loss: 55.8910 - val_mean_squared_error: 55.8910\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3809 - mean_squared_error: 58.3809 - val_loss: 55.7762 - val_mean_squared_error: 55.7762\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.4343 - mean_squared_error: 58.4343 - val_loss: 55.8430 - val_mean_squared_error: 55.8430\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3516 - mean_squared_error: 58.3516 - val_loss: 55.8196 - val_mean_squared_error: 55.8196\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.5740 - mean_squared_error: 58.5740 - val_loss: 56.7963 - val_mean_squared_error: 56.7963\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.6156 - mean_squared_error: 58.6156 - val_loss: 55.6678 - val_mean_squared_error: 55.6678\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 58.3619 - mean_squared_error: 58.3618 - val_loss: 56.3969 - val_mean_squared_error: 56.3969\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 58.8182 - mean_squared_error: 58.8182 - val_loss: 55.7108 - val_mean_squared_error: 55.7108\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 58.6944 - mean_squared_error: 58.6944 - val_loss: 55.7632 - val_mean_squared_error: 55.7632\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 58.4733 - mean_squared_error: 58.4733 - val_loss: 56.1481 - val_mean_squared_error: 56.1481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2617 - mean_squared_error: 58.2617 - val_loss: 55.6124 - val_mean_squared_error: 55.6124\n",
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.5324 - mean_squared_error: 58.5324 - val_loss: 56.5411 - val_mean_squared_error: 56.5411\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.5209 - mean_squared_error: 58.5209 - val_loss: 55.7365 - val_mean_squared_error: 55.7365\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 58.6057 - mean_squared_error: 58.6057 - val_loss: 55.6394 - val_mean_squared_error: 55.6394\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 58.5244 - mean_squared_error: 58.5244 - val_loss: 55.7592 - val_mean_squared_error: 55.7592\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 58.3998 - mean_squared_error: 58.3998 - val_loss: 55.7695 - val_mean_squared_error: 55.7695\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 58.2132 - mean_squared_error: 58.2132 - val_loss: 56.1346 - val_mean_squared_error: 56.1346\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 58.4417 - mean_squared_error: 58.4417 - val_loss: 56.1069 - val_mean_squared_error: 56.1069\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.2552 - mean_squared_error: 58.2552 - val_loss: 56.8226 - val_mean_squared_error: 56.8226\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.6052 - mean_squared_error: 58.6052 - val_loss: 58.0716 - val_mean_squared_error: 58.0716\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 58.4356 - mean_squared_error: 58.4356 - val_loss: 55.7036 - val_mean_squared_error: 55.7036\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.0337 - mean_squared_error: 59.0337 - val_loss: 55.5717 - val_mean_squared_error: 55.5717\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.1930 - mean_squared_error: 58.1930 - val_loss: 55.6337 - val_mean_squared_error: 55.6336\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.3910 - mean_squared_error: 58.3910 - val_loss: 56.3079 - val_mean_squared_error: 56.3079\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2743 - mean_squared_error: 58.2743 - val_loss: 57.0167 - val_mean_squared_error: 57.0167\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.7204 - mean_squared_error: 58.7204 - val_loss: 55.6578 - val_mean_squared_error: 55.6578\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0637 - mean_squared_error: 58.0637 - val_loss: 55.5597 - val_mean_squared_error: 55.5597\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2441 - mean_squared_error: 58.2441 - val_loss: 55.5049 - val_mean_squared_error: 55.5049\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.4883 - mean_squared_error: 58.4883 - val_loss: 55.5017 - val_mean_squared_error: 55.5017\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3083 - mean_squared_error: 58.3083 - val_loss: 56.5678 - val_mean_squared_error: 56.5678\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2112 - mean_squared_error: 58.2112 - val_loss: 55.7082 - val_mean_squared_error: 55.7082\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.1819 - mean_squared_error: 58.1819 - val_loss: 55.5608 - val_mean_squared_error: 55.5608\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.4798 - mean_squared_error: 58.4798 - val_loss: 55.6741 - val_mean_squared_error: 55.6741\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.5073 - mean_squared_error: 58.5073 - val_loss: 55.7887 - val_mean_squared_error: 55.7887\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 58.2640 - mean_squared_error: 58.2640 - val_loss: 56.5248 - val_mean_squared_error: 56.5248\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.2174 - mean_squared_error: 58.2174 - val_loss: 56.9640 - val_mean_squared_error: 56.9640\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.2415 - mean_squared_error: 58.2415 - val_loss: 56.1797 - val_mean_squared_error: 56.1798\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.5884 - mean_squared_error: 58.5884 - val_loss: 55.7721 - val_mean_squared_error: 55.7721\n",
      "Epoch 216/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.1722 - mean_squared_error: 58.1722 - val_loss: 57.1316 - val_mean_squared_error: 57.1316\n",
      "Epoch 217/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2808 - mean_squared_error: 58.2808 - val_loss: 57.1917 - val_mean_squared_error: 57.1917\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.0306 - mean_squared_error: 58.0306 - val_loss: 55.6251 - val_mean_squared_error: 55.6251\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9789 - mean_squared_error: 57.9789 - val_loss: 55.7609 - val_mean_squared_error: 55.7609\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2863 - mean_squared_error: 58.2863 - val_loss: 55.4596 - val_mean_squared_error: 55.4596\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.6796 - mean_squared_error: 58.6796 - val_loss: 55.7301 - val_mean_squared_error: 55.7301\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.1069 - mean_squared_error: 58.1069 - val_loss: 56.5438 - val_mean_squared_error: 56.5438\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.0073 - mean_squared_error: 58.0072 - val_loss: 55.7642 - val_mean_squared_error: 55.7643\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.1469 - mean_squared_error: 58.1469 - val_loss: 55.7227 - val_mean_squared_error: 55.7227\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.2977 - mean_squared_error: 58.2976 - val_loss: 55.5374 - val_mean_squared_error: 55.5374\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.9385 - mean_squared_error: 57.9385 - val_loss: 55.8372 - val_mean_squared_error: 55.8372\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.1274 - mean_squared_error: 58.1274 - val_loss: 56.0460 - val_mean_squared_error: 56.0460\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9863 - mean_squared_error: 57.9863 - val_loss: 55.4597 - val_mean_squared_error: 55.4597\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.0546 - mean_squared_error: 58.0546 - val_loss: 57.6048 - val_mean_squared_error: 57.6048\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0765 - mean_squared_error: 58.0765 - val_loss: 55.8789 - val_mean_squared_error: 55.8789\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.4575 - mean_squared_error: 58.4575 - val_loss: 56.6222 - val_mean_squared_error: 56.6222\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0701 - mean_squared_error: 58.0702 - val_loss: 55.6410 - val_mean_squared_error: 55.6410\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.0591 - mean_squared_error: 58.0592 - val_loss: 55.4017 - val_mean_squared_error: 55.4017\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.4138 - mean_squared_error: 58.4138 - val_loss: 56.1513 - val_mean_squared_error: 56.1513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.9292 - mean_squared_error: 57.9292 - val_loss: 55.3581 - val_mean_squared_error: 55.3581\n",
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.9631 - mean_squared_error: 57.9631 - val_loss: 55.9113 - val_mean_squared_error: 55.9113\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1096 - mean_squared_error: 58.1096 - val_loss: 55.6972 - val_mean_squared_error: 55.6972\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.2841 - mean_squared_error: 58.2841 - val_loss: 55.7615 - val_mean_squared_error: 55.7615\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7757 - mean_squared_error: 57.7757 - val_loss: 55.4557 - val_mean_squared_error: 55.4557\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0016 - mean_squared_error: 58.0016 - val_loss: 56.6115 - val_mean_squared_error: 56.6115\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2538 - mean_squared_error: 58.2538 - val_loss: 55.4526 - val_mean_squared_error: 55.4526\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 57.9349 - mean_squared_error: 57.9349 - val_loss: 55.4177 - val_mean_squared_error: 55.4177\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.9420 - mean_squared_error: 57.9420 - val_loss: 55.4701 - val_mean_squared_error: 55.4701\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 58.2309 - mean_squared_error: 58.2309 - val_loss: 56.8150 - val_mean_squared_error: 56.8150\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 58.1988 - mean_squared_error: 58.1988 - val_loss: 55.3238 - val_mean_squared_error: 55.3238\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 58.0818 - mean_squared_error: 58.0818 - val_loss: 57.7793 - val_mean_squared_error: 57.7793\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.1959 - mean_squared_error: 58.1958 - val_loss: 56.7677 - val_mean_squared_error: 56.7677\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2264 - mean_squared_error: 58.2264 - val_loss: 56.4825 - val_mean_squared_error: 56.4825\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.0006 - mean_squared_error: 58.0006 - val_loss: 55.7430 - val_mean_squared_error: 55.7430\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.4180 - mean_squared_error: 58.4180 - val_loss: 55.7376 - val_mean_squared_error: 55.7376\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2665 - mean_squared_error: 58.2665 - val_loss: 55.8790 - val_mean_squared_error: 55.8790\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8011 - mean_squared_error: 57.8012 - val_loss: 56.2507 - val_mean_squared_error: 56.2507\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.2980 - mean_squared_error: 58.2981 - val_loss: 56.0567 - val_mean_squared_error: 56.0568\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0379 - mean_squared_error: 58.0379 - val_loss: 55.9357 - val_mean_squared_error: 55.9357\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.0364 - mean_squared_error: 58.0364 - val_loss: 56.0982 - val_mean_squared_error: 56.0982\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7862 - mean_squared_error: 57.7862 - val_loss: 55.8937 - val_mean_squared_error: 55.8937\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0869 - mean_squared_error: 58.0869 - val_loss: 55.6091 - val_mean_squared_error: 55.6091\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3251 - mean_squared_error: 58.3252 - val_loss: 55.6249 - val_mean_squared_error: 55.6249\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9884 - mean_squared_error: 57.9884 - val_loss: 55.3156 - val_mean_squared_error: 55.3156\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.2131 - mean_squared_error: 58.2131 - val_loss: 57.5360 - val_mean_squared_error: 57.5361\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2064 - mean_squared_error: 58.2064 - val_loss: 55.3140 - val_mean_squared_error: 55.3140\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9547 - mean_squared_error: 57.9547 - val_loss: 55.6771 - val_mean_squared_error: 55.6771\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0540 - mean_squared_error: 58.0540 - val_loss: 56.6491 - val_mean_squared_error: 56.6491\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1837 - mean_squared_error: 58.1838 - val_loss: 57.3207 - val_mean_squared_error: 57.3207\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9568 - mean_squared_error: 57.9568 - val_loss: 55.3091 - val_mean_squared_error: 55.3091\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9949 - mean_squared_error: 57.9949 - val_loss: 55.3153 - val_mean_squared_error: 55.3153\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.1207 - mean_squared_error: 58.1207 - val_loss: 55.9988 - val_mean_squared_error: 55.9987\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9821 - mean_squared_error: 57.9821 - val_loss: 55.3083 - val_mean_squared_error: 55.3083\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8523 - mean_squared_error: 57.8523 - val_loss: 56.1085 - val_mean_squared_error: 56.1085\n",
      "Epoch 270/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.8693 - mean_squared_error: 57.8693 - val_loss: 57.5148 - val_mean_squared_error: 57.5148\n",
      "Epoch 271/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8836 - mean_squared_error: 57.8836 - val_loss: 55.5800 - val_mean_squared_error: 55.5800\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9488 - mean_squared_error: 57.9488 - val_loss: 55.5647 - val_mean_squared_error: 55.5646\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8073 - mean_squared_error: 57.8073 - val_loss: 56.7591 - val_mean_squared_error: 56.7591\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7439 - mean_squared_error: 57.7439 - val_loss: 55.3180 - val_mean_squared_error: 55.3180\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1402 - mean_squared_error: 58.1402 - val_loss: 55.6156 - val_mean_squared_error: 55.6156\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.6789 - mean_squared_error: 57.6789 - val_loss: 55.5808 - val_mean_squared_error: 55.5808\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8574 - mean_squared_error: 57.8574 - val_loss: 55.2663 - val_mean_squared_error: 55.2663\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7897 - mean_squared_error: 57.7897 - val_loss: 55.7807 - val_mean_squared_error: 55.7808\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9064 - mean_squared_error: 57.9064 - val_loss: 56.1962 - val_mean_squared_error: 56.1962\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7996 - mean_squared_error: 57.7996 - val_loss: 55.2320 - val_mean_squared_error: 55.2320\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.9720 - mean_squared_error: 57.9720 - val_loss: 55.5779 - val_mean_squared_error: 55.5779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8616 - mean_squared_error: 57.8616 - val_loss: 55.2306 - val_mean_squared_error: 55.2306\n",
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9079 - mean_squared_error: 57.9079 - val_loss: 55.4501 - val_mean_squared_error: 55.4501\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7048 - mean_squared_error: 57.7048 - val_loss: 55.4064 - val_mean_squared_error: 55.4064\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9303 - mean_squared_error: 57.9303 - val_loss: 55.3633 - val_mean_squared_error: 55.3633\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.6545 - mean_squared_error: 57.6545 - val_loss: 55.7425 - val_mean_squared_error: 55.7424\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.7630 - mean_squared_error: 57.7630 - val_loss: 55.3557 - val_mean_squared_error: 55.3557\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8297 - mean_squared_error: 57.8297 - val_loss: 55.2526 - val_mean_squared_error: 55.2526\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9668 - mean_squared_error: 57.9668 - val_loss: 55.2293 - val_mean_squared_error: 55.2293\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.8905 - mean_squared_error: 57.8904 - val_loss: 56.2224 - val_mean_squared_error: 56.2224\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7624 - mean_squared_error: 57.7624 - val_loss: 55.3314 - val_mean_squared_error: 55.3314\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.0835 - mean_squared_error: 58.0836 - val_loss: 55.2780 - val_mean_squared_error: 55.2780\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.6994 - mean_squared_error: 57.6994 - val_loss: 55.5255 - val_mean_squared_error: 55.5255\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7847 - mean_squared_error: 57.7847 - val_loss: 55.4709 - val_mean_squared_error: 55.4709\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7849 - mean_squared_error: 57.7849 - val_loss: 55.2187 - val_mean_squared_error: 55.2187\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.6559 - mean_squared_error: 57.6559 - val_loss: 55.2592 - val_mean_squared_error: 55.2592\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0613 - mean_squared_error: 58.0612 - val_loss: 55.1788 - val_mean_squared_error: 55.1788\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7965 - mean_squared_error: 57.7964 - val_loss: 55.1545 - val_mean_squared_error: 55.1545\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.9763 - mean_squared_error: 57.9763 - val_loss: 55.3856 - val_mean_squared_error: 55.3856\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7213 - mean_squared_error: 57.7214 - val_loss: 55.1782 - val_mean_squared_error: 55.1782\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8101 - mean_squared_error: 57.8101 - val_loss: 55.7196 - val_mean_squared_error: 55.7196\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7336 - mean_squared_error: 57.7336 - val_loss: 55.6240 - val_mean_squared_error: 55.6240\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.7123 - mean_squared_error: 57.7123 - val_loss: 56.1038 - val_mean_squared_error: 56.1038\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.0745 - mean_squared_error: 58.0745 - val_loss: 55.2699 - val_mean_squared_error: 55.2699\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8712 - mean_squared_error: 57.8712 - val_loss: 55.1445 - val_mean_squared_error: 55.1445\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6150 - mean_squared_error: 57.6150 - val_loss: 55.4593 - val_mean_squared_error: 55.4593\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7059 - mean_squared_error: 57.7059 - val_loss: 56.7801 - val_mean_squared_error: 56.7801\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6560 - mean_squared_error: 57.6560 - val_loss: 55.9445 - val_mean_squared_error: 55.9445\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.0004 - mean_squared_error: 58.0004 - val_loss: 55.1313 - val_mean_squared_error: 55.1313\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.6619 - mean_squared_error: 57.6619 - val_loss: 55.1716 - val_mean_squared_error: 55.1716\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9410 - mean_squared_error: 57.9410 - val_loss: 55.3651 - val_mean_squared_error: 55.3651\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.6488 - mean_squared_error: 57.6488 - val_loss: 57.0466 - val_mean_squared_error: 57.0466\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8091 - mean_squared_error: 57.8091 - val_loss: 55.7573 - val_mean_squared_error: 55.7573\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7971 - mean_squared_error: 57.7971 - val_loss: 55.4721 - val_mean_squared_error: 55.4721\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7335 - mean_squared_error: 57.7335 - val_loss: 55.7050 - val_mean_squared_error: 55.7050\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.7828 - mean_squared_error: 57.7827 - val_loss: 55.1739 - val_mean_squared_error: 55.1739\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7650 - mean_squared_error: 57.7650 - val_loss: 55.3190 - val_mean_squared_error: 55.3190\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7122 - mean_squared_error: 57.7122 - val_loss: 55.7911 - val_mean_squared_error: 55.7911\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6789 - mean_squared_error: 57.6789 - val_loss: 55.5265 - val_mean_squared_error: 55.5265\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.6369 - mean_squared_error: 57.6370 - val_loss: 55.1719 - val_mean_squared_error: 55.1719\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.4879 - mean_squared_error: 57.4879 - val_loss: 55.4760 - val_mean_squared_error: 55.4760\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8362 - mean_squared_error: 57.8362 - val_loss: 55.3973 - val_mean_squared_error: 55.3973\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8122 - mean_squared_error: 57.8122 - val_loss: 55.4904 - val_mean_squared_error: 55.4904\n",
      "Epoch 324/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7749 - mean_squared_error: 57.7749 - val_loss: 55.2046 - val_mean_squared_error: 55.2046\n",
      "Epoch 325/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.6242 - mean_squared_error: 57.6242 - val_loss: 55.6559 - val_mean_squared_error: 55.6559\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0448 - mean_squared_error: 58.0448 - val_loss: 55.1100 - val_mean_squared_error: 55.1100\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5623 - mean_squared_error: 57.5622 - val_loss: 55.1377 - val_mean_squared_error: 55.1377\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5493 - mean_squared_error: 57.5493 - val_loss: 55.2100 - val_mean_squared_error: 55.2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9299 - mean_squared_error: 57.9299 - val_loss: 55.8250 - val_mean_squared_error: 55.8249\n",
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.5148 - mean_squared_error: 57.5148 - val_loss: 55.3156 - val_mean_squared_error: 55.3156\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 57.6433 - mean_squared_error: 57.6433 - val_loss: 55.2460 - val_mean_squared_error: 55.2460\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.6399 - mean_squared_error: 57.6398 - val_loss: 55.2691 - val_mean_squared_error: 55.2691\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 57.5623 - mean_squared_error: 57.5623 - val_loss: 55.6560 - val_mean_squared_error: 55.6560\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 57.7643 - mean_squared_error: 57.7643 - val_loss: 55.9803 - val_mean_squared_error: 55.9803\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 58.0746 - mean_squared_error: 58.0746 - val_loss: 55.1144 - val_mean_squared_error: 55.1144\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.5272 - mean_squared_error: 57.5272 - val_loss: 56.2611 - val_mean_squared_error: 56.2611\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.8996 - mean_squared_error: 57.8996 - val_loss: 55.1302 - val_mean_squared_error: 55.1302\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6319 - mean_squared_error: 57.6319 - val_loss: 55.7340 - val_mean_squared_error: 55.7340\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.3966 - mean_squared_error: 57.3966 - val_loss: 55.4724 - val_mean_squared_error: 55.4724\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6335 - mean_squared_error: 57.6335 - val_loss: 55.8119 - val_mean_squared_error: 55.8119\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.5243 - mean_squared_error: 57.5243 - val_loss: 57.1197 - val_mean_squared_error: 57.1197\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.5110 - mean_squared_error: 57.5110 - val_loss: 55.3776 - val_mean_squared_error: 55.3776\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7191 - mean_squared_error: 57.7191 - val_loss: 56.3994 - val_mean_squared_error: 56.3993\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.5927 - mean_squared_error: 57.5927 - val_loss: 55.1323 - val_mean_squared_error: 55.1323\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.4584 - mean_squared_error: 57.4584 - val_loss: 55.1996 - val_mean_squared_error: 55.1996\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.6132 - mean_squared_error: 57.6132 - val_loss: 55.1732 - val_mean_squared_error: 55.1732\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 57.8612 - mean_squared_error: 57.8613 - val_loss: 55.4612 - val_mean_squared_error: 55.4613\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.2115 - mean_squared_error: 57.2115 - val_loss: 57.5614 - val_mean_squared_error: 57.5614\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.9922 - mean_squared_error: 57.9921 - val_loss: 55.0612 - val_mean_squared_error: 55.0612\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.4078 - mean_squared_error: 57.4078 - val_loss: 55.0806 - val_mean_squared_error: 55.0806\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.3924 - mean_squared_error: 57.3924 - val_loss: 55.1620 - val_mean_squared_error: 55.1620\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 57.6258 - mean_squared_error: 57.6258 - val_loss: 55.9318 - val_mean_squared_error: 55.9318\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.3439 - mean_squared_error: 57.3439 - val_loss: 55.2358 - val_mean_squared_error: 55.2358\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.7601 - mean_squared_error: 57.7601 - val_loss: 55.1582 - val_mean_squared_error: 55.1582\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.8380 - mean_squared_error: 57.8380 - val_loss: 55.5212 - val_mean_squared_error: 55.5212\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8976 - mean_squared_error: 57.8975 - val_loss: 55.0772 - val_mean_squared_error: 55.0772\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6022 - mean_squared_error: 57.6023 - val_loss: 55.8643 - val_mean_squared_error: 55.8643\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.4685 - mean_squared_error: 57.4684 - val_loss: 55.1390 - val_mean_squared_error: 55.1390\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.3767 - mean_squared_error: 57.3767 - val_loss: 55.1582 - val_mean_squared_error: 55.1582\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 89us/sample - loss: 57.8456 - mean_squared_error: 57.8455 - val_loss: 56.1355 - val_mean_squared_error: 56.1355\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.3610 - mean_squared_error: 57.3610 - val_loss: 56.1369 - val_mean_squared_error: 56.1369\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.4654 - mean_squared_error: 57.4654 - val_loss: 55.2317 - val_mean_squared_error: 55.2317\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 57.3512 - mean_squared_error: 57.3512 - val_loss: 55.1701 - val_mean_squared_error: 55.1701\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7801 - mean_squared_error: 57.7801 - val_loss: 55.0250 - val_mean_squared_error: 55.0250\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.6871 - mean_squared_error: 57.6871 - val_loss: 55.3361 - val_mean_squared_error: 55.3361\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.7175 - mean_squared_error: 57.7175 - val_loss: 56.0958 - val_mean_squared_error: 56.0958\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.3863 - mean_squared_error: 57.3863 - val_loss: 55.8924 - val_mean_squared_error: 55.8924\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.5703 - mean_squared_error: 57.5703 - val_loss: 55.1482 - val_mean_squared_error: 55.1482\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.3734 - mean_squared_error: 57.3734 - val_loss: 55.9073 - val_mean_squared_error: 55.9073\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5505 - mean_squared_error: 57.5505 - val_loss: 55.1035 - val_mean_squared_error: 55.1035\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.5580 - mean_squared_error: 57.5580 - val_loss: 56.8672 - val_mean_squared_error: 56.8672\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.4655 - mean_squared_error: 57.4655 - val_loss: 57.0793 - val_mean_squared_error: 57.0793\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.4822 - mean_squared_error: 57.4822 - val_loss: 56.2650 - val_mean_squared_error: 56.2650\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.7557 - mean_squared_error: 57.7557 - val_loss: 56.8678 - val_mean_squared_error: 56.8678\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 57.5219 - mean_squared_error: 57.5219 - val_loss: 55.0290 - val_mean_squared_error: 55.0290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 57.4209 - mean_squared_error: 57.4209 - val_loss: 57.3944 - val_mean_squared_error: 57.3944\n",
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 57.4597 - mean_squared_error: 57.4597 - val_loss: 55.0191 - val_mean_squared_error: 55.0191\n",
      "Epoch 378/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 57.7274 - mean_squared_error: 57.7274 - val_loss: 56.4136 - val_mean_squared_error: 56.4136\n",
      "Epoch 379/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 57.3296 - mean_squared_error: 57.3296 - val_loss: 55.0241 - val_mean_squared_error: 55.0241\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 57.4486 - mean_squared_error: 57.4486 - val_loss: 55.0495 - val_mean_squared_error: 55.0495\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 57.5915 - mean_squared_error: 57.5915 - val_loss: 55.9724 - val_mean_squared_error: 55.9724\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 57.5207 - mean_squared_error: 57.5207 - val_loss: 55.0849 - val_mean_squared_error: 55.0849\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 57.5968 - mean_squared_error: 57.5969 - val_loss: 59.0863 - val_mean_squared_error: 59.0863\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.5950 - mean_squared_error: 57.5950 - val_loss: 55.2610 - val_mean_squared_error: 55.2610\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.8869 - mean_squared_error: 57.8869 - val_loss: 55.7151 - val_mean_squared_error: 55.7151\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 1s 124us/sample - loss: 57.3756 - mean_squared_error: 57.3756 - val_loss: 55.2962 - val_mean_squared_error: 55.2962\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 1s 111us/sample - loss: 57.2858 - mean_squared_error: 57.2858 - val_loss: 56.4233 - val_mean_squared_error: 56.4233\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 57.1074 - mean_squared_error: 57.1074 - val_loss: 55.2253 - val_mean_squared_error: 55.2253\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 1s 115us/sample - loss: 57.8479 - mean_squared_error: 57.8479 - val_loss: 55.0089 - val_mean_squared_error: 55.0089\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 1s 89us/sample - loss: 57.5683 - mean_squared_error: 57.5684 - val_loss: 55.1371 - val_mean_squared_error: 55.1372\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 57.4839 - mean_squared_error: 57.4839 - val_loss: 56.8603 - val_mean_squared_error: 56.8603\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 1s 97us/sample - loss: 57.5509 - mean_squared_error: 57.5509 - val_loss: 55.6969 - val_mean_squared_error: 55.6969\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 57.4793 - mean_squared_error: 57.4793 - val_loss: 55.2180 - val_mean_squared_error: 55.2180\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 57.3052 - mean_squared_error: 57.3052 - val_loss: 55.3349 - val_mean_squared_error: 55.3349\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 57.3644 - mean_squared_error: 57.3644 - val_loss: 55.0164 - val_mean_squared_error: 55.0164\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 57.2241 - mean_squared_error: 57.2241 - val_loss: 54.9937 - val_mean_squared_error: 54.9937\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.5521 - mean_squared_error: 57.5521 - val_loss: 55.0299 - val_mean_squared_error: 55.0299\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.3203 - mean_squared_error: 57.3203 - val_loss: 55.0950 - val_mean_squared_error: 55.0950\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.4768 - mean_squared_error: 57.4768 - val_loss: 55.1229 - val_mean_squared_error: 55.1229\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.3395 - mean_squared_error: 57.3395 - val_loss: 56.0048 - val_mean_squared_error: 56.0048\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.2689 - mean_squared_error: 57.2689 - val_loss: 55.2850 - val_mean_squared_error: 55.2850\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 57.5196 - mean_squared_error: 57.5196 - val_loss: 55.2617 - val_mean_squared_error: 55.2617\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 57.3424 - mean_squared_error: 57.3424 - val_loss: 58.3634 - val_mean_squared_error: 58.3634\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.3433 - mean_squared_error: 57.3433 - val_loss: 55.1730 - val_mean_squared_error: 55.1730\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 57.5474 - mean_squared_error: 57.5474 - val_loss: 54.9538 - val_mean_squared_error: 54.9538\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.4020 - mean_squared_error: 57.4020 - val_loss: 55.1681 - val_mean_squared_error: 55.1681\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.3628 - mean_squared_error: 57.3628 - val_loss: 55.0440 - val_mean_squared_error: 55.0440\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 57.4263 - mean_squared_error: 57.4264 - val_loss: 55.3418 - val_mean_squared_error: 55.3418\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 57.6070 - mean_squared_error: 57.6070 - val_loss: 55.0555 - val_mean_squared_error: 55.0555\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 57.3356 - mean_squared_error: 57.3356 - val_loss: 54.9491 - val_mean_squared_error: 54.9491\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.2660 - mean_squared_error: 57.2660 - val_loss: 55.1379 - val_mean_squared_error: 55.1379\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 57.3639 - mean_squared_error: 57.3639 - val_loss: 55.5022 - val_mean_squared_error: 55.5022\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.2693 - mean_squared_error: 57.2693 - val_loss: 55.0860 - val_mean_squared_error: 55.0860\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.4990 - mean_squared_error: 57.4990 - val_loss: 54.9506 - val_mean_squared_error: 54.9506\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 57.5398 - mean_squared_error: 57.5398 - val_loss: 57.9701 - val_mean_squared_error: 57.9701\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 57.3032 - mean_squared_error: 57.3032 - val_loss: 54.9277 - val_mean_squared_error: 54.9277\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 57.3716 - mean_squared_error: 57.3715 - val_loss: 54.9394 - val_mean_squared_error: 54.9394\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 57.2092 - mean_squared_error: 57.2092 - val_loss: 54.9456 - val_mean_squared_error: 54.9456\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 89us/sample - loss: 57.3587 - mean_squared_error: 57.3587 - val_loss: 55.0930 - val_mean_squared_error: 55.0930\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 1s 108us/sample - loss: 57.2432 - mean_squared_error: 57.2432 - val_loss: 56.0049 - val_mean_squared_error: 56.0049\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 57.4613 - mean_squared_error: 57.4613 - val_loss: 57.1243 - val_mean_squared_error: 57.1243\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 57.2810 - mean_squared_error: 57.2810 - val_loss: 55.6024 - val_mean_squared_error: 55.6024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7060 - mean_squared_error: 57.7060 - val_loss: 57.9237 - val_mean_squared_error: 57.9237\n",
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 57.3172 - mean_squared_error: 57.3172 - val_loss: 57.5966 - val_mean_squared_error: 57.5966\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.5006 - mean_squared_error: 57.5006 - val_loss: 55.0006 - val_mean_squared_error: 55.0005\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 57.4969 - mean_squared_error: 57.4969 - val_loss: 55.0195 - val_mean_squared_error: 55.0195\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 57.2946 - mean_squared_error: 57.2946 - val_loss: 56.5771 - val_mean_squared_error: 56.5771\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 57.4157 - mean_squared_error: 57.4157 - val_loss: 55.3452 - val_mean_squared_error: 55.3452\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 1s 98us/sample - loss: 57.0958 - mean_squared_error: 57.0958 - val_loss: 55.0883 - val_mean_squared_error: 55.0883\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 57.5187 - mean_squared_error: 57.5187 - val_loss: 55.0077 - val_mean_squared_error: 55.0077\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 1s 111us/sample - loss: 57.1402 - mean_squared_error: 57.1402 - val_loss: 55.0472 - val_mean_squared_error: 55.0472\n",
      "Epoch 432/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 57.4795 - mean_squared_error: 57.4795 - val_loss: 55.1219 - val_mean_squared_error: 55.1219\n",
      "Epoch 433/1000\n",
      "5600/5600 [==============================] - 1s 125us/sample - loss: 57.2826 - mean_squared_error: 57.2826 - val_loss: 56.5070 - val_mean_squared_error: 56.5071\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 57.5274 - mean_squared_error: 57.5274 - val_loss: 55.4408 - val_mean_squared_error: 55.4408\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 57.1103 - mean_squared_error: 57.1103 - val_loss: 55.0552 - val_mean_squared_error: 55.0552\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 57.2817 - mean_squared_error: 57.2817 - val_loss: 56.9990 - val_mean_squared_error: 56.9990\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 57.2914 - mean_squared_error: 57.2914 - val_loss: 54.9748 - val_mean_squared_error: 54.9748\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.3478 - mean_squared_error: 57.3478 - val_loss: 55.5496 - val_mean_squared_error: 55.5496\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 57.3688 - mean_squared_error: 57.3688 - val_loss: 54.9826 - val_mean_squared_error: 54.9826\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 57.8517 - mean_squared_error: 57.8517 - val_loss: 55.1950 - val_mean_squared_error: 55.1950\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 57.1274 - mean_squared_error: 57.1274 - val_loss: 54.9883 - val_mean_squared_error: 54.9883\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 56.9811 - mean_squared_error: 56.9811 - val_loss: 55.0403 - val_mean_squared_error: 55.0403\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 1s 97us/sample - loss: 57.5480 - mean_squared_error: 57.5480 - val_loss: 55.3711 - val_mean_squared_error: 55.3711\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 56.9794 - mean_squared_error: 56.9794 - val_loss: 55.7703 - val_mean_squared_error: 55.7703\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 57.0601 - mean_squared_error: 57.0600 - val_loss: 54.9236 - val_mean_squared_error: 54.9236\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.2546 - mean_squared_error: 57.2546 - val_loss: 56.1989 - val_mean_squared_error: 56.1989\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.3923 - mean_squared_error: 57.3923 - val_loss: 54.9514 - val_mean_squared_error: 54.9514\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.3410 - mean_squared_error: 57.3410 - val_loss: 55.7238 - val_mean_squared_error: 55.7238\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.1004 - mean_squared_error: 57.1004 - val_loss: 55.0070 - val_mean_squared_error: 55.0070\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.1777 - mean_squared_error: 57.1777 - val_loss: 56.4441 - val_mean_squared_error: 56.4441\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.4751 - mean_squared_error: 57.4750 - val_loss: 55.1223 - val_mean_squared_error: 55.1223\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.2984 - mean_squared_error: 57.2984 - val_loss: 55.8667 - val_mean_squared_error: 55.8667\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 57.0427 - mean_squared_error: 57.0427 - val_loss: 54.9694 - val_mean_squared_error: 54.9694\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 56.9373 - mean_squared_error: 56.9373 - val_loss: 54.9030 - val_mean_squared_error: 54.9030\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.2547 - mean_squared_error: 57.2547 - val_loss: 55.5148 - val_mean_squared_error: 55.5148\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 57.1283 - mean_squared_error: 57.1283 - val_loss: 54.9845 - val_mean_squared_error: 54.9845\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.1366 - mean_squared_error: 57.1366 - val_loss: 55.3304 - val_mean_squared_error: 55.3304\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 56.9818 - mean_squared_error: 56.9818 - val_loss: 55.1905 - val_mean_squared_error: 55.1904\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.0790 - mean_squared_error: 57.0790 - val_loss: 56.4130 - val_mean_squared_error: 56.4130\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.4699 - mean_squared_error: 57.4699 - val_loss: 55.5935 - val_mean_squared_error: 55.5935\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.2933 - mean_squared_error: 57.2933 - val_loss: 55.5816 - val_mean_squared_error: 55.5816\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.3902 - mean_squared_error: 57.3902 - val_loss: 55.3948 - val_mean_squared_error: 55.3948\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.2383 - mean_squared_error: 57.2383 - val_loss: 55.7606 - val_mean_squared_error: 55.7606\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 57.0313 - mean_squared_error: 57.0313 - val_loss: 54.9371 - val_mean_squared_error: 54.9371\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.0390 - mean_squared_error: 57.0390 - val_loss: 54.9450 - val_mean_squared_error: 54.9450\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.9982 - mean_squared_error: 56.9982 - val_loss: 55.2927 - val_mean_squared_error: 55.2927\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.9802 - mean_squared_error: 56.9802 - val_loss: 55.4774 - val_mean_squared_error: 55.4774\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2377 - mean_squared_error: 57.2377 - val_loss: 55.2938 - val_mean_squared_error: 55.2938\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.8890 - mean_squared_error: 56.8890 - val_loss: 56.0245 - val_mean_squared_error: 56.0245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.1560 - mean_squared_error: 57.1560 - val_loss: 55.2184 - val_mean_squared_error: 55.2184\n",
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.0713 - mean_squared_error: 57.0713 - val_loss: 54.9836 - val_mean_squared_error: 54.9836\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.9340 - mean_squared_error: 56.9340 - val_loss: 55.0961 - val_mean_squared_error: 55.0961\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.1482 - mean_squared_error: 57.1482 - val_loss: 55.5379 - val_mean_squared_error: 55.5379\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 57.1898 - mean_squared_error: 57.1898 - val_loss: 55.1277 - val_mean_squared_error: 55.1277\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 56.9087 - mean_squared_error: 56.9087 - val_loss: 56.3499 - val_mean_squared_error: 56.3498\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.2903 - mean_squared_error: 57.2904 - val_loss: 54.9733 - val_mean_squared_error: 54.9733\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.3860 - mean_squared_error: 57.3860 - val_loss: 55.0138 - val_mean_squared_error: 55.0138\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.0775 - mean_squared_error: 57.0775 - val_loss: 55.4315 - val_mean_squared_error: 55.4314\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 57.6307 - mean_squared_error: 57.6307 - val_loss: 55.1617 - val_mean_squared_error: 55.1617\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.0215 - mean_squared_error: 57.0216 - val_loss: 55.2236 - val_mean_squared_error: 55.2236\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.0590 - mean_squared_error: 57.0590 - val_loss: 55.8264 - val_mean_squared_error: 55.8264\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.0319 - mean_squared_error: 57.0319 - val_loss: 55.1888 - val_mean_squared_error: 55.1888\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 57.3810 - mean_squared_error: 57.3810 - val_loss: 55.0786 - val_mean_squared_error: 55.0786\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 57.1658 - mean_squared_error: 57.1658 - val_loss: 54.9767 - val_mean_squared_error: 54.9767\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 56.8757 - mean_squared_error: 56.8757 - val_loss: 55.3083 - val_mean_squared_error: 55.3083\n",
      "Epoch 486/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 57.0844 - mean_squared_error: 57.0844 - val_loss: 56.0531 - val_mean_squared_error: 56.0531\n",
      "Epoch 487/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.9906 - mean_squared_error: 56.9906 - val_loss: 55.4740 - val_mean_squared_error: 55.4740\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2409 - mean_squared_error: 57.2409 - val_loss: 55.6211 - val_mean_squared_error: 55.6211\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 56.9403 - mean_squared_error: 56.9404 - val_loss: 54.8828 - val_mean_squared_error: 54.8828\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 56.9749 - mean_squared_error: 56.9749 - val_loss: 54.9848 - val_mean_squared_error: 54.9848\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.0350 - mean_squared_error: 57.0350 - val_loss: 54.9393 - val_mean_squared_error: 54.9393\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.2092 - mean_squared_error: 57.2092 - val_loss: 54.9703 - val_mean_squared_error: 54.9703\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.2332 - mean_squared_error: 57.2332 - val_loss: 59.6874 - val_mean_squared_error: 59.6874\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.3131 - mean_squared_error: 57.3131 - val_loss: 54.9062 - val_mean_squared_error: 54.9062\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.1693 - mean_squared_error: 57.1693 - val_loss: 54.9771 - val_mean_squared_error: 54.9771\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 57.3238 - mean_squared_error: 57.3238 - val_loss: 54.8915 - val_mean_squared_error: 54.8915\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0427 - mean_squared_error: 57.0427 - val_loss: 55.1277 - val_mean_squared_error: 55.1277\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 56.9962 - mean_squared_error: 56.9962 - val_loss: 55.4311 - val_mean_squared_error: 55.4311\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.7639 - mean_squared_error: 56.7640 - val_loss: 54.9642 - val_mean_squared_error: 54.9642\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 56.9168 - mean_squared_error: 56.9168 - val_loss: 55.6989 - val_mean_squared_error: 55.6989\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.6978 - mean_squared_error: 56.6978 - val_loss: 55.1140 - val_mean_squared_error: 55.1140\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 56.8772 - mean_squared_error: 56.8772 - val_loss: 54.9120 - val_mean_squared_error: 54.9120\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.0515 - mean_squared_error: 57.0515 - val_loss: 55.4044 - val_mean_squared_error: 55.4044\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 56.9720 - mean_squared_error: 56.9720 - val_loss: 55.2521 - val_mean_squared_error: 55.2521\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.3887 - mean_squared_error: 57.3887 - val_loss: 55.0589 - val_mean_squared_error: 55.0589\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.7936 - mean_squared_error: 56.7936 - val_loss: 55.0338 - val_mean_squared_error: 55.0338\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.1655 - mean_squared_error: 57.1655 - val_loss: 55.5570 - val_mean_squared_error: 55.5570\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7849 - mean_squared_error: 56.7849 - val_loss: 55.2064 - val_mean_squared_error: 55.2064\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9386 - mean_squared_error: 56.9386 - val_loss: 54.8360 - val_mean_squared_error: 54.8360\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8298 - mean_squared_error: 56.8298 - val_loss: 54.9843 - val_mean_squared_error: 54.9844\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.8677 - mean_squared_error: 56.8677 - val_loss: 54.9301 - val_mean_squared_error: 54.9301\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9455 - mean_squared_error: 56.9455 - val_loss: 55.4786 - val_mean_squared_error: 55.4786\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8603 - mean_squared_error: 56.8603 - val_loss: 55.1398 - val_mean_squared_error: 55.1398\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.0274 - mean_squared_error: 57.0274 - val_loss: 55.6648 - val_mean_squared_error: 55.6648\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9228 - mean_squared_error: 56.9228 - val_loss: 55.3540 - val_mean_squared_error: 55.3540\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.8659 - mean_squared_error: 56.8659 - val_loss: 55.7693 - val_mean_squared_error: 55.7693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7900 - mean_squared_error: 56.7900 - val_loss: 55.0361 - val_mean_squared_error: 55.0361\n",
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0563 - mean_squared_error: 57.0563 - val_loss: 55.0837 - val_mean_squared_error: 55.0837\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8942 - mean_squared_error: 56.8942 - val_loss: 55.0330 - val_mean_squared_error: 55.0330\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2604 - mean_squared_error: 57.2604 - val_loss: 54.8664 - val_mean_squared_error: 54.8664\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8569 - mean_squared_error: 56.8569 - val_loss: 54.9699 - val_mean_squared_error: 54.9699\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8565 - mean_squared_error: 56.8565 - val_loss: 55.6989 - val_mean_squared_error: 55.6989\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9814 - mean_squared_error: 56.9814 - val_loss: 54.9427 - val_mean_squared_error: 54.9427\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.3146 - mean_squared_error: 57.3146 - val_loss: 55.6855 - val_mean_squared_error: 55.6855\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.0936 - mean_squared_error: 57.0936 - val_loss: 55.0062 - val_mean_squared_error: 55.0062\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.9986 - mean_squared_error: 56.9986 - val_loss: 55.0851 - val_mean_squared_error: 55.0851\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.7024 - mean_squared_error: 56.7024 - val_loss: 55.6617 - val_mean_squared_error: 55.6617\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8370 - mean_squared_error: 56.8370 - val_loss: 55.0845 - val_mean_squared_error: 55.0845\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.8586 - mean_squared_error: 56.8586 - val_loss: 54.9076 - val_mean_squared_error: 54.9076\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.8649 - mean_squared_error: 56.8649 - val_loss: 55.5219 - val_mean_squared_error: 55.5220\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7669 - mean_squared_error: 56.7669 - val_loss: 55.3486 - val_mean_squared_error: 55.3487\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7339 - mean_squared_error: 56.7339 - val_loss: 55.2090 - val_mean_squared_error: 55.2090\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.7000 - mean_squared_error: 56.7000 - val_loss: 54.9098 - val_mean_squared_error: 54.9098\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.7700 - mean_squared_error: 56.7700 - val_loss: 55.1005 - val_mean_squared_error: 55.1005\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6464 - mean_squared_error: 56.6464 - val_loss: 54.8657 - val_mean_squared_error: 54.8657\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7417 - mean_squared_error: 56.7417 - val_loss: 54.9296 - val_mean_squared_error: 54.9296\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9896 - mean_squared_error: 56.9896 - val_loss: 56.6796 - val_mean_squared_error: 56.6796\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1718 - mean_squared_error: 57.1718 - val_loss: 55.0017 - val_mean_squared_error: 55.0017\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.7486 - mean_squared_error: 56.7486 - val_loss: 54.8291 - val_mean_squared_error: 54.8291\n",
      "Epoch 540/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8520 - mean_squared_error: 56.8520 - val_loss: 55.1346 - val_mean_squared_error: 55.1346\n",
      "Epoch 541/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7857 - mean_squared_error: 56.7858 - val_loss: 55.8035 - val_mean_squared_error: 55.8035\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9023 - mean_squared_error: 56.9023 - val_loss: 55.1413 - val_mean_squared_error: 55.1413\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9949 - mean_squared_error: 56.9949 - val_loss: 55.1244 - val_mean_squared_error: 55.1244\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6042 - mean_squared_error: 56.6041 - val_loss: 55.6444 - val_mean_squared_error: 55.6444\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.1610 - mean_squared_error: 57.1610 - val_loss: 55.2515 - val_mean_squared_error: 55.2515\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.1789 - mean_squared_error: 57.1789 - val_loss: 56.1319 - val_mean_squared_error: 56.1319\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.1712 - mean_squared_error: 57.1712 - val_loss: 55.0658 - val_mean_squared_error: 55.0658\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.2509 - mean_squared_error: 57.2509 - val_loss: 54.9290 - val_mean_squared_error: 54.9290\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 56.6135 - mean_squared_error: 56.6135 - val_loss: 55.4139 - val_mean_squared_error: 55.4139\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.5319 - mean_squared_error: 56.5319 - val_loss: 55.0932 - val_mean_squared_error: 55.0932\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5203 - mean_squared_error: 56.5203 - val_loss: 55.6741 - val_mean_squared_error: 55.6741\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5409 - mean_squared_error: 56.5409 - val_loss: 55.3588 - val_mean_squared_error: 55.3588\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.8046 - mean_squared_error: 56.8045 - val_loss: 54.9286 - val_mean_squared_error: 54.9286\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.0045 - mean_squared_error: 57.0045 - val_loss: 56.8025 - val_mean_squared_error: 56.8025\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.8214 - mean_squared_error: 56.8214 - val_loss: 54.9238 - val_mean_squared_error: 54.9238\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.2306 - mean_squared_error: 57.2306 - val_loss: 55.0623 - val_mean_squared_error: 55.0623\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6552 - mean_squared_error: 56.6552 - val_loss: 54.9777 - val_mean_squared_error: 54.9777\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7713 - mean_squared_error: 56.7713 - val_loss: 54.8582 - val_mean_squared_error: 54.8582\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6762 - mean_squared_error: 56.6761 - val_loss: 55.0728 - val_mean_squared_error: 55.0728\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 56.5665 - mean_squared_error: 56.5665 - val_loss: 54.9813 - val_mean_squared_error: 54.9813\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 56.8381 - mean_squared_error: 56.8381 - val_loss: 55.4368 - val_mean_squared_error: 55.4368\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 56.7180 - mean_squared_error: 56.7180 - val_loss: 58.8457 - val_mean_squared_error: 58.8457\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 56.7816 - mean_squared_error: 56.7816 - val_loss: 54.8908 - val_mean_squared_error: 54.8908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 56.8436 - mean_squared_error: 56.8437 - val_loss: 54.9924 - val_mean_squared_error: 54.9924\n",
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.6368 - mean_squared_error: 56.6368 - val_loss: 54.9619 - val_mean_squared_error: 54.9619\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6723 - mean_squared_error: 56.6723 - val_loss: 55.3110 - val_mean_squared_error: 55.3110\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4998 - mean_squared_error: 56.4997 - val_loss: 55.0994 - val_mean_squared_error: 55.0994\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.7254 - mean_squared_error: 56.7254 - val_loss: 55.5912 - val_mean_squared_error: 55.5912\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5402 - mean_squared_error: 56.5402 - val_loss: 55.8719 - val_mean_squared_error: 55.8719\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 56.5636 - mean_squared_error: 56.5636 - val_loss: 55.8020 - val_mean_squared_error: 55.8020\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7033 - mean_squared_error: 56.7033 - val_loss: 55.3995 - val_mean_squared_error: 55.3996\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5904 - mean_squared_error: 56.5904 - val_loss: 56.5415 - val_mean_squared_error: 56.5416\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.3221 - mean_squared_error: 57.3221 - val_loss: 56.9101 - val_mean_squared_error: 56.9101\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.8006 - mean_squared_error: 56.8006 - val_loss: 54.9514 - val_mean_squared_error: 54.9514\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5663 - mean_squared_error: 56.5663 - val_loss: 54.9573 - val_mean_squared_error: 54.9573\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5500 - mean_squared_error: 56.5500 - val_loss: 55.8113 - val_mean_squared_error: 55.8113\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.5082 - mean_squared_error: 56.5081 - val_loss: 54.8450 - val_mean_squared_error: 54.8450\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8401 - mean_squared_error: 56.8401 - val_loss: 55.1287 - val_mean_squared_error: 55.1287\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6598 - mean_squared_error: 56.6598 - val_loss: 55.0605 - val_mean_squared_error: 55.0605\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7821 - mean_squared_error: 56.7821 - val_loss: 54.9211 - val_mean_squared_error: 54.9211\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5602 - mean_squared_error: 56.5602 - val_loss: 55.0020 - val_mean_squared_error: 55.0020\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5266 - mean_squared_error: 56.5267 - val_loss: 55.0079 - val_mean_squared_error: 55.0079\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3283 - mean_squared_error: 56.3283 - val_loss: 55.5368 - val_mean_squared_error: 55.5368\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4224 - mean_squared_error: 56.4223 - val_loss: 55.1859 - val_mean_squared_error: 55.1859\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.5069 - mean_squared_error: 56.5069 - val_loss: 54.9111 - val_mean_squared_error: 54.9111\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5369 - mean_squared_error: 56.5369 - val_loss: 56.3697 - val_mean_squared_error: 56.3697\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.0636 - mean_squared_error: 57.0636 - val_loss: 54.9313 - val_mean_squared_error: 54.9313\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4287 - mean_squared_error: 56.4286 - val_loss: 55.3369 - val_mean_squared_error: 55.3369\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0168 - mean_squared_error: 57.0168 - val_loss: 54.9512 - val_mean_squared_error: 54.9512\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5983 - mean_squared_error: 56.5983 - val_loss: 55.2259 - val_mean_squared_error: 55.2259\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 56.5773 - mean_squared_error: 56.5773 - val_loss: 54.9758 - val_mean_squared_error: 54.9758\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.6876 - mean_squared_error: 56.6876 - val_loss: 56.1420 - val_mean_squared_error: 56.1421\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4034 - mean_squared_error: 56.4034 - val_loss: 56.0695 - val_mean_squared_error: 56.0695\n",
      "Epoch 594/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 56.4921 - mean_squared_error: 56.4921 - val_loss: 55.3793 - val_mean_squared_error: 55.3793\n",
      "Epoch 595/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 56.4522 - mean_squared_error: 56.4523 - val_loss: 55.1216 - val_mean_squared_error: 55.1216\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 56.8575 - mean_squared_error: 56.8575 - val_loss: 54.9143 - val_mean_squared_error: 54.9142\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 1s 97us/sample - loss: 56.3967 - mean_squared_error: 56.3967 - val_loss: 55.0132 - val_mean_squared_error: 55.0132\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3710 - mean_squared_error: 56.3710 - val_loss: 55.6222 - val_mean_squared_error: 55.6222\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 56.6517 - mean_squared_error: 56.6517 - val_loss: 55.4297 - val_mean_squared_error: 55.4296\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.5696 - mean_squared_error: 56.5696 - val_loss: 55.2452 - val_mean_squared_error: 55.2452\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 56.5756 - mean_squared_error: 56.5756 - val_loss: 55.1039 - val_mean_squared_error: 55.1039\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.4896 - mean_squared_error: 56.4896 - val_loss: 55.0474 - val_mean_squared_error: 55.0474\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4102 - mean_squared_error: 56.4102 - val_loss: 55.4579 - val_mean_squared_error: 55.4579\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 1s 133us/sample - loss: 56.4547 - mean_squared_error: 56.4546 - val_loss: 59.3046 - val_mean_squared_error: 59.3046\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 56.6415 - mean_squared_error: 56.6415 - val_loss: 55.4545 - val_mean_squared_error: 55.4545\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 56.7172 - mean_squared_error: 56.7172 - val_loss: 54.9916 - val_mean_squared_error: 54.9916\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 56.5125 - mean_squared_error: 56.5125 - val_loss: 55.5256 - val_mean_squared_error: 55.5256\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 56.6660 - mean_squared_error: 56.6660 - val_loss: 55.1793 - val_mean_squared_error: 55.1793\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 56.6793 - mean_squared_error: 56.6793 - val_loss: 56.0650 - val_mean_squared_error: 56.0650\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 1s 96us/sample - loss: 56.5027 - mean_squared_error: 56.5027 - val_loss: 55.0165 - val_mean_squared_error: 55.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 56.4164 - mean_squared_error: 56.4164 - val_loss: 55.7387 - val_mean_squared_error: 55.7387\n",
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 56.6000 - mean_squared_error: 56.6000 - val_loss: 55.0260 - val_mean_squared_error: 55.0260\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 1s 109us/sample - loss: 56.6299 - mean_squared_error: 56.6299 - val_loss: 55.4363 - val_mean_squared_error: 55.4363\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 56.3944 - mean_squared_error: 56.3944 - val_loss: 55.3768 - val_mean_squared_error: 55.3768\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.4909 - mean_squared_error: 56.4909 - val_loss: 55.2601 - val_mean_squared_error: 55.2601\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.4112 - mean_squared_error: 56.4112 - val_loss: 55.3291 - val_mean_squared_error: 55.3291\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4434 - mean_squared_error: 56.4434 - val_loss: 55.2903 - val_mean_squared_error: 55.2903\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 56.4526 - mean_squared_error: 56.4526 - val_loss: 55.0217 - val_mean_squared_error: 55.0217\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 56.4145 - mean_squared_error: 56.4145 - val_loss: 54.9407 - val_mean_squared_error: 54.9408\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.3309 - mean_squared_error: 56.3309 - val_loss: 54.9897 - val_mean_squared_error: 54.9897\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5690 - mean_squared_error: 56.5690 - val_loss: 55.9205 - val_mean_squared_error: 55.9205\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3992 - mean_squared_error: 56.3992 - val_loss: 54.9483 - val_mean_squared_error: 54.9483\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5414 - mean_squared_error: 56.5414 - val_loss: 55.0002 - val_mean_squared_error: 55.0002\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5157 - mean_squared_error: 56.5157 - val_loss: 54.9762 - val_mean_squared_error: 54.9762\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3459 - mean_squared_error: 56.3459 - val_loss: 55.2571 - val_mean_squared_error: 55.2571\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.4189 - mean_squared_error: 56.4189 - val_loss: 54.9634 - val_mean_squared_error: 54.9634\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5072 - mean_squared_error: 56.5072 - val_loss: 54.9406 - val_mean_squared_error: 54.9406\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3428 - mean_squared_error: 56.3428 - val_loss: 55.8920 - val_mean_squared_error: 55.8920\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.7401 - mean_squared_error: 56.7401 - val_loss: 55.5448 - val_mean_squared_error: 55.5448\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.5441 - mean_squared_error: 56.5441 - val_loss: 54.9408 - val_mean_squared_error: 54.9408\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4812 - mean_squared_error: 56.4811 - val_loss: 55.7405 - val_mean_squared_error: 55.7405\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1011 - mean_squared_error: 57.1011 - val_loss: 54.9665 - val_mean_squared_error: 54.9665\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.4016 - mean_squared_error: 56.4016 - val_loss: 55.1890 - val_mean_squared_error: 55.1890\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4733 - mean_squared_error: 56.4733 - val_loss: 55.0692 - val_mean_squared_error: 55.0692\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.0750 - mean_squared_error: 57.0751 - val_loss: 54.9596 - val_mean_squared_error: 54.9596\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0886 - mean_squared_error: 56.0886 - val_loss: 55.0552 - val_mean_squared_error: 55.0552\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4610 - mean_squared_error: 56.4610 - val_loss: 55.0240 - val_mean_squared_error: 55.0240\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3298 - mean_squared_error: 56.3298 - val_loss: 55.1037 - val_mean_squared_error: 55.1037\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3562 - mean_squared_error: 56.3562 - val_loss: 55.0111 - val_mean_squared_error: 55.0111\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7025 - mean_squared_error: 56.7025 - val_loss: 55.3820 - val_mean_squared_error: 55.3820\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.4978 - mean_squared_error: 56.4978 - val_loss: 55.1716 - val_mean_squared_error: 55.1716\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4110 - mean_squared_error: 56.4110 - val_loss: 55.0432 - val_mean_squared_error: 55.0432\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2436 - mean_squared_error: 56.2436 - val_loss: 55.2860 - val_mean_squared_error: 55.2860\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.6209 - mean_squared_error: 56.6209 - val_loss: 59.4616 - val_mean_squared_error: 59.4616\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.5513 - mean_squared_error: 56.5513 - val_loss: 55.9425 - val_mean_squared_error: 55.9425\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2353 - mean_squared_error: 56.2353 - val_loss: 55.0511 - val_mean_squared_error: 55.0511\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3377 - mean_squared_error: 56.3377 - val_loss: 55.1447 - val_mean_squared_error: 55.1447\n",
      "Epoch 648/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.2471 - mean_squared_error: 56.2471 - val_loss: 55.0036 - val_mean_squared_error: 55.0036\n",
      "Epoch 649/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.3583 - mean_squared_error: 56.3583 - val_loss: 55.0947 - val_mean_squared_error: 55.0947\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 56.4214 - mean_squared_error: 56.4215 - val_loss: 54.9845 - val_mean_squared_error: 54.9845\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4221 - mean_squared_error: 56.4221 - val_loss: 55.1216 - val_mean_squared_error: 55.1216\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2874 - mean_squared_error: 56.2874 - val_loss: 55.0349 - val_mean_squared_error: 55.0349\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 56.3074 - mean_squared_error: 56.3074 - val_loss: 55.4264 - val_mean_squared_error: 55.4264\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 56.1415 - mean_squared_error: 56.1415 - val_loss: 55.0948 - val_mean_squared_error: 55.0948\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 56.5214 - mean_squared_error: 56.5214 - val_loss: 55.1903 - val_mean_squared_error: 55.1903\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.4908 - mean_squared_error: 56.4908 - val_loss: 55.3337 - val_mean_squared_error: 55.3337\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 56.1537 - mean_squared_error: 56.1537 - val_loss: 55.8894 - val_mean_squared_error: 55.8894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.6166 - mean_squared_error: 56.6166 - val_loss: 55.2178 - val_mean_squared_error: 55.2178\n",
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.0921 - mean_squared_error: 56.0921 - val_loss: 55.2717 - val_mean_squared_error: 55.2717\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 56.0556 - mean_squared_error: 56.0556 - val_loss: 56.5824 - val_mean_squared_error: 56.5824\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 56.6157 - mean_squared_error: 56.6157 - val_loss: 55.2918 - val_mean_squared_error: 55.2918\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 56.2201 - mean_squared_error: 56.2201 - val_loss: 55.1469 - val_mean_squared_error: 55.1469\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.3003 - mean_squared_error: 56.3003 - val_loss: 55.0073 - val_mean_squared_error: 55.0073\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 56.2483 - mean_squared_error: 56.2483 - val_loss: 56.4989 - val_mean_squared_error: 56.4989\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 56.3240 - mean_squared_error: 56.3240 - val_loss: 55.0968 - val_mean_squared_error: 55.0968\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 56.2226 - mean_squared_error: 56.2226 - val_loss: 57.0801 - val_mean_squared_error: 57.0801\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.3623 - mean_squared_error: 56.3623 - val_loss: 57.2436 - val_mean_squared_error: 57.2436\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.3606 - mean_squared_error: 56.3605 - val_loss: 55.0277 - val_mean_squared_error: 55.0277\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.2213 - mean_squared_error: 56.2213 - val_loss: 55.2510 - val_mean_squared_error: 55.2510\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.5896 - mean_squared_error: 56.5896 - val_loss: 55.4004 - val_mean_squared_error: 55.4004\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2311 - mean_squared_error: 56.2311 - val_loss: 55.0725 - val_mean_squared_error: 55.0725\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2781 - mean_squared_error: 56.2781 - val_loss: 55.0632 - val_mean_squared_error: 55.0632\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.1058 - mean_squared_error: 56.1057 - val_loss: 55.1547 - val_mean_squared_error: 55.1547\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.3071 - mean_squared_error: 56.3071 - val_loss: 56.4913 - val_mean_squared_error: 56.4913\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 56.7075 - mean_squared_error: 56.7075 - val_loss: 55.0658 - val_mean_squared_error: 55.0658\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.9335 - mean_squared_error: 55.9335 - val_loss: 55.1578 - val_mean_squared_error: 55.1578\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 1s 95us/sample - loss: 56.4106 - mean_squared_error: 56.4106 - val_loss: 55.3143 - val_mean_squared_error: 55.3143\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 56.4533 - mean_squared_error: 56.4533 - val_loss: 55.7362 - val_mean_squared_error: 55.7362\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 56.1375 - mean_squared_error: 56.1375 - val_loss: 55.3228 - val_mean_squared_error: 55.3228\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 56.4502 - mean_squared_error: 56.4502 - val_loss: 55.4104 - val_mean_squared_error: 55.4104\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 56.3198 - mean_squared_error: 56.3199 - val_loss: 55.2750 - val_mean_squared_error: 55.2751\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0100 - mean_squared_error: 56.0100 - val_loss: 55.1575 - val_mean_squared_error: 55.1576\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 86us/sample - loss: 56.6914 - mean_squared_error: 56.6914 - val_loss: 56.0639 - val_mean_squared_error: 56.0639\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 56.2047 - mean_squared_error: 56.2047 - val_loss: 56.0607 - val_mean_squared_error: 56.0607\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 56.1546 - mean_squared_error: 56.1546 - val_loss: 55.0952 - val_mean_squared_error: 55.0952\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 56.0221 - mean_squared_error: 56.0221 - val_loss: 55.8866 - val_mean_squared_error: 55.8866\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.3296 - mean_squared_error: 56.3296 - val_loss: 55.0733 - val_mean_squared_error: 55.0733\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 56.2275 - mean_squared_error: 56.2275 - val_loss: 55.4675 - val_mean_squared_error: 55.4675\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0186 - mean_squared_error: 56.0186 - val_loss: 55.1523 - val_mean_squared_error: 55.1523\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.9406 - mean_squared_error: 55.9406 - val_loss: 56.7355 - val_mean_squared_error: 56.7355\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 56.3884 - mean_squared_error: 56.3884 - val_loss: 55.1876 - val_mean_squared_error: 55.1876\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0778 - mean_squared_error: 56.0778 - val_loss: 55.1547 - val_mean_squared_error: 55.1547\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1654 - mean_squared_error: 56.1654 - val_loss: 55.2236 - val_mean_squared_error: 55.2236\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2171 - mean_squared_error: 56.2171 - val_loss: 55.0289 - val_mean_squared_error: 55.0289\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9991 - mean_squared_error: 55.9991 - val_loss: 56.3840 - val_mean_squared_error: 56.3840\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3344 - mean_squared_error: 56.3344 - val_loss: 56.2939 - val_mean_squared_error: 56.2939\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2109 - mean_squared_error: 56.2109 - val_loss: 55.4745 - val_mean_squared_error: 55.4745\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2379 - mean_squared_error: 56.2379 - val_loss: 55.6871 - val_mean_squared_error: 55.6871\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.1887 - mean_squared_error: 56.1887 - val_loss: 55.6387 - val_mean_squared_error: 55.6387\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3197 - mean_squared_error: 56.3197 - val_loss: 55.0801 - val_mean_squared_error: 55.0801\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4265 - mean_squared_error: 56.4265 - val_loss: 55.5518 - val_mean_squared_error: 55.5518\n",
      "Epoch 702/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.0904 - mean_squared_error: 56.0904 - val_loss: 55.1394 - val_mean_squared_error: 55.1394\n",
      "Epoch 703/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4726 - mean_squared_error: 56.4726 - val_loss: 56.1853 - val_mean_squared_error: 56.1853\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1351 - mean_squared_error: 56.1351 - val_loss: 55.3280 - val_mean_squared_error: 55.3280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.0690 - mean_squared_error: 56.0690 - val_loss: 55.2310 - val_mean_squared_error: 55.2310\n",
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.9800 - mean_squared_error: 55.9800 - val_loss: 55.1744 - val_mean_squared_error: 55.1744\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0153 - mean_squared_error: 56.0153 - val_loss: 55.9364 - val_mean_squared_error: 55.9364\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0925 - mean_squared_error: 56.0926 - val_loss: 56.2805 - val_mean_squared_error: 56.2805\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.1500 - mean_squared_error: 56.1500 - val_loss: 55.2365 - val_mean_squared_error: 55.2365\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 1s 89us/sample - loss: 56.1037 - mean_squared_error: 56.1037 - val_loss: 56.1100 - val_mean_squared_error: 56.1100\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.8803 - mean_squared_error: 55.8803 - val_loss: 55.2013 - val_mean_squared_error: 55.2013\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.0341 - mean_squared_error: 56.0341 - val_loss: 57.1922 - val_mean_squared_error: 57.1922\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 56.2317 - mean_squared_error: 56.2317 - val_loss: 55.8270 - val_mean_squared_error: 55.8270\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.3837 - mean_squared_error: 56.3837 - val_loss: 55.6099 - val_mean_squared_error: 55.6099\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 56.3182 - mean_squared_error: 56.3182 - val_loss: 55.1727 - val_mean_squared_error: 55.1727\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 1s 108us/sample - loss: 56.1826 - mean_squared_error: 56.1826 - val_loss: 55.3066 - val_mean_squared_error: 55.3066\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.9523 - mean_squared_error: 55.9523 - val_loss: 55.5261 - val_mean_squared_error: 55.5261\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.2536 - mean_squared_error: 56.2536 - val_loss: 55.4416 - val_mean_squared_error: 55.4416\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 1s 97us/sample - loss: 56.1151 - mean_squared_error: 56.1151 - val_loss: 55.1860 - val_mean_squared_error: 55.1860\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 1s 116us/sample - loss: 55.9860 - mean_squared_error: 55.9860 - val_loss: 55.1923 - val_mean_squared_error: 55.1923\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 56.0871 - mean_squared_error: 56.0871 - val_loss: 56.2393 - val_mean_squared_error: 56.2393\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.1345 - mean_squared_error: 56.1345 - val_loss: 55.1001 - val_mean_squared_error: 55.1001\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0095 - mean_squared_error: 56.0095 - val_loss: 55.8334 - val_mean_squared_error: 55.8334\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 1s 98us/sample - loss: 55.9796 - mean_squared_error: 55.9796 - val_loss: 55.5287 - val_mean_squared_error: 55.5287\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7939 - mean_squared_error: 55.7939 - val_loss: 56.1299 - val_mean_squared_error: 56.1299\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 56.1967 - mean_squared_error: 56.1968 - val_loss: 55.5078 - val_mean_squared_error: 55.5078\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2842 - mean_squared_error: 56.2842 - val_loss: 55.1555 - val_mean_squared_error: 55.1556\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.0886 - mean_squared_error: 56.0886 - val_loss: 55.3529 - val_mean_squared_error: 55.3529\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 1s 122us/sample - loss: 56.1195 - mean_squared_error: 56.1195 - val_loss: 55.4104 - val_mean_squared_error: 55.4104\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 1s 101us/sample - loss: 55.9105 - mean_squared_error: 55.9105 - val_loss: 55.1444 - val_mean_squared_error: 55.1444\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 56.0899 - mean_squared_error: 56.0899 - val_loss: 55.2275 - val_mean_squared_error: 55.2275\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.9684 - mean_squared_error: 55.9684 - val_loss: 55.2282 - val_mean_squared_error: 55.2282\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 56.2410 - mean_squared_error: 56.2410 - val_loss: 57.3171 - val_mean_squared_error: 57.3171\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.8706 - mean_squared_error: 55.8706 - val_loss: 55.1549 - val_mean_squared_error: 55.1549\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0616 - mean_squared_error: 56.0616 - val_loss: 55.4449 - val_mean_squared_error: 55.4449\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.5403 - mean_squared_error: 56.5403 - val_loss: 55.1609 - val_mean_squared_error: 55.1609\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 56.0009 - mean_squared_error: 56.0009 - val_loss: 57.1695 - val_mean_squared_error: 57.1695\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.8981 - mean_squared_error: 55.8981 - val_loss: 55.4222 - val_mean_squared_error: 55.4222\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 55.9893 - mean_squared_error: 55.9892 - val_loss: 55.2239 - val_mean_squared_error: 55.2239\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.9021 - mean_squared_error: 55.9021 - val_loss: 55.3550 - val_mean_squared_error: 55.3550\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 56.0089 - mean_squared_error: 56.0089 - val_loss: 55.7144 - val_mean_squared_error: 55.7144\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.7418 - mean_squared_error: 55.7418 - val_loss: 55.2630 - val_mean_squared_error: 55.2630\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.9717 - mean_squared_error: 55.9717 - val_loss: 55.1805 - val_mean_squared_error: 55.1805\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 56.0107 - mean_squared_error: 56.0107 - val_loss: 56.1267 - val_mean_squared_error: 56.1267\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 1s 101us/sample - loss: 56.1026 - mean_squared_error: 56.1026 - val_loss: 55.8033 - val_mean_squared_error: 55.8033\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.9796 - mean_squared_error: 55.9797 - val_loss: 55.2522 - val_mean_squared_error: 55.2522\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.0329 - mean_squared_error: 56.0329 - val_loss: 57.4518 - val_mean_squared_error: 57.4518\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 55.8740 - mean_squared_error: 55.8741 - val_loss: 55.5032 - val_mean_squared_error: 55.5032\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.8602 - mean_squared_error: 55.8602 - val_loss: 57.0150 - val_mean_squared_error: 57.0150\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.9849 - mean_squared_error: 55.9849 - val_loss: 55.4731 - val_mean_squared_error: 55.4732\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.7988 - mean_squared_error: 55.7988 - val_loss: 55.4949 - val_mean_squared_error: 55.4949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 55.8327 - mean_squared_error: 55.8327 - val_loss: 55.3748 - val_mean_squared_error: 55.3748\n",
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.9640 - mean_squared_error: 55.9640 - val_loss: 55.4066 - val_mean_squared_error: 55.4066\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.9707 - mean_squared_error: 55.9706 - val_loss: 55.9169 - val_mean_squared_error: 55.9169\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.8414 - mean_squared_error: 55.8414 - val_loss: 55.4687 - val_mean_squared_error: 55.4687\n",
      "Epoch 756/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9273 - mean_squared_error: 55.9273 - val_loss: 55.2538 - val_mean_squared_error: 55.2538\n",
      "Epoch 757/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.7349 - mean_squared_error: 55.7349 - val_loss: 55.6157 - val_mean_squared_error: 55.6157\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.9971 - mean_squared_error: 55.9971 - val_loss: 55.3080 - val_mean_squared_error: 55.3080\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9363 - mean_squared_error: 55.9363 - val_loss: 55.2316 - val_mean_squared_error: 55.2316\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9207 - mean_squared_error: 55.9207 - val_loss: 55.8129 - val_mean_squared_error: 55.8129\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2889 - mean_squared_error: 56.2889 - val_loss: 55.3404 - val_mean_squared_error: 55.3404\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.7626 - mean_squared_error: 55.7626 - val_loss: 55.3023 - val_mean_squared_error: 55.3023\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.8858 - mean_squared_error: 55.8858 - val_loss: 55.6523 - val_mean_squared_error: 55.6523\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.8529 - mean_squared_error: 55.8529 - val_loss: 55.4847 - val_mean_squared_error: 55.4847\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.8818 - mean_squared_error: 55.8818 - val_loss: 56.0058 - val_mean_squared_error: 56.0058\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8981 - mean_squared_error: 55.8981 - val_loss: 55.4035 - val_mean_squared_error: 55.4035\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.8642 - mean_squared_error: 55.8642 - val_loss: 55.2808 - val_mean_squared_error: 55.2808\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9745 - mean_squared_error: 55.9745 - val_loss: 55.3222 - val_mean_squared_error: 55.3222\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.8672 - mean_squared_error: 55.8672 - val_loss: 57.4133 - val_mean_squared_error: 57.4133\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9011 - mean_squared_error: 55.9011 - val_loss: 55.6507 - val_mean_squared_error: 55.6507\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8396 - mean_squared_error: 55.8396 - val_loss: 56.0398 - val_mean_squared_error: 56.0398\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0001 - mean_squared_error: 56.0001 - val_loss: 55.3246 - val_mean_squared_error: 55.3246\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9608 - mean_squared_error: 55.9608 - val_loss: 56.2369 - val_mean_squared_error: 56.2369\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0107 - mean_squared_error: 56.0107 - val_loss: 55.3172 - val_mean_squared_error: 55.3172\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8075 - mean_squared_error: 55.8076 - val_loss: 56.0035 - val_mean_squared_error: 56.0035\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.0632 - mean_squared_error: 56.0632 - val_loss: 56.0476 - val_mean_squared_error: 56.0476\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0019 - mean_squared_error: 56.0019 - val_loss: 55.6794 - val_mean_squared_error: 55.6794\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7603 - mean_squared_error: 55.7603 - val_loss: 55.3779 - val_mean_squared_error: 55.3779\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.6585 - mean_squared_error: 55.6586 - val_loss: 55.6679 - val_mean_squared_error: 55.6679\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8476 - mean_squared_error: 55.8476 - val_loss: 56.7366 - val_mean_squared_error: 56.7366\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7110 - mean_squared_error: 55.7111 - val_loss: 55.2985 - val_mean_squared_error: 55.2985\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6947 - mean_squared_error: 55.6947 - val_loss: 55.4181 - val_mean_squared_error: 55.4181\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.7603 - mean_squared_error: 55.7603 - val_loss: 55.6038 - val_mean_squared_error: 55.6038\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7376 - mean_squared_error: 55.7376 - val_loss: 55.4022 - val_mean_squared_error: 55.4022\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9131 - mean_squared_error: 55.9131 - val_loss: 58.4141 - val_mean_squared_error: 58.4141\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.9481 - mean_squared_error: 55.9481 - val_loss: 56.0919 - val_mean_squared_error: 56.0919\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9181 - mean_squared_error: 55.9181 - val_loss: 55.6870 - val_mean_squared_error: 55.6870\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3010 - mean_squared_error: 56.3010 - val_loss: 55.3238 - val_mean_squared_error: 55.3238\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.7417 - mean_squared_error: 55.7416 - val_loss: 55.3212 - val_mean_squared_error: 55.3212\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7334 - mean_squared_error: 55.7334 - val_loss: 55.3133 - val_mean_squared_error: 55.3133\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7706 - mean_squared_error: 55.7706 - val_loss: 57.9010 - val_mean_squared_error: 57.9010\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1986 - mean_squared_error: 56.1986 - val_loss: 56.1627 - val_mean_squared_error: 56.1627\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6661 - mean_squared_error: 55.6661 - val_loss: 56.0686 - val_mean_squared_error: 56.0686\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.6613 - mean_squared_error: 55.6613 - val_loss: 55.4377 - val_mean_squared_error: 55.4377\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7941 - mean_squared_error: 55.7941 - val_loss: 55.3573 - val_mean_squared_error: 55.3573\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7003 - mean_squared_error: 55.7003 - val_loss: 56.6447 - val_mean_squared_error: 56.6447\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.7415 - mean_squared_error: 55.7415 - val_loss: 55.3225 - val_mean_squared_error: 55.3225\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6824 - mean_squared_error: 55.6824 - val_loss: 55.8177 - val_mean_squared_error: 55.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.5847 - mean_squared_error: 55.5847 - val_loss: 55.3233 - val_mean_squared_error: 55.3233\n",
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7581 - mean_squared_error: 55.7581 - val_loss: 55.9551 - val_mean_squared_error: 55.9551\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7574 - mean_squared_error: 55.7574 - val_loss: 58.7651 - val_mean_squared_error: 58.7651\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 1s 104us/sample - loss: 55.7684 - mean_squared_error: 55.7684 - val_loss: 55.5799 - val_mean_squared_error: 55.5799\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 1s 116us/sample - loss: 55.7850 - mean_squared_error: 55.7850 - val_loss: 55.4217 - val_mean_squared_error: 55.4217\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 1s 137us/sample - loss: 56.0715 - mean_squared_error: 56.0715 - val_loss: 55.6305 - val_mean_squared_error: 55.6305\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 55.7239 - mean_squared_error: 55.7239 - val_loss: 55.3298 - val_mean_squared_error: 55.3298\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6311 - mean_squared_error: 55.6311 - val_loss: 55.5073 - val_mean_squared_error: 55.5073\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7517 - mean_squared_error: 55.7517 - val_loss: 55.4548 - val_mean_squared_error: 55.4548\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 55.8342 - mean_squared_error: 55.8342 - val_loss: 55.3768 - val_mean_squared_error: 55.3768\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 55.8652 - mean_squared_error: 55.8652 - val_loss: 55.4617 - val_mean_squared_error: 55.4616\n",
      "Epoch 810/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7991 - mean_squared_error: 55.7991 - val_loss: 56.8101 - val_mean_squared_error: 56.8101\n",
      "Epoch 811/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7870 - mean_squared_error: 55.7870 - val_loss: 55.7063 - val_mean_squared_error: 55.7063\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8858 - mean_squared_error: 55.8858 - val_loss: 55.6029 - val_mean_squared_error: 55.6029\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8195 - mean_squared_error: 55.8195 - val_loss: 55.4475 - val_mean_squared_error: 55.4475\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7013 - mean_squared_error: 55.7013 - val_loss: 55.3529 - val_mean_squared_error: 55.3529\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7057 - mean_squared_error: 55.7057 - val_loss: 56.4215 - val_mean_squared_error: 56.4215\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.5847 - mean_squared_error: 55.5847 - val_loss: 56.2701 - val_mean_squared_error: 56.2701\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9401 - mean_squared_error: 55.9401 - val_loss: 55.4993 - val_mean_squared_error: 55.4993\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6460 - mean_squared_error: 55.6460 - val_loss: 55.5030 - val_mean_squared_error: 55.5030\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8059 - mean_squared_error: 55.8059 - val_loss: 55.4967 - val_mean_squared_error: 55.4967\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.6209 - mean_squared_error: 55.6209 - val_loss: 55.5509 - val_mean_squared_error: 55.5509\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.4152 - mean_squared_error: 55.4151 - val_loss: 56.6564 - val_mean_squared_error: 56.6564\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.7893 - mean_squared_error: 55.7893 - val_loss: 55.7618 - val_mean_squared_error: 55.7618\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.7284 - mean_squared_error: 55.7284 - val_loss: 56.1885 - val_mean_squared_error: 56.1886\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4807 - mean_squared_error: 55.4807 - val_loss: 55.7837 - val_mean_squared_error: 55.7837\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.5946 - mean_squared_error: 55.5945 - val_loss: 55.4174 - val_mean_squared_error: 55.4175\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6442 - mean_squared_error: 55.6442 - val_loss: 55.5441 - val_mean_squared_error: 55.5440\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9382 - mean_squared_error: 55.9382 - val_loss: 55.5355 - val_mean_squared_error: 55.5355\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6203 - mean_squared_error: 55.6203 - val_loss: 55.6107 - val_mean_squared_error: 55.6107\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8431 - mean_squared_error: 55.8431 - val_loss: 55.4198 - val_mean_squared_error: 55.4198\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0168 - mean_squared_error: 56.0168 - val_loss: 55.5488 - val_mean_squared_error: 55.5488\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7081 - mean_squared_error: 55.7081 - val_loss: 55.4531 - val_mean_squared_error: 55.4531\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9855 - mean_squared_error: 55.9855 - val_loss: 56.1605 - val_mean_squared_error: 56.1605\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8702 - mean_squared_error: 55.8702 - val_loss: 56.6136 - val_mean_squared_error: 56.6136\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.6685 - mean_squared_error: 55.6685 - val_loss: 55.5795 - val_mean_squared_error: 55.5795\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.5789 - mean_squared_error: 55.5788 - val_loss: 55.5629 - val_mean_squared_error: 55.5629\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.5224 - mean_squared_error: 55.5224 - val_loss: 56.1195 - val_mean_squared_error: 56.1195\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.5963 - mean_squared_error: 55.5963 - val_loss: 55.4917 - val_mean_squared_error: 55.4917\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.5839 - mean_squared_error: 55.5839 - val_loss: 55.5588 - val_mean_squared_error: 55.5588\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.6806 - mean_squared_error: 55.6806 - val_loss: 55.5634 - val_mean_squared_error: 55.5634\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.5683 - mean_squared_error: 55.5683 - val_loss: 57.2992 - val_mean_squared_error: 57.2992\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6636 - mean_squared_error: 55.6636 - val_loss: 55.5567 - val_mean_squared_error: 55.5567\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.5787 - mean_squared_error: 55.5787 - val_loss: 55.5524 - val_mean_squared_error: 55.5524\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.5477 - mean_squared_error: 55.5477 - val_loss: 55.8461 - val_mean_squared_error: 55.8461\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.6269 - mean_squared_error: 55.6269 - val_loss: 55.9033 - val_mean_squared_error: 55.9033\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.6787 - mean_squared_error: 55.6787 - val_loss: 56.0340 - val_mean_squared_error: 56.0340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6208 - mean_squared_error: 55.6208 - val_loss: 55.5324 - val_mean_squared_error: 55.5324\n",
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7421 - mean_squared_error: 55.7421 - val_loss: 55.6852 - val_mean_squared_error: 55.6852\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.5689 - mean_squared_error: 55.5688 - val_loss: 55.5550 - val_mean_squared_error: 55.5550\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.5363 - mean_squared_error: 55.5363 - val_loss: 55.6681 - val_mean_squared_error: 55.6681\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6482 - mean_squared_error: 55.6482 - val_loss: 55.8272 - val_mean_squared_error: 55.8272\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.6772 - mean_squared_error: 55.6772 - val_loss: 56.6382 - val_mean_squared_error: 56.6382\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.4039 - mean_squared_error: 55.4039 - val_loss: 55.5694 - val_mean_squared_error: 55.5694\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.4392 - mean_squared_error: 55.4392 - val_loss: 55.5550 - val_mean_squared_error: 55.5550\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4937 - mean_squared_error: 55.4937 - val_loss: 55.5471 - val_mean_squared_error: 55.5471\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.2700 - mean_squared_error: 55.2700 - val_loss: 55.7689 - val_mean_squared_error: 55.7689\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.5921 - mean_squared_error: 55.5922 - val_loss: 57.4148 - val_mean_squared_error: 57.4148\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.4388 - mean_squared_error: 55.4388 - val_loss: 55.5544 - val_mean_squared_error: 55.5544\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.5439 - mean_squared_error: 55.5439 - val_loss: 56.6617 - val_mean_squared_error: 56.6617\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.5719 - mean_squared_error: 55.5719 - val_loss: 56.7971 - val_mean_squared_error: 56.7971\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4431 - mean_squared_error: 55.4431 - val_loss: 55.9843 - val_mean_squared_error: 55.9843\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4875 - mean_squared_error: 55.4875 - val_loss: 55.6208 - val_mean_squared_error: 55.6208\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.6263 - mean_squared_error: 55.6262 - val_loss: 56.1000 - val_mean_squared_error: 56.1000\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 55.6821 - mean_squared_error: 55.6821 - val_loss: 55.5163 - val_mean_squared_error: 55.5163\n",
      "Epoch 864/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.7146 - mean_squared_error: 55.7146 - val_loss: 55.7368 - val_mean_squared_error: 55.7368\n",
      "Epoch 865/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.5219 - mean_squared_error: 55.5219 - val_loss: 55.8405 - val_mean_squared_error: 55.8405\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.3247 - mean_squared_error: 55.3247 - val_loss: 55.5829 - val_mean_squared_error: 55.5829\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 55.5547 - mean_squared_error: 55.5547 - val_loss: 55.5626 - val_mean_squared_error: 55.5626\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 55.6566 - mean_squared_error: 55.6566 - val_loss: 55.5325 - val_mean_squared_error: 55.5325\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 55.2727 - mean_squared_error: 55.2726 - val_loss: 55.7050 - val_mean_squared_error: 55.7050\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.4002 - mean_squared_error: 55.4002 - val_loss: 57.6621 - val_mean_squared_error: 57.6621\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.5706 - mean_squared_error: 55.5705 - val_loss: 56.1441 - val_mean_squared_error: 56.1441\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.5906 - mean_squared_error: 55.5906 - val_loss: 57.0360 - val_mean_squared_error: 57.0360\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.6755 - mean_squared_error: 55.6756 - val_loss: 55.9900 - val_mean_squared_error: 55.9900\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.7140 - mean_squared_error: 55.7140 - val_loss: 55.5342 - val_mean_squared_error: 55.5342\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.5433 - mean_squared_error: 55.5433 - val_loss: 55.6369 - val_mean_squared_error: 55.6369\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 1s 115us/sample - loss: 55.5911 - mean_squared_error: 55.5911 - val_loss: 55.6765 - val_mean_squared_error: 55.6765\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 1s 110us/sample - loss: 55.6054 - mean_squared_error: 55.6054 - val_loss: 55.6815 - val_mean_squared_error: 55.6815\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 1s 106us/sample - loss: 55.4001 - mean_squared_error: 55.4001 - val_loss: 55.6485 - val_mean_squared_error: 55.6485\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 55.5279 - mean_squared_error: 55.5279 - val_loss: 56.0893 - val_mean_squared_error: 56.0893\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4831 - mean_squared_error: 55.4831 - val_loss: 57.9956 - val_mean_squared_error: 57.9956\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.3964 - mean_squared_error: 55.3964 - val_loss: 56.3955 - val_mean_squared_error: 56.3955\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7648 - mean_squared_error: 55.7648 - val_loss: 56.1142 - val_mean_squared_error: 56.1142\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 1s 96us/sample - loss: 55.4363 - mean_squared_error: 55.4363 - val_loss: 56.2640 - val_mean_squared_error: 56.2640\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.8774 - mean_squared_error: 55.8774 - val_loss: 55.6445 - val_mean_squared_error: 55.6445\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.4161 - mean_squared_error: 55.4161 - val_loss: 60.5255 - val_mean_squared_error: 60.5255\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.6939 - mean_squared_error: 55.6939 - val_loss: 55.9024 - val_mean_squared_error: 55.9024\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.4554 - mean_squared_error: 55.4554 - val_loss: 56.3455 - val_mean_squared_error: 56.3455\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.2730 - mean_squared_error: 55.2730 - val_loss: 56.2778 - val_mean_squared_error: 56.2779\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.4882 - mean_squared_error: 55.4882 - val_loss: 55.5790 - val_mean_squared_error: 55.5790\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.6037 - mean_squared_error: 55.6037 - val_loss: 55.5967 - val_mean_squared_error: 55.5967\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.4923 - mean_squared_error: 55.4923 - val_loss: 55.7830 - val_mean_squared_error: 55.7830\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.3286 - mean_squared_error: 55.3286 - val_loss: 55.9287 - val_mean_squared_error: 55.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.3948 - mean_squared_error: 55.3948 - val_loss: 55.7574 - val_mean_squared_error: 55.7574\n",
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.3516 - mean_squared_error: 55.3516 - val_loss: 57.5355 - val_mean_squared_error: 57.5355\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.5477 - mean_squared_error: 55.5477 - val_loss: 55.8214 - val_mean_squared_error: 55.8214\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.2723 - mean_squared_error: 55.2723 - val_loss: 55.6430 - val_mean_squared_error: 55.6430\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.2528 - mean_squared_error: 55.2528 - val_loss: 57.6586 - val_mean_squared_error: 57.6586\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.5013 - mean_squared_error: 55.5013 - val_loss: 56.4014 - val_mean_squared_error: 56.4014\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.3049 - mean_squared_error: 55.3048 - val_loss: 55.8187 - val_mean_squared_error: 55.8187\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.4468 - mean_squared_error: 55.4468 - val_loss: 56.1330 - val_mean_squared_error: 56.1330\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.4573 - mean_squared_error: 55.4573 - val_loss: 55.6279 - val_mean_squared_error: 55.6279\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.5820 - mean_squared_error: 55.5820 - val_loss: 56.1302 - val_mean_squared_error: 56.1302\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.1903 - mean_squared_error: 55.1904 - val_loss: 58.3338 - val_mean_squared_error: 58.3337\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.6607 - mean_squared_error: 55.6607 - val_loss: 56.6401 - val_mean_squared_error: 56.6401\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7421 - mean_squared_error: 55.7421 - val_loss: 55.9438 - val_mean_squared_error: 55.9438\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.3979 - mean_squared_error: 55.3979 - val_loss: 56.0431 - val_mean_squared_error: 56.0431\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7122 - mean_squared_error: 55.7122 - val_loss: 55.6665 - val_mean_squared_error: 55.6665\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.4787 - mean_squared_error: 55.4787 - val_loss: 56.2666 - val_mean_squared_error: 56.2666\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.3385 - mean_squared_error: 55.3385 - val_loss: 57.6742 - val_mean_squared_error: 57.6742\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.3971 - mean_squared_error: 55.3971 - val_loss: 56.1374 - val_mean_squared_error: 56.1374\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.3479 - mean_squared_error: 55.3479 - val_loss: 55.6715 - val_mean_squared_error: 55.6715\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.5410 - mean_squared_error: 55.5410 - val_loss: 55.6264 - val_mean_squared_error: 55.6264\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.3472 - mean_squared_error: 55.3472 - val_loss: 55.7435 - val_mean_squared_error: 55.7435\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.6418 - mean_squared_error: 55.6418 - val_loss: 55.9020 - val_mean_squared_error: 55.9020\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.1659 - mean_squared_error: 55.1659 - val_loss: 55.8653 - val_mean_squared_error: 55.8653\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.2499 - mean_squared_error: 55.2499 - val_loss: 56.2784 - val_mean_squared_error: 56.2784\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.4878 - mean_squared_error: 55.4878 - val_loss: 56.0337 - val_mean_squared_error: 56.0337\n",
      "Epoch 918/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.4876 - mean_squared_error: 55.4876 - val_loss: 55.7200 - val_mean_squared_error: 55.7200\n",
      "Epoch 919/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.3985 - mean_squared_error: 55.3985 - val_loss: 57.2464 - val_mean_squared_error: 57.2464\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.3319 - mean_squared_error: 55.3319 - val_loss: 56.1502 - val_mean_squared_error: 56.1502\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.3785 - mean_squared_error: 55.3785 - val_loss: 55.8014 - val_mean_squared_error: 55.8014\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.3956 - mean_squared_error: 55.3956 - val_loss: 55.9131 - val_mean_squared_error: 55.9130\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.1644 - mean_squared_error: 55.1644 - val_loss: 56.3960 - val_mean_squared_error: 56.3960\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.2473 - mean_squared_error: 55.2473 - val_loss: 55.8464 - val_mean_squared_error: 55.8465\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.2447 - mean_squared_error: 55.2447 - val_loss: 56.0480 - val_mean_squared_error: 56.0480\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.2996 - mean_squared_error: 55.2996 - val_loss: 56.1953 - val_mean_squared_error: 56.1953\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.2248 - mean_squared_error: 55.2248 - val_loss: 55.8437 - val_mean_squared_error: 55.8437\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.2529 - mean_squared_error: 55.2529 - val_loss: 55.7580 - val_mean_squared_error: 55.7580\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.3639 - mean_squared_error: 55.3639 - val_loss: 55.8827 - val_mean_squared_error: 55.8827\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.2479 - mean_squared_error: 55.2479 - val_loss: 55.8386 - val_mean_squared_error: 55.8386\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.2044 - mean_squared_error: 55.2044 - val_loss: 55.7487 - val_mean_squared_error: 55.7487\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.0071 - mean_squared_error: 55.0071 - val_loss: 55.7509 - val_mean_squared_error: 55.7509\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.3380 - mean_squared_error: 55.3380 - val_loss: 55.7492 - val_mean_squared_error: 55.7492\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 55.4529 - mean_squared_error: 55.4529 - val_loss: 56.4544 - val_mean_squared_error: 56.4544\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.3766 - mean_squared_error: 55.3766 - val_loss: 55.9076 - val_mean_squared_error: 55.9076\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 55.1362 - mean_squared_error: 55.1362 - val_loss: 55.7221 - val_mean_squared_error: 55.7221\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.1844 - mean_squared_error: 55.1844 - val_loss: 56.0280 - val_mean_squared_error: 56.0280\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 55.6273 - mean_squared_error: 55.6273 - val_loss: 56.4545 - val_mean_squared_error: 56.4545\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.3068 - mean_squared_error: 55.3068 - val_loss: 55.8345 - val_mean_squared_error: 55.8345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 1s 100us/sample - loss: 55.3186 - mean_squared_error: 55.3187 - val_loss: 56.9179 - val_mean_squared_error: 56.9179\n",
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 1s 139us/sample - loss: 55.2092 - mean_squared_error: 55.2092 - val_loss: 57.1364 - val_mean_squared_error: 57.1364\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 55.2026 - mean_squared_error: 55.2026 - val_loss: 55.8503 - val_mean_squared_error: 55.8503\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 55.1727 - mean_squared_error: 55.1727 - val_loss: 56.7058 - val_mean_squared_error: 56.7058\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 55.4930 - mean_squared_error: 55.4930 - val_loss: 55.8251 - val_mean_squared_error: 55.8251\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.1626 - mean_squared_error: 55.1626 - val_loss: 55.8940 - val_mean_squared_error: 55.8940\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 55.2596 - mean_squared_error: 55.2596 - val_loss: 55.7961 - val_mean_squared_error: 55.7961\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 55.1947 - mean_squared_error: 55.1947 - val_loss: 55.9552 - val_mean_squared_error: 55.9552\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 55.5564 - mean_squared_error: 55.5564 - val_loss: 56.3730 - val_mean_squared_error: 56.3730\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.1718 - mean_squared_error: 55.1718 - val_loss: 55.7445 - val_mean_squared_error: 55.7444\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 55.1930 - mean_squared_error: 55.1930 - val_loss: 56.3025 - val_mean_squared_error: 56.3025\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 88us/sample - loss: 55.3802 - mean_squared_error: 55.3802 - val_loss: 56.3111 - val_mean_squared_error: 56.3111\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 55.1254 - mean_squared_error: 55.1254 - val_loss: 56.1048 - val_mean_squared_error: 56.1048\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.2400 - mean_squared_error: 55.2400 - val_loss: 55.8577 - val_mean_squared_error: 55.8577\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 55.3372 - mean_squared_error: 55.3372 - val_loss: 56.2839 - val_mean_squared_error: 56.2839\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 54.9876 - mean_squared_error: 54.9876 - val_loss: 55.7350 - val_mean_squared_error: 55.7350\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 55.2577 - mean_squared_error: 55.2577 - val_loss: 56.0546 - val_mean_squared_error: 56.0546\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 1s 102us/sample - loss: 55.6785 - mean_squared_error: 55.6785 - val_loss: 55.8308 - val_mean_squared_error: 55.8308\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 1s 117us/sample - loss: 55.1167 - mean_squared_error: 55.1167 - val_loss: 56.7617 - val_mean_squared_error: 56.7617\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 54.9907 - mean_squared_error: 54.9907 - val_loss: 56.0048 - val_mean_squared_error: 56.0048\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.1541 - mean_squared_error: 55.1542 - val_loss: 55.8875 - val_mean_squared_error: 55.8875\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 54.9727 - mean_squared_error: 54.9727 - val_loss: 55.8307 - val_mean_squared_error: 55.8307\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.0260 - mean_squared_error: 55.0260 - val_loss: 56.0484 - val_mean_squared_error: 56.0484\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.0909 - mean_squared_error: 55.0909 - val_loss: 55.9116 - val_mean_squared_error: 55.9116\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.2012 - mean_squared_error: 55.2012 - val_loss: 56.2150 - val_mean_squared_error: 56.2150\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.1862 - mean_squared_error: 55.1861 - val_loss: 55.9402 - val_mean_squared_error: 55.9402\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.2500 - mean_squared_error: 55.2500 - val_loss: 58.1477 - val_mean_squared_error: 58.1477\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.3341 - mean_squared_error: 55.3341 - val_loss: 56.2182 - val_mean_squared_error: 56.2183\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.1080 - mean_squared_error: 55.1081 - val_loss: 55.9795 - val_mean_squared_error: 55.9795\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 1s 108us/sample - loss: 55.1342 - mean_squared_error: 55.1342 - val_loss: 55.8181 - val_mean_squared_error: 55.8181\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.0859 - mean_squared_error: 55.0858 - val_loss: 56.9323 - val_mean_squared_error: 56.9323\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 1s 93us/sample - loss: 55.4745 - mean_squared_error: 55.4745 - val_loss: 56.0710 - val_mean_squared_error: 56.0710\n",
      "Epoch 972/1000\n",
      "5600/5600 [==============================] - 1s 135us/sample - loss: 55.1144 - mean_squared_error: 55.1144 - val_loss: 56.3624 - val_mean_squared_error: 56.3624\n",
      "Epoch 973/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 55.2046 - mean_squared_error: 55.2046 - val_loss: 56.4711 - val_mean_squared_error: 56.4711\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.3831 - mean_squared_error: 55.3831 - val_loss: 56.1499 - val_mean_squared_error: 56.1499\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 1s 130us/sample - loss: 55.1471 - mean_squared_error: 55.1471 - val_loss: 56.0275 - val_mean_squared_error: 56.0275\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 1s 96us/sample - loss: 55.0700 - mean_squared_error: 55.0700 - val_loss: 56.1319 - val_mean_squared_error: 56.1319\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 1s 103us/sample - loss: 55.1871 - mean_squared_error: 55.1871 - val_loss: 56.1959 - val_mean_squared_error: 56.1959\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.1280 - mean_squared_error: 55.1280 - val_loss: 57.5808 - val_mean_squared_error: 57.5808\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.1349 - mean_squared_error: 55.1349 - val_loss: 56.4401 - val_mean_squared_error: 56.4401\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.0644 - mean_squared_error: 55.0644 - val_loss: 55.8740 - val_mean_squared_error: 55.8740\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 55.1310 - mean_squared_error: 55.1309 - val_loss: 55.8958 - val_mean_squared_error: 55.8958\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.2231 - mean_squared_error: 55.2231 - val_loss: 56.7513 - val_mean_squared_error: 56.7513\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.1809 - mean_squared_error: 55.1809 - val_loss: 56.3949 - val_mean_squared_error: 56.3949\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.0250 - mean_squared_error: 55.0250 - val_loss: 55.8918 - val_mean_squared_error: 55.8918\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.1527 - mean_squared_error: 55.1527 - val_loss: 56.0946 - val_mean_squared_error: 56.0946\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.6397 - mean_squared_error: 55.6397 - val_loss: 56.5526 - val_mean_squared_error: 56.5526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.3702 - mean_squared_error: 55.3702 - val_loss: 55.9742 - val_mean_squared_error: 55.9742\n",
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 54.8918 - mean_squared_error: 54.8918 - val_loss: 56.5696 - val_mean_squared_error: 56.5696\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.1011 - mean_squared_error: 55.1011 - val_loss: 56.1393 - val_mean_squared_error: 56.1393\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.0721 - mean_squared_error: 55.0721 - val_loss: 55.8920 - val_mean_squared_error: 55.8920\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.0316 - mean_squared_error: 55.0316 - val_loss: 56.5205 - val_mean_squared_error: 56.5205\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.4669 - mean_squared_error: 55.4669 - val_loss: 56.9794 - val_mean_squared_error: 56.9794\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.2087 - mean_squared_error: 55.2087 - val_loss: 56.2490 - val_mean_squared_error: 56.2490\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.0926 - mean_squared_error: 55.0926 - val_loss: 56.1183 - val_mean_squared_error: 56.1183\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.0539 - mean_squared_error: 55.0539 - val_loss: 55.9316 - val_mean_squared_error: 55.9316\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.0407 - mean_squared_error: 55.0407 - val_loss: 55.9265 - val_mean_squared_error: 55.9265\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.1037 - mean_squared_error: 55.1037 - val_loss: 56.5445 - val_mean_squared_error: 56.5445\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 54.9642 - mean_squared_error: 54.9642 - val_loss: 56.1900 - val_mean_squared_error: 56.1900\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.0004 - mean_squared_error: 55.0004 - val_loss: 56.5694 - val_mean_squared_error: 56.5693\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.2987 - mean_squared_error: 55.2987 - val_loss: 56.8171 - val_mean_squared_error: 56.8171\n"
     ]
    }
   ],
   "source": [
    "# quantitative: ANN\n",
    "# specify network layers\n",
    "quant_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid', input_shape = (13, )),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'linear')\n",
    "])\n",
    "\n",
    "# compile and fit network\n",
    "quant_ann.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error']) \n",
    "history = quant_ann.fit(X_train, y_train, epochs = 1000, batch_size = 32, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('Model MSE vs. Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(bottom = 40)\n",
    "plt.ylim(top = 100)\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
