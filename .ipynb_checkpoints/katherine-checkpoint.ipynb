{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(112358)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from random import randint \n",
    "\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Track ID</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Key</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>Sunflower - Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>3KkXRkHbMCARz0aVfEt68P</td>\n",
       "      <td>90</td>\n",
       "      <td>0.55600</td>\n",
       "      <td>0.760</td>\n",
       "      <td>158040</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>-5.574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>89.911</td>\n",
       "      <td>4</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lil Baby</td>\n",
       "      <td>Drip Too Hard (Lil Baby &amp; Gunna)</td>\n",
       "      <td>78QR3Wp35dqAhFEc2qAGjE</td>\n",
       "      <td>86</td>\n",
       "      <td>0.08520</td>\n",
       "      <td>0.897</td>\n",
       "      <td>145543</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>-6.903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>112.511</td>\n",
       "      <td>4</td>\n",
       "      <td>0.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Travis Scott</td>\n",
       "      <td>SICKO MODE</td>\n",
       "      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n",
       "      <td>89</td>\n",
       "      <td>0.00513</td>\n",
       "      <td>0.834</td>\n",
       "      <td>312820</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>-3.714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>155.008</td>\n",
       "      <td>4</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Juice WRLD</td>\n",
       "      <td>Lucid Dreams</td>\n",
       "      <td>285pBltuF7vW8TeWk8hdRR</td>\n",
       "      <td>88</td>\n",
       "      <td>0.34900</td>\n",
       "      <td>0.511</td>\n",
       "      <td>239836</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>83.903</td>\n",
       "      <td>4</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>YNW Melly</td>\n",
       "      <td>Murder On My Mind</td>\n",
       "      <td>7eBqSVxrzQZtK2mmgRG6lC</td>\n",
       "      <td>86</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.759</td>\n",
       "      <td>268434</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>-7.985</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>115.007</td>\n",
       "      <td>4</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Artist                                     Track Name                Track ID  Popularity  Acousticness  Danceability  Duration_ms  Energy  Instrumentalness  Key  Liveness  Loudness  Mode  Speechiness    Tempo  Time Signature  Valence\n",
       "0           0   Post Malone  Sunflower - Spider-Man: Into the Spider-Verse  3KkXRkHbMCARz0aVfEt68P          90       0.55600         0.760       158040   0.479          0.000000    2    0.0703    -5.574     1       0.0466   89.911               4    0.913\n",
       "1           1      Lil Baby               Drip Too Hard (Lil Baby & Gunna)  78QR3Wp35dqAhFEc2qAGjE          86       0.08520         0.897       145543   0.662          0.000000    1    0.5340    -6.903     0       0.2920  112.511               4    0.389\n",
       "2           2  Travis Scott                                     SICKO MODE  2xLMifQCjDGFmkHkpNLD9h          89       0.00513         0.834       312820   0.730          0.000000    8    0.1240    -3.714     1       0.2220  155.008               4    0.446\n",
       "3           3    Juice WRLD                                   Lucid Dreams  285pBltuF7vW8TeWk8hdRR          88       0.34900         0.511       239836   0.566          0.000000    6    0.3400    -7.230     0       0.2000   83.903               4    0.218\n",
       "4           4     YNW Melly                              Murder On My Mind  7eBqSVxrzQZtK2mmgRG6lC          86       0.14500         0.759       268434   0.730          0.000003    0    0.1100    -7.985     0       0.0516  115.007               4    0.740"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 2018 songs\n",
    "songs_df = pd.read_csv('data/songs_10000.csv')\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop additional index column\n",
    "songs_df = songs_df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# calculate summary statistics\n",
    "display(songs_df.describe())\n",
    "\n",
    "# print out variable types\n",
    "print(songs_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of pairwise scatterplots\n",
    "scatter_matrix(songs_df, alpha = 0.8, figsize = (30, 20), diagonal = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new binary response variable 'tophit'\n",
    "# classify as top hit if popularity > 60 (about halfway split)\n",
    "songs_df['tophit'] = np.where(songs_df['Popularity'] > 60, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for modeling\n",
    "songs_df_clean = songs_df.drop(columns = ['Artist', 'Track Name', 'Track ID', 'Popularity'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(songs_df_clean.loc[:, songs_df_clean.columns != 'tophit'], \n",
    "                                                    songs_df_clean.tophit, test_size = 0.2, \n",
    "                                                    random_state = 100, stratify = songs_df_clean.tophit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5dff457b70d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcvmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcvmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_meanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5dff457b70d9>\u001b[0m in \u001b[0;36mcalc_meanstd\u001b[0;34m(X_train, y_train, depths)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# get cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcvmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcvstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit cross-validated single decision tree\n",
    "depths = list(range(1, 21))\n",
    "\n",
    "def calc_meanstd(X_train, y_train, depths):\n",
    "    cvmeans = {}\n",
    "    cvstds = {}\n",
    "    train_scores = {}\n",
    "    for i in depths:\n",
    "        model = DecisionTreeClassifier(max_depth = i)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_train)\n",
    "        # get training set scores\n",
    "        train_scores[i] = accuracy_score(y_train, y_pred)\n",
    "        # get cross-validation scores\n",
    "        score = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5, n_jobs = -1)\n",
    "        cvmeans[i] = score.mean()\n",
    "        cvstds[i] = score.std()\n",
    "    return cvmeans, cvstds, train_scores\n",
    "\n",
    "cvmeans, cvstds, train_scores = calc_meanstd(X_train, y_train, depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best depth: 3\n",
      "Classification accuracy on training set: 0.645\n",
      "Classification accuracy on test set: 0.6395\n"
     ]
    }
   ],
   "source": [
    "# report best tree depth from cross-validation\n",
    "best_depth = sorted(cvmeans, key = cvmeans.get, reverse = True)[0]\n",
    "print('Best depth:', best_depth)\n",
    "\n",
    "# refit on best tree depth, then report classification accuracies\n",
    "best_model = DecisionTreeClassifier(max_depth = best_depth)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "best_cv_tree_train_score = accuracy_score(y_train, y_train_pred)\n",
    "print('Classification accuracy on training set:', best_cv_tree_train_score)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "best_cv_tree_test_score = accuracy_score(y_test, y_test_pred)\n",
    "print('Classification accuracy on test set:', best_cv_tree_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['Key', 'Time Signature', 'Mode']\n",
    "X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "num_features = X_train_num.columns.tolist()\n",
    "num_index_train = X_train.index.tolist()\n",
    "num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "X_train_dum = X_train[cat_cols]\n",
    "X_test_dum = X_test[cat_cols]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.299492</td>\n",
       "      <td>0.256368</td>\n",
       "      <td>0.716427</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.231333</td>\n",
       "      <td>0.850080</td>\n",
       "      <td>0.073395</td>\n",
       "      <td>0.394990</td>\n",
       "      <td>0.246939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832329</td>\n",
       "      <td>0.492386</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>0.619231</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.813003</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.591706</td>\n",
       "      <td>0.465306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7125</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210842</td>\n",
       "      <td>0.576650</td>\n",
       "      <td>0.267664</td>\n",
       "      <td>0.302591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086815</td>\n",
       "      <td>0.821384</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.378979</td>\n",
       "      <td>0.218367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527107</td>\n",
       "      <td>0.390863</td>\n",
       "      <td>0.346355</td>\n",
       "      <td>0.362713</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.076553</td>\n",
       "      <td>0.794274</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>0.538967</td>\n",
       "      <td>0.188776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.748223</td>\n",
       "      <td>0.348043</td>\n",
       "      <td>0.724443</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.066918</td>\n",
       "      <td>0.864429</td>\n",
       "      <td>0.051760</td>\n",
       "      <td>0.524237</td>\n",
       "      <td>0.666327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.616244</td>\n",
       "      <td>0.250428</td>\n",
       "      <td>0.744484</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.188397</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.038923</td>\n",
       "      <td>0.581293</td>\n",
       "      <td>0.767347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>0.982741</td>\n",
       "      <td>0.351473</td>\n",
       "      <td>0.590172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054456</td>\n",
       "      <td>0.821585</td>\n",
       "      <td>0.263975</td>\n",
       "      <td>0.590611</td>\n",
       "      <td>0.273469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.627411</td>\n",
       "      <td>0.200290</td>\n",
       "      <td>0.561113</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.195727</td>\n",
       "      <td>0.712118</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>0.454586</td>\n",
       "      <td>0.121429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795180</td>\n",
       "      <td>0.356345</td>\n",
       "      <td>0.378925</td>\n",
       "      <td>0.290567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304639</td>\n",
       "      <td>0.809695</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>0.653706</td>\n",
       "      <td>0.343878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.237563</td>\n",
       "      <td>0.253184</td>\n",
       "      <td>0.928856</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.864920</td>\n",
       "      <td>0.448240</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>0.283673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559236</td>\n",
       "      <td>0.873096</td>\n",
       "      <td>0.226852</td>\n",
       "      <td>0.688371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209341</td>\n",
       "      <td>0.835710</td>\n",
       "      <td>0.072567</td>\n",
       "      <td>0.511415</td>\n",
       "      <td>0.763265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.852792</td>\n",
       "      <td>0.213696</td>\n",
       "      <td>0.544079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>0.823149</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.772684</td>\n",
       "      <td>0.164286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.168713</td>\n",
       "      <td>0.567126</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.428212</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.467871</td>\n",
       "      <td>0.123469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072689</td>\n",
       "      <td>0.638579</td>\n",
       "      <td>0.290009</td>\n",
       "      <td>0.609211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069431</td>\n",
       "      <td>0.834123</td>\n",
       "      <td>0.079814</td>\n",
       "      <td>0.468067</td>\n",
       "      <td>0.421429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.348392</td>\n",
       "      <td>0.968528</td>\n",
       "      <td>0.205972</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.118232</td>\n",
       "      <td>0.796643</td>\n",
       "      <td>0.379917</td>\n",
       "      <td>0.581570</td>\n",
       "      <td>0.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965863</td>\n",
       "      <td>0.615228</td>\n",
       "      <td>0.229943</td>\n",
       "      <td>0.218421</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.681142</td>\n",
       "      <td>0.070083</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.538776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.225381</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.417740</td>\n",
       "      <td>0.165162</td>\n",
       "      <td>0.066563</td>\n",
       "      <td>0.449593</td>\n",
       "      <td>0.037143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.243781</td>\n",
       "      <td>0.873745</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.289978</td>\n",
       "      <td>0.902356</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>0.558812</td>\n",
       "      <td>0.386735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.726904</td>\n",
       "      <td>0.328276</td>\n",
       "      <td>0.542075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.154244</td>\n",
       "      <td>0.635977</td>\n",
       "      <td>0.394898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>0.396954</td>\n",
       "      <td>0.303313</td>\n",
       "      <td>0.650294</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.227144</td>\n",
       "      <td>0.841096</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.787141</td>\n",
       "      <td>0.205102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588352</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.621235</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.173735</td>\n",
       "      <td>0.837878</td>\n",
       "      <td>0.087578</td>\n",
       "      <td>0.708849</td>\n",
       "      <td>0.412245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.967513</td>\n",
       "      <td>0.212773</td>\n",
       "      <td>0.555101</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.088910</td>\n",
       "      <td>0.881660</td>\n",
       "      <td>0.078675</td>\n",
       "      <td>0.458934</td>\n",
       "      <td>0.466327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6979</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513051</td>\n",
       "      <td>0.716751</td>\n",
       "      <td>0.247447</td>\n",
       "      <td>0.380749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.845365</td>\n",
       "      <td>0.338509</td>\n",
       "      <td>0.294681</td>\n",
       "      <td>0.388776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.151605</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.233786</td>\n",
       "      <td>0.491974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.829474</td>\n",
       "      <td>0.078364</td>\n",
       "      <td>0.590462</td>\n",
       "      <td>0.268367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.727911</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.219042</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.270081</td>\n",
       "      <td>0.647036</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.454423</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.264132</td>\n",
       "      <td>0.729453</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.866284</td>\n",
       "      <td>0.043375</td>\n",
       "      <td>0.558821</td>\n",
       "      <td>0.520408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.564467</td>\n",
       "      <td>0.322924</td>\n",
       "      <td>0.672338</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.853813</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>0.539086</td>\n",
       "      <td>0.268392</td>\n",
       "      <td>0.848694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073830</td>\n",
       "      <td>0.908770</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.477194</td>\n",
       "      <td>0.387755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.958835</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.099799</td>\n",
       "      <td>0.359707</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.073725</td>\n",
       "      <td>0.779099</td>\n",
       "      <td>0.305383</td>\n",
       "      <td>0.382578</td>\n",
       "      <td>0.848980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065159</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092052</td>\n",
       "      <td>0.854237</td>\n",
       "      <td>0.365424</td>\n",
       "      <td>0.680530</td>\n",
       "      <td>0.723469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.521827</td>\n",
       "      <td>0.251391</td>\n",
       "      <td>0.362713</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>0.746178</td>\n",
       "      <td>0.045756</td>\n",
       "      <td>0.345163</td>\n",
       "      <td>0.076837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.273091</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>0.303535</td>\n",
       "      <td>0.508006</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.867602</td>\n",
       "      <td>0.052588</td>\n",
       "      <td>0.464718</td>\n",
       "      <td>0.690816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.574619</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.412814</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.839777</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.372010</td>\n",
       "      <td>0.479592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117468</td>\n",
       "      <td>0.458883</td>\n",
       "      <td>0.288229</td>\n",
       "      <td>0.758512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127657</td>\n",
       "      <td>0.899473</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.485868</td>\n",
       "      <td>0.304082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237950</td>\n",
       "      <td>0.665990</td>\n",
       "      <td>0.195294</td>\n",
       "      <td>0.380749</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>0.867424</td>\n",
       "      <td>0.052070</td>\n",
       "      <td>0.622452</td>\n",
       "      <td>0.954082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703815</td>\n",
       "      <td>0.567513</td>\n",
       "      <td>0.388915</td>\n",
       "      <td>0.422834</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.067232</td>\n",
       "      <td>0.730355</td>\n",
       "      <td>0.201863</td>\n",
       "      <td>0.336221</td>\n",
       "      <td>0.194898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633533</td>\n",
       "      <td>0.168528</td>\n",
       "      <td>0.246161</td>\n",
       "      <td>0.319626</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.414598</td>\n",
       "      <td>0.645383</td>\n",
       "      <td>0.035611</td>\n",
       "      <td>0.507781</td>\n",
       "      <td>0.034796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.255018</td>\n",
       "      <td>0.565482</td>\n",
       "      <td>0.303818</td>\n",
       "      <td>0.512014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.844381</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.708963</td>\n",
       "      <td>0.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222890</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.221952</td>\n",
       "      <td>0.856711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503613</td>\n",
       "      <td>0.877235</td>\n",
       "      <td>0.244306</td>\n",
       "      <td>0.572211</td>\n",
       "      <td>0.784694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.621235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.902132</td>\n",
       "      <td>0.083954</td>\n",
       "      <td>0.442537</td>\n",
       "      <td>0.413265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072387</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.381846</td>\n",
       "      <td>0.608209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359095</td>\n",
       "      <td>0.863557</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.599680</td>\n",
       "      <td>0.531633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.146584</td>\n",
       "      <td>0.534010</td>\n",
       "      <td>0.240864</td>\n",
       "      <td>0.412814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.753615</td>\n",
       "      <td>0.270408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.433420</td>\n",
       "      <td>0.563117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294167</td>\n",
       "      <td>0.842728</td>\n",
       "      <td>0.169772</td>\n",
       "      <td>0.613424</td>\n",
       "      <td>0.447959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370481</td>\n",
       "      <td>0.758376</td>\n",
       "      <td>0.249414</td>\n",
       "      <td>0.558107</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.085768</td>\n",
       "      <td>0.795593</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.681352</td>\n",
       "      <td>0.563265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458834</td>\n",
       "      <td>0.773604</td>\n",
       "      <td>0.206872</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>0.867245</td>\n",
       "      <td>0.107660</td>\n",
       "      <td>0.704192</td>\n",
       "      <td>0.813265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>0.641624</td>\n",
       "      <td>0.177856</td>\n",
       "      <td>0.728451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>0.831754</td>\n",
       "      <td>0.077536</td>\n",
       "      <td>0.734701</td>\n",
       "      <td>0.415306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.725888</td>\n",
       "      <td>0.151669</td>\n",
       "      <td>0.618229</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.172688</td>\n",
       "      <td>0.871871</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.800172</td>\n",
       "      <td>0.288776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.883249</td>\n",
       "      <td>0.304406</td>\n",
       "      <td>0.665324</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.568058</td>\n",
       "      <td>0.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035440</td>\n",
       "      <td>0.939086</td>\n",
       "      <td>0.260079</td>\n",
       "      <td>0.737470</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.667940</td>\n",
       "      <td>0.503061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>0.752284</td>\n",
       "      <td>0.190576</td>\n",
       "      <td>0.557105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080637</td>\n",
       "      <td>0.847533</td>\n",
       "      <td>0.098033</td>\n",
       "      <td>0.636332</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300199</td>\n",
       "      <td>0.326904</td>\n",
       "      <td>0.432783</td>\n",
       "      <td>0.755506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120327</td>\n",
       "      <td>0.852695</td>\n",
       "      <td>0.284679</td>\n",
       "      <td>0.285503</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.689758</td>\n",
       "      <td>0.712690</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.144413</td>\n",
       "      <td>0.843487</td>\n",
       "      <td>0.156315</td>\n",
       "      <td>0.759108</td>\n",
       "      <td>0.231633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296183</td>\n",
       "      <td>0.656853</td>\n",
       "      <td>0.238091</td>\n",
       "      <td>0.646285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095193</td>\n",
       "      <td>0.916525</td>\n",
       "      <td>0.032195</td>\n",
       "      <td>0.608867</td>\n",
       "      <td>0.439796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161645</td>\n",
       "      <td>0.792893</td>\n",
       "      <td>0.306218</td>\n",
       "      <td>0.629251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073934</td>\n",
       "      <td>0.876743</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.536086</td>\n",
       "      <td>0.293878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488955</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.387787</td>\n",
       "      <td>0.313613</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.699133</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.654124</td>\n",
       "      <td>0.230612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.869036</td>\n",
       "      <td>0.152085</td>\n",
       "      <td>0.546083</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.131846</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.731489</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.366464</td>\n",
       "      <td>0.868020</td>\n",
       "      <td>0.179658</td>\n",
       "      <td>0.307601</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.087863</td>\n",
       "      <td>0.759208</td>\n",
       "      <td>0.075776</td>\n",
       "      <td>0.408875</td>\n",
       "      <td>0.177551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046986</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.269306</td>\n",
       "      <td>0.880759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073097</td>\n",
       "      <td>0.920369</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.626913</td>\n",
       "      <td>0.772449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684738</td>\n",
       "      <td>0.411168</td>\n",
       "      <td>0.453074</td>\n",
       "      <td>0.420830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053304</td>\n",
       "      <td>0.835308</td>\n",
       "      <td>0.035197</td>\n",
       "      <td>0.641184</td>\n",
       "      <td>0.353061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.791164</td>\n",
       "      <td>0.525888</td>\n",
       "      <td>0.132431</td>\n",
       "      <td>0.442875</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.167452</td>\n",
       "      <td>0.785089</td>\n",
       "      <td>0.259834</td>\n",
       "      <td>0.372501</td>\n",
       "      <td>0.531633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Key  Time Signature  Mode  Acousticness  Danceability  Duration_ms    Energy  Instrumentalness  Liveness  Loudness  Speechiness     Tempo   Valence\n",
       "5760    2               4     1      0.002126      0.299492     0.256368  0.716427          0.000002  0.231333  0.850080     0.073395  0.394990  0.246939\n",
       "5683    2               4     1      0.832329      0.492386     0.225507  0.619231          0.000004  0.155933  0.813003     0.065942  0.591706  0.465306\n",
       "7125    6               1     1      0.210842      0.576650     0.267664  0.302591          0.000000  0.086815  0.821384     0.029089  0.378979  0.218367\n",
       "8092    2               4     1      0.527107      0.390863     0.346355  0.362713          0.000120  0.076553  0.794274     0.036128  0.538967  0.188776\n",
       "5228    8               4     1      0.262047      0.748223     0.348043  0.724443          0.000097  0.066918  0.864429     0.051760  0.524237  0.666327\n",
       "4715    2               4     1      0.000569      0.616244     0.250428  0.744484          0.000010  0.188397  0.884700     0.038923  0.581293  0.767347\n",
       "645    11               4     1      0.051303      0.982741     0.351473  0.590172          0.000000  0.054456  0.821585     0.263975  0.590611  0.273469\n",
       "4600    4               4     0      0.052307      0.627411     0.200290  0.561113          0.938000  0.195727  0.712118     0.025466  0.454586  0.121429\n",
       "6146    0               4     1      0.795180      0.356345     0.378925  0.290567          0.000000  0.304639  0.809695     0.030642  0.653706  0.343878\n",
       "7682    5               3     0      0.000010      0.237563     0.253184  0.928856          0.000039  0.097288  0.864920     0.448240  0.595200  0.283673\n",
       "3434    5               4     1      0.559236      0.873096     0.226852  0.688371          0.000000  0.209341  0.835710     0.072567  0.511415  0.763265\n",
       "4881    1               4     1      0.040058      0.852792     0.213696  0.544079          0.000000  0.100429  0.823149     0.492754  0.772684  0.164286\n",
       "9798    0               4     1      0.014255      0.560406     0.168713  0.567126          0.958000  0.428212  0.829452     0.029296  0.467871  0.123469\n",
       "2733    2               4     0      0.072689      0.638579     0.290009  0.609211          0.000000  0.069431  0.834123     0.079814  0.468067  0.421429\n",
       "1072    7               4     1      0.348392      0.968528     0.205972  0.581154          0.000002  0.118232  0.796643     0.379917  0.581570  0.489796\n",
       "8222    4               4     1      0.965863      0.615228     0.229943  0.218421          0.860000  0.094146  0.681142     0.070083  0.363673  0.538776\n",
       "9466    4               4     0      0.004626      0.225381     0.052391  0.312611          0.847000  0.417740  0.165162     0.066563  0.449593  0.037143\n",
       "1437    6               4     1      0.033130      0.786802     0.243781  0.873745          0.057900  0.289978  0.902356     0.048551  0.558812  0.386735\n",
       "201     1               4     1      0.002106      0.726904     0.328276  0.542075          0.000000  0.098335  0.819328     0.154244  0.635977  0.394898\n",
       "6710    3               4     0      0.351404      0.396954     0.303313  0.650294          0.000100  0.227144  0.841096     0.040890  0.787141  0.205102\n",
       "1868    5               4     0      0.588352      0.769543     0.221996  0.621235          0.000078  0.173735  0.837878     0.087578  0.708849  0.412245\n",
       "2018    0               4     1      0.618473      0.967513     0.212773  0.555101          0.000869  0.088910  0.881660     0.078675  0.458934  0.466327\n",
       "6979    2               4     1      0.513051      0.716751     0.247447  0.380749          0.000000  0.234475  0.845365     0.338509  0.294681  0.388776\n",
       "6392    7               4     0      0.151605      0.769543     0.233786  0.491974          0.000000  0.094146  0.829474     0.078364  0.590462  0.268367\n",
       "2611    4               4     0      0.727911      0.667005     0.219042  0.159302          0.730000  0.270081  0.647036     0.040890  0.454423  0.085000\n",
       "260     7               4     1      0.001755      0.676142     0.264132  0.729453          0.000004  0.022830  0.866284     0.043375  0.558821  0.520408\n",
       "6771   11               4     0      0.032026      0.564467     0.322924  0.672338          0.000377  0.094146  0.853813     0.066977  0.477317  0.257143\n",
       "3105    1               4     1      0.007458      0.539086     0.268392  0.848694          0.000000  0.073830  0.908770     0.054762  0.477194  0.387755\n",
       "5767    1               4     1      0.958835      0.707614     0.099799  0.359707          0.899000  0.073725  0.779099     0.305383  0.382578  0.848980\n",
       "6017    1               4     1      0.065159      0.857868     0.205877  0.581154          0.000000  0.092052  0.854237     0.365424  0.680530  0.723469\n",
       "...   ...             ...   ...           ...           ...          ...       ...               ...       ...       ...          ...       ...       ...\n",
       "7371    9               4     0      0.794176      0.521827     0.251391  0.362713          0.013900  0.084721  0.746178     0.045756  0.345163  0.076837\n",
       "9581    2               4     1      0.273091      0.867005     0.303535  0.508006          0.001970  0.058435  0.867602     0.052588  0.464718  0.690816\n",
       "1485    5               4     1      0.683734      0.574619     0.242194  0.412814          0.017000  0.055503  0.839777     0.027640  0.372010  0.479592\n",
       "6906    8               3     0      0.117468      0.458883     0.288229  0.758512          0.000000  0.127657  0.899473     0.261905  0.485868  0.304082\n",
       "1690    8               4     1      0.237950      0.665990     0.195294  0.380749          0.000456  0.084721  0.867424     0.052070  0.622452  0.954082\n",
       "409    11               4     1      0.703815      0.567513     0.388915  0.422834          0.000002  0.067232  0.730355     0.201863  0.336221  0.194898\n",
       "9326    0               4     1      0.633533      0.168528     0.246161  0.319626          0.842000  0.414598  0.645383     0.035611  0.507781  0.034796\n",
       "321     8               4     1      0.255018      0.565482     0.303818  0.512014          0.000000  0.081579  0.844381     0.030124  0.708963  0.614286\n",
       "4448   11               4     0      0.222890      0.676142     0.221952  0.856711          0.000000  0.503613  0.877235     0.244306  0.572211  0.784694\n",
       "4822    6               3     1      0.618473      0.392893     0.247304  0.621235          0.000000  0.064509  0.902132     0.083954  0.442537  0.413265\n",
       "9559    7               4     0      0.072387      0.734010     0.381846  0.608209          0.000000  0.359095  0.863557     0.158385  0.599680  0.531633\n",
       "4724    9               4     1      0.146584      0.534010     0.240864  0.412814          0.000000  0.087863  0.778808     0.046480  0.753615  0.270408\n",
       "1191   11               4     1      0.021685      0.560406     0.433420  0.563117          0.000000  0.294167  0.842728     0.169772  0.613424  0.447959\n",
       "4855    1               4     0      0.370481      0.758376     0.249414  0.558107          0.000035  0.085768  0.795593     0.071429  0.681352  0.563265\n",
       "2364    5               4     0      0.458834      0.773604     0.206872  0.739474          0.000000  0.098335  0.867245     0.107660  0.704192  0.813265\n",
       "637     0               4     1      0.045179      0.641624     0.177856  0.728451          0.000000  0.100429  0.831754     0.077536  0.734701  0.415306\n",
       "1094   11               4     0      0.099998      0.725888     0.151669  0.618229          0.000003  0.172688  0.871871     0.248447  0.800172  0.288776\n",
       "7665    9               4     1      0.007558      0.883249     0.304406  0.665324          0.341000  0.031731  0.820021     0.064182  0.568058  0.871429\n",
       "4581    1               4     1      0.035440      0.939086     0.260079  0.737470          0.000156  0.079066  0.856115     0.136646  0.667940  0.503061\n",
       "504     7               4     0      0.220882      0.752284     0.190576  0.557105          0.000000  0.080637  0.847533     0.098033  0.636332  0.628571\n",
       "1126   10               4     0      0.300199      0.326904     0.432783  0.755506          0.000000  0.120327  0.852695     0.284679  0.285503  0.400000\n",
       "3523    4               4     0      0.689758      0.712690     0.043799  0.499990          0.000002  0.144413  0.843487     0.156315  0.759108  0.231633\n",
       "7591    7               3     1      0.296183      0.656853     0.238091  0.646285          0.000000  0.095193  0.916525     0.032195  0.608867  0.439796\n",
       "5373    1               4     1      0.161645      0.792893     0.306218  0.629251          0.000000  0.073934  0.876743     0.069151  0.536086  0.293878\n",
       "7228    5               3     1      0.488955      0.670051     0.387787  0.313613          0.003490  0.087863  0.699133     0.027226  0.654124  0.230612\n",
       "4915    1               4     1      0.000601      0.869036     0.152085  0.546083          0.000973  0.131846  0.752749     0.261905  0.731489  0.050000\n",
       "2563    1               4     0      0.366464      0.868020     0.179658  0.307601          0.854000  0.087863  0.759208     0.075776  0.408875  0.177551\n",
       "223     4               4     1      0.046986      0.613198     0.269306  0.880759          0.000000  0.073097  0.920369     0.033333  0.626913  0.772449\n",
       "7161    9               3     1      0.684738      0.411168     0.453074  0.420830          0.000000  0.053304  0.835308     0.035197  0.641184  0.353061\n",
       "3638    0               4     0      0.791164      0.525888     0.132431  0.442875          0.904000  0.167452  0.785089     0.259834  0.372501  0.531633\n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 1s 95us/sample - loss: 0.6562 - accuracy: 0.6414 - val_loss: 0.6504 - val_accuracy: 0.6454\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6534 - accuracy: 0.6416 - val_loss: 0.6483 - val_accuracy: 0.6454\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6495 - accuracy: 0.6416 - val_loss: 0.6501 - val_accuracy: 0.6454\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6518 - accuracy: 0.6409 - val_loss: 0.6433 - val_accuracy: 0.6454\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6441 - accuracy: 0.6420 - val_loss: 0.6436 - val_accuracy: 0.6446\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6461 - accuracy: 0.6414 - val_loss: 0.6497 - val_accuracy: 0.6442\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6437 - accuracy: 0.6423 - val_loss: 0.6435 - val_accuracy: 0.6442\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6441 - accuracy: 0.6425 - val_loss: 0.6411 - val_accuracy: 0.6438\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6471 - accuracy: 0.6413 - val_loss: 0.6425 - val_accuracy: 0.6442\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 0s 53us/sample - loss: 0.6414 - accuracy: 0.6427 - val_loss: 0.6383 - val_accuracy: 0.6442\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 0.6402 - accuracy: 0.6432 - val_loss: 0.6391 - val_accuracy: 0.6438\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6383 - accuracy: 0.6434 - val_loss: 0.6428 - val_accuracy: 0.6429\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 0.6383 - accuracy: 0.6439 - val_loss: 0.6409 - val_accuracy: 0.6421\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 0s 51us/sample - loss: 0.6367 - accuracy: 0.6432 - val_loss: 0.6367 - val_accuracy: 0.6433\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 48us/sample - loss: 0.6360 - accuracy: 0.6432 - val_loss: 0.6459 - val_accuracy: 0.6375\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6352 - accuracy: 0.6432 - val_loss: 0.6410 - val_accuracy: 0.6408\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6356 - accuracy: 0.6443 - val_loss: 0.6437 - val_accuracy: 0.6383\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6389 - accuracy: 0.6455 - val_loss: 0.6377 - val_accuracy: 0.6429\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6345 - accuracy: 0.6446 - val_loss: 0.6433 - val_accuracy: 0.6433\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6324 - accuracy: 0.6439 - val_loss: 0.6392 - val_accuracy: 0.6429\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6322 - accuracy: 0.6459 - val_loss: 0.6395 - val_accuracy: 0.6425\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6324 - accuracy: 0.6450 - val_loss: 0.6395 - val_accuracy: 0.6408\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6290 - accuracy: 0.6457 - val_loss: 0.6411 - val_accuracy: 0.6383\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6307 - accuracy: 0.6425 - val_loss: 0.6399 - val_accuracy: 0.6392\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6273 - accuracy: 0.6475 - val_loss: 0.6430 - val_accuracy: 0.6358\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6266 - accuracy: 0.6486 - val_loss: 0.6398 - val_accuracy: 0.6383\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6285 - accuracy: 0.6446 - val_loss: 0.6434 - val_accuracy: 0.6400\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6263 - accuracy: 0.6495 - val_loss: 0.6428 - val_accuracy: 0.6404\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6254 - accuracy: 0.6482 - val_loss: 0.6450 - val_accuracy: 0.6363\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6282 - accuracy: 0.6466 - val_loss: 0.6415 - val_accuracy: 0.6388\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 0s 41us/sample - loss: 0.6230 - accuracy: 0.6489 - val_loss: 0.6473 - val_accuracy: 0.6404\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6250 - accuracy: 0.6511 - val_loss: 0.6444 - val_accuracy: 0.6371\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6242 - accuracy: 0.6500 - val_loss: 0.6454 - val_accuracy: 0.6358\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6224 - accuracy: 0.6507 - val_loss: 0.6433 - val_accuracy: 0.6404\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6240 - accuracy: 0.6496 - val_loss: 0.6419 - val_accuracy: 0.6438\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6210 - accuracy: 0.6513 - val_loss: 0.6511 - val_accuracy: 0.6379\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6208 - accuracy: 0.6507 - val_loss: 0.6502 - val_accuracy: 0.6442\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6200 - accuracy: 0.6511 - val_loss: 0.6451 - val_accuracy: 0.6388\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6178 - accuracy: 0.6539 - val_loss: 0.6462 - val_accuracy: 0.6371\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6172 - accuracy: 0.6548 - val_loss: 0.6495 - val_accuracy: 0.6371\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6191 - accuracy: 0.6521 - val_loss: 0.6501 - val_accuracy: 0.6350\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6171 - accuracy: 0.6552 - val_loss: 0.6450 - val_accuracy: 0.6325\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6163 - accuracy: 0.6571 - val_loss: 0.6482 - val_accuracy: 0.6346\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6140 - accuracy: 0.6550 - val_loss: 0.6477 - val_accuracy: 0.6338\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6135 - accuracy: 0.6479 - val_loss: 0.6520 - val_accuracy: 0.6425\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6162 - accuracy: 0.6509 - val_loss: 0.6541 - val_accuracy: 0.6413\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6121 - accuracy: 0.6582 - val_loss: 0.6502 - val_accuracy: 0.6325\n",
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6117 - accuracy: 0.6557 - val_loss: 0.6477 - val_accuracy: 0.6321\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6116 - accuracy: 0.6539 - val_loss: 0.6476 - val_accuracy: 0.6329\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6077 - accuracy: 0.6546 - val_loss: 0.6584 - val_accuracy: 0.6383\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6104 - accuracy: 0.6555 - val_loss: 0.6521 - val_accuracy: 0.6250\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6067 - accuracy: 0.6557 - val_loss: 0.6550 - val_accuracy: 0.6342\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6076 - accuracy: 0.6562 - val_loss: 0.6568 - val_accuracy: 0.6325\n",
      "Epoch 54/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6059 - accuracy: 0.6577 - val_loss: 0.6537 - val_accuracy: 0.6304\n",
      "Epoch 55/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6048 - accuracy: 0.6646 - val_loss: 0.6602 - val_accuracy: 0.6233\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6042 - accuracy: 0.6587 - val_loss: 0.6627 - val_accuracy: 0.6304\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6051 - accuracy: 0.6536 - val_loss: 0.6586 - val_accuracy: 0.6375\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6014 - accuracy: 0.6605 - val_loss: 0.6655 - val_accuracy: 0.6292\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6029 - accuracy: 0.6591 - val_loss: 0.6615 - val_accuracy: 0.6096\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6022 - accuracy: 0.6627 - val_loss: 0.6667 - val_accuracy: 0.6358\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6021 - accuracy: 0.6555 - val_loss: 0.6660 - val_accuracy: 0.6329\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5993 - accuracy: 0.6591 - val_loss: 0.6618 - val_accuracy: 0.6258\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5970 - accuracy: 0.6587 - val_loss: 0.6701 - val_accuracy: 0.6212\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5979 - accuracy: 0.6643 - val_loss: 0.6697 - val_accuracy: 0.6229\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5976 - accuracy: 0.6664 - val_loss: 0.6776 - val_accuracy: 0.6338\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5958 - accuracy: 0.6614 - val_loss: 0.6709 - val_accuracy: 0.6313\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5964 - accuracy: 0.6591 - val_loss: 0.6704 - val_accuracy: 0.6067\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5979 - accuracy: 0.6621 - val_loss: 0.6722 - val_accuracy: 0.6279\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5922 - accuracy: 0.6652 - val_loss: 0.6677 - val_accuracy: 0.6267\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5919 - accuracy: 0.6643 - val_loss: 0.6742 - val_accuracy: 0.6104\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5917 - accuracy: 0.6584 - val_loss: 0.6765 - val_accuracy: 0.6263\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5899 - accuracy: 0.6686 - val_loss: 0.6781 - val_accuracy: 0.6187\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5865 - accuracy: 0.6698 - val_loss: 0.6761 - val_accuracy: 0.6246\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5869 - accuracy: 0.6684 - val_loss: 0.6763 - val_accuracy: 0.6254\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5854 - accuracy: 0.6704 - val_loss: 0.6822 - val_accuracy: 0.6196\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5857 - accuracy: 0.6734 - val_loss: 0.6814 - val_accuracy: 0.6317\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5829 - accuracy: 0.6746 - val_loss: 0.6828 - val_accuracy: 0.6283\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5816 - accuracy: 0.6768 - val_loss: 0.6877 - val_accuracy: 0.6112\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5818 - accuracy: 0.6687 - val_loss: 0.6925 - val_accuracy: 0.6217\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5824 - accuracy: 0.6687 - val_loss: 0.6975 - val_accuracy: 0.6217\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5811 - accuracy: 0.6698 - val_loss: 0.6895 - val_accuracy: 0.6033\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5791 - accuracy: 0.6741 - val_loss: 0.6880 - val_accuracy: 0.6242\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5810 - accuracy: 0.6779 - val_loss: 0.6927 - val_accuracy: 0.6075\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5772 - accuracy: 0.6770 - val_loss: 0.6905 - val_accuracy: 0.6158\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5741 - accuracy: 0.6775 - val_loss: 0.7001 - val_accuracy: 0.6263\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5732 - accuracy: 0.6770 - val_loss: 0.6987 - val_accuracy: 0.6254\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5722 - accuracy: 0.6777 - val_loss: 0.7091 - val_accuracy: 0.6258\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5742 - accuracy: 0.6812 - val_loss: 0.6964 - val_accuracy: 0.6046\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5709 - accuracy: 0.6809 - val_loss: 0.7158 - val_accuracy: 0.6208\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5743 - accuracy: 0.6716 - val_loss: 0.6993 - val_accuracy: 0.6288\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5719 - accuracy: 0.6770 - val_loss: 0.6992 - val_accuracy: 0.6283\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5660 - accuracy: 0.6814 - val_loss: 0.7104 - val_accuracy: 0.6246\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5636 - accuracy: 0.6830 - val_loss: 0.7164 - val_accuracy: 0.6275\n",
      "Epoch 94/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5672 - accuracy: 0.6804 - val_loss: 0.7065 - val_accuracy: 0.6271\n",
      "Epoch 95/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5652 - accuracy: 0.6877 - val_loss: 0.7183 - val_accuracy: 0.6279\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5654 - accuracy: 0.6841 - val_loss: 0.7133 - val_accuracy: 0.6121\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5592 - accuracy: 0.6882 - val_loss: 0.7214 - val_accuracy: 0.6125\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5603 - accuracy: 0.6875 - val_loss: 0.7384 - val_accuracy: 0.6117\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5635 - accuracy: 0.6913 - val_loss: 0.7101 - val_accuracy: 0.6225\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5578 - accuracy: 0.6857 - val_loss: 0.7221 - val_accuracy: 0.6142\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5580 - accuracy: 0.6932 - val_loss: 0.7296 - val_accuracy: 0.6204\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5536 - accuracy: 0.6939 - val_loss: 0.7342 - val_accuracy: 0.6183\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5572 - accuracy: 0.6913 - val_loss: 0.7248 - val_accuracy: 0.6275\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5522 - accuracy: 0.6963 - val_loss: 0.7259 - val_accuracy: 0.6192\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5496 - accuracy: 0.6952 - val_loss: 0.7387 - val_accuracy: 0.6192\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5487 - accuracy: 0.6902 - val_loss: 0.7315 - val_accuracy: 0.6187\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5478 - accuracy: 0.6975 - val_loss: 0.7389 - val_accuracy: 0.6150\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5446 - accuracy: 0.6934 - val_loss: 0.7450 - val_accuracy: 0.6208\n",
      "Epoch 109/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.5410 - accuracy: 0.7054 - val_loss: 0.7483 - val_accuracy: 0.6167\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5445 - accuracy: 0.7027 - val_loss: 0.7545 - val_accuracy: 0.6400\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5441 - accuracy: 0.7068 - val_loss: 0.7428 - val_accuracy: 0.6150\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5410 - accuracy: 0.7055 - val_loss: 0.7522 - val_accuracy: 0.6146\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5419 - accuracy: 0.7054 - val_loss: 0.7487 - val_accuracy: 0.6217\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5381 - accuracy: 0.7027 - val_loss: 0.7562 - val_accuracy: 0.6087\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5396 - accuracy: 0.7039 - val_loss: 0.7732 - val_accuracy: 0.5954\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5419 - accuracy: 0.7004 - val_loss: 0.7543 - val_accuracy: 0.6100\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5330 - accuracy: 0.7082 - val_loss: 0.7597 - val_accuracy: 0.6242\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5370 - accuracy: 0.7034 - val_loss: 0.7595 - val_accuracy: 0.6292\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5385 - accuracy: 0.7066 - val_loss: 0.7658 - val_accuracy: 0.6237\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5328 - accuracy: 0.7086 - val_loss: 0.7579 - val_accuracy: 0.6196\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5245 - accuracy: 0.7191 - val_loss: 0.7705 - val_accuracy: 0.6121\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5282 - accuracy: 0.7173 - val_loss: 0.7607 - val_accuracy: 0.6171\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5273 - accuracy: 0.7139 - val_loss: 0.7723 - val_accuracy: 0.6229\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5270 - accuracy: 0.7159 - val_loss: 0.7786 - val_accuracy: 0.6217\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5264 - accuracy: 0.7096 - val_loss: 0.7642 - val_accuracy: 0.6067\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5229 - accuracy: 0.7179 - val_loss: 0.7789 - val_accuracy: 0.6150\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.5187 - accuracy: 0.7195 - val_loss: 0.7798 - val_accuracy: 0.6167\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5184 - accuracy: 0.7221 - val_loss: 0.7823 - val_accuracy: 0.6183\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5176 - accuracy: 0.7255 - val_loss: 0.7854 - val_accuracy: 0.6117\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5147 - accuracy: 0.7259 - val_loss: 0.7919 - val_accuracy: 0.6175\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5176 - accuracy: 0.7204 - val_loss: 0.7935 - val_accuracy: 0.6246\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5104 - accuracy: 0.7236 - val_loss: 0.7881 - val_accuracy: 0.6162\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.5173 - accuracy: 0.7234 - val_loss: 0.8011 - val_accuracy: 0.6150\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5141 - accuracy: 0.7223 - val_loss: 0.7950 - val_accuracy: 0.6208\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5075 - accuracy: 0.7275 - val_loss: 0.8097 - val_accuracy: 0.6150\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5061 - accuracy: 0.7309 - val_loss: 0.8010 - val_accuracy: 0.6200\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.5046 - accuracy: 0.7334 - val_loss: 0.7977 - val_accuracy: 0.6142\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5060 - accuracy: 0.7323 - val_loss: 0.8090 - val_accuracy: 0.6221\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5001 - accuracy: 0.7364 - val_loss: 0.8134 - val_accuracy: 0.6146\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5048 - accuracy: 0.7314 - val_loss: 0.8158 - val_accuracy: 0.6183\n",
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5016 - accuracy: 0.7362 - val_loss: 0.8188 - val_accuracy: 0.6150\n",
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.5040 - accuracy: 0.7296 - val_loss: 0.8289 - val_accuracy: 0.6196\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.5016 - accuracy: 0.7334 - val_loss: 0.8164 - val_accuracy: 0.6196\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4924 - accuracy: 0.7427 - val_loss: 0.8233 - val_accuracy: 0.6208\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4937 - accuracy: 0.7352 - val_loss: 0.8361 - val_accuracy: 0.6167\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4976 - accuracy: 0.7375 - val_loss: 0.8452 - val_accuracy: 0.6150\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4911 - accuracy: 0.7454 - val_loss: 0.8308 - val_accuracy: 0.6162\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4812 - accuracy: 0.7480 - val_loss: 0.8256 - val_accuracy: 0.6167\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4804 - accuracy: 0.7482 - val_loss: 0.8584 - val_accuracy: 0.6208\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4808 - accuracy: 0.7484 - val_loss: 0.8325 - val_accuracy: 0.6142\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4817 - accuracy: 0.7437 - val_loss: 0.8479 - val_accuracy: 0.6017\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4851 - accuracy: 0.7391 - val_loss: 0.8620 - val_accuracy: 0.5867\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4910 - accuracy: 0.7464 - val_loss: 0.8443 - val_accuracy: 0.6183\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4834 - accuracy: 0.7521 - val_loss: 0.8483 - val_accuracy: 0.6242\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4801 - accuracy: 0.7379 - val_loss: 0.8483 - val_accuracy: 0.6121\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4728 - accuracy: 0.7580 - val_loss: 0.8500 - val_accuracy: 0.6075\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4730 - accuracy: 0.7502 - val_loss: 0.8678 - val_accuracy: 0.6008\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4777 - accuracy: 0.7486 - val_loss: 0.8945 - val_accuracy: 0.6154\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4732 - accuracy: 0.7479 - val_loss: 0.8694 - val_accuracy: 0.6096\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4691 - accuracy: 0.7550 - val_loss: 0.8745 - val_accuracy: 0.6204\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4709 - accuracy: 0.7552 - val_loss: 0.8818 - val_accuracy: 0.5996\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4695 - accuracy: 0.7527 - val_loss: 0.8732 - val_accuracy: 0.6079\n",
      "Epoch 163/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4670 - accuracy: 0.7564 - val_loss: 0.8851 - val_accuracy: 0.6092\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4581 - accuracy: 0.7659 - val_loss: 0.8759 - val_accuracy: 0.6029\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4587 - accuracy: 0.7650 - val_loss: 0.8811 - val_accuracy: 0.6154\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4587 - accuracy: 0.7634 - val_loss: 0.8858 - val_accuracy: 0.6058\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4550 - accuracy: 0.7623 - val_loss: 0.8952 - val_accuracy: 0.6125\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4631 - accuracy: 0.7596 - val_loss: 0.9162 - val_accuracy: 0.6104\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4538 - accuracy: 0.7713 - val_loss: 0.8966 - val_accuracy: 0.6108\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4553 - accuracy: 0.7661 - val_loss: 0.9001 - val_accuracy: 0.6021\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4533 - accuracy: 0.7659 - val_loss: 0.9029 - val_accuracy: 0.6121\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4595 - accuracy: 0.7579 - val_loss: 0.9178 - val_accuracy: 0.5971\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4616 - accuracy: 0.7634 - val_loss: 0.9341 - val_accuracy: 0.6087\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4481 - accuracy: 0.7686 - val_loss: 0.9181 - val_accuracy: 0.6012\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.4431 - accuracy: 0.7752 - val_loss: 0.9064 - val_accuracy: 0.5904\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.4446 - accuracy: 0.7741 - val_loss: 0.9221 - val_accuracy: 0.6225\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4420 - accuracy: 0.7764 - val_loss: 0.9269 - val_accuracy: 0.5983\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4392 - accuracy: 0.7761 - val_loss: 0.9074 - val_accuracy: 0.6067\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.4416 - accuracy: 0.7743 - val_loss: 0.9365 - val_accuracy: 0.6079\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4405 - accuracy: 0.7814 - val_loss: 0.9238 - val_accuracy: 0.5967\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4342 - accuracy: 0.7784 - val_loss: 0.9299 - val_accuracy: 0.6171\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4373 - accuracy: 0.7732 - val_loss: 0.9417 - val_accuracy: 0.6025\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4341 - accuracy: 0.7796 - val_loss: 0.9450 - val_accuracy: 0.6104\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4298 - accuracy: 0.7807 - val_loss: 0.9639 - val_accuracy: 0.6046\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4327 - accuracy: 0.7745 - val_loss: 0.9531 - val_accuracy: 0.5987\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4366 - accuracy: 0.7752 - val_loss: 0.9506 - val_accuracy: 0.5892\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4367 - accuracy: 0.7741 - val_loss: 0.9370 - val_accuracy: 0.6154\n",
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4336 - accuracy: 0.7823 - val_loss: 0.9574 - val_accuracy: 0.6004\n",
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4285 - accuracy: 0.7802 - val_loss: 0.9861 - val_accuracy: 0.6162\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4290 - accuracy: 0.7823 - val_loss: 0.9698 - val_accuracy: 0.6087\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4542 - accuracy: 0.7704 - val_loss: 0.9526 - val_accuracy: 0.5871\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4262 - accuracy: 0.7880 - val_loss: 0.9594 - val_accuracy: 0.5992\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4225 - accuracy: 0.7886 - val_loss: 0.9706 - val_accuracy: 0.5883\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4212 - accuracy: 0.7896 - val_loss: 0.9636 - val_accuracy: 0.6175\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4175 - accuracy: 0.7889 - val_loss: 0.9832 - val_accuracy: 0.5983\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4150 - accuracy: 0.7918 - val_loss: 0.9830 - val_accuracy: 0.6192\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.4177 - accuracy: 0.7887 - val_loss: 0.9598 - val_accuracy: 0.6050\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4156 - accuracy: 0.7877 - val_loss: 0.9742 - val_accuracy: 0.6037\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4142 - accuracy: 0.7959 - val_loss: 1.0062 - val_accuracy: 0.6104\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4090 - accuracy: 0.7957 - val_loss: 1.0141 - val_accuracy: 0.6112\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4079 - accuracy: 0.7979 - val_loss: 0.9935 - val_accuracy: 0.6012\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.4085 - accuracy: 0.7979 - val_loss: 1.0296 - val_accuracy: 0.6121\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.4088 - accuracy: 0.7970 - val_loss: 1.0069 - val_accuracy: 0.6108\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4030 - accuracy: 0.8021 - val_loss: 1.0040 - val_accuracy: 0.6008\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4022 - accuracy: 0.7995 - val_loss: 1.0312 - val_accuracy: 0.6012\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4008 - accuracy: 0.8005 - val_loss: 1.0322 - val_accuracy: 0.6079\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4035 - accuracy: 0.7948 - val_loss: 1.0128 - val_accuracy: 0.6104\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.4065 - accuracy: 0.7939 - val_loss: 1.0249 - val_accuracy: 0.5983\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3981 - accuracy: 0.8018 - val_loss: 1.0374 - val_accuracy: 0.6021\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3998 - accuracy: 0.8037 - val_loss: 1.0265 - val_accuracy: 0.5962\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.3970 - accuracy: 0.8052 - val_loss: 1.0607 - val_accuracy: 0.6137\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3978 - accuracy: 0.8007 - val_loss: 1.0626 - val_accuracy: 0.6129\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.4004 - accuracy: 0.7989 - val_loss: 1.0481 - val_accuracy: 0.5817\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3996 - accuracy: 0.7982 - val_loss: 1.0769 - val_accuracy: 0.5900\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.3974 - accuracy: 0.8039 - val_loss: 1.0523 - val_accuracy: 0.6067\n",
      "Epoch 216/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.4007 - accuracy: 0.8000 - val_loss: 1.0812 - val_accuracy: 0.6204\n",
      "Epoch 217/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3894 - accuracy: 0.8080 - val_loss: 1.0480 - val_accuracy: 0.6046\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3838 - accuracy: 0.8091 - val_loss: 1.0520 - val_accuracy: 0.5967\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3914 - accuracy: 0.8032 - val_loss: 1.0569 - val_accuracy: 0.5962\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3874 - accuracy: 0.8091 - val_loss: 1.1049 - val_accuracy: 0.6229\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3917 - accuracy: 0.8086 - val_loss: 1.0373 - val_accuracy: 0.5875\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3877 - accuracy: 0.8075 - val_loss: 1.0706 - val_accuracy: 0.6012\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3818 - accuracy: 0.8107 - val_loss: 1.0756 - val_accuracy: 0.5896\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3854 - accuracy: 0.8102 - val_loss: 1.0737 - val_accuracy: 0.6092\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3825 - accuracy: 0.8139 - val_loss: 1.0488 - val_accuracy: 0.5987\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3744 - accuracy: 0.8188 - val_loss: 1.0874 - val_accuracy: 0.6008\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3760 - accuracy: 0.8168 - val_loss: 1.1057 - val_accuracy: 0.6158\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3765 - accuracy: 0.8173 - val_loss: 1.0947 - val_accuracy: 0.6079\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3801 - accuracy: 0.8121 - val_loss: 1.0863 - val_accuracy: 0.5946\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3748 - accuracy: 0.8170 - val_loss: 1.0819 - val_accuracy: 0.5883\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3658 - accuracy: 0.8225 - val_loss: 1.0875 - val_accuracy: 0.6008\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3678 - accuracy: 0.8200 - val_loss: 1.0863 - val_accuracy: 0.5992\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3654 - accuracy: 0.8254 - val_loss: 1.1197 - val_accuracy: 0.5829\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3662 - accuracy: 0.8259 - val_loss: 1.0841 - val_accuracy: 0.6000\n",
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3671 - accuracy: 0.8255 - val_loss: 1.0885 - val_accuracy: 0.5792\n",
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3644 - accuracy: 0.8227 - val_loss: 1.1330 - val_accuracy: 0.6212\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3650 - accuracy: 0.8264 - val_loss: 1.1377 - val_accuracy: 0.6087\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3635 - accuracy: 0.8261 - val_loss: 1.1048 - val_accuracy: 0.5900\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3792 - accuracy: 0.8180 - val_loss: 1.1442 - val_accuracy: 0.5867\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3671 - accuracy: 0.8236 - val_loss: 1.1247 - val_accuracy: 0.5875\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3590 - accuracy: 0.8259 - val_loss: 1.1085 - val_accuracy: 0.6017\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3634 - accuracy: 0.8211 - val_loss: 1.1137 - val_accuracy: 0.5908\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3567 - accuracy: 0.8232 - val_loss: 1.1484 - val_accuracy: 0.5946\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3525 - accuracy: 0.8334 - val_loss: 1.1423 - val_accuracy: 0.5933\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3677 - accuracy: 0.8209 - val_loss: 1.1185 - val_accuracy: 0.5879\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3684 - accuracy: 0.8170 - val_loss: 1.1286 - val_accuracy: 0.5813\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3653 - accuracy: 0.8245 - val_loss: 1.1318 - val_accuracy: 0.5917\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3583 - accuracy: 0.8225 - val_loss: 1.1349 - val_accuracy: 0.5854\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3618 - accuracy: 0.8198 - val_loss: 1.1305 - val_accuracy: 0.6162\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.3567 - accuracy: 0.8293 - val_loss: 1.1627 - val_accuracy: 0.6000\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.3475 - accuracy: 0.8309 - val_loss: 1.1714 - val_accuracy: 0.6042\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3470 - accuracy: 0.8357 - val_loss: 1.1879 - val_accuracy: 0.5946\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3472 - accuracy: 0.8339 - val_loss: 1.1953 - val_accuracy: 0.6137\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3436 - accuracy: 0.8404 - val_loss: 1.1433 - val_accuracy: 0.5971\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3522 - accuracy: 0.8266 - val_loss: 1.1617 - val_accuracy: 0.5946\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3450 - accuracy: 0.8350 - val_loss: 1.1576 - val_accuracy: 0.5925\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3488 - accuracy: 0.8377 - val_loss: 1.1783 - val_accuracy: 0.5896\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3420 - accuracy: 0.8370 - val_loss: 1.1710 - val_accuracy: 0.5913\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.3364 - accuracy: 0.8348 - val_loss: 1.1858 - val_accuracy: 0.5846\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3403 - accuracy: 0.8384 - val_loss: 1.2281 - val_accuracy: 0.6092\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3436 - accuracy: 0.8346 - val_loss: 1.1929 - val_accuracy: 0.5942\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3387 - accuracy: 0.8391 - val_loss: 1.1952 - val_accuracy: 0.5908\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3375 - accuracy: 0.8373 - val_loss: 1.2107 - val_accuracy: 0.6012\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3409 - accuracy: 0.8389 - val_loss: 1.1684 - val_accuracy: 0.5946\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3386 - accuracy: 0.8384 - val_loss: 1.2315 - val_accuracy: 0.6079\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3368 - accuracy: 0.8420 - val_loss: 1.2175 - val_accuracy: 0.6021\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3376 - accuracy: 0.8321 - val_loss: 1.2025 - val_accuracy: 0.5946\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3321 - accuracy: 0.8375 - val_loss: 1.1952 - val_accuracy: 0.5825\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3251 - accuracy: 0.8486 - val_loss: 1.2227 - val_accuracy: 0.5933\n",
      "Epoch 270/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3311 - accuracy: 0.8413 - val_loss: 1.2221 - val_accuracy: 0.5950\n",
      "Epoch 271/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3260 - accuracy: 0.8445 - val_loss: 1.2499 - val_accuracy: 0.6133\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3310 - accuracy: 0.8402 - val_loss: 1.2221 - val_accuracy: 0.5925\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3218 - accuracy: 0.8516 - val_loss: 1.2725 - val_accuracy: 0.6108\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3307 - accuracy: 0.8411 - val_loss: 1.2344 - val_accuracy: 0.5983\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3166 - accuracy: 0.8468 - val_loss: 1.2395 - val_accuracy: 0.5971\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3223 - accuracy: 0.8461 - val_loss: 1.2725 - val_accuracy: 0.5917\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3257 - accuracy: 0.8473 - val_loss: 1.2307 - val_accuracy: 0.6029\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3286 - accuracy: 0.8455 - val_loss: 1.2471 - val_accuracy: 0.6012\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3279 - accuracy: 0.8438 - val_loss: 1.2694 - val_accuracy: 0.5871\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3163 - accuracy: 0.8468 - val_loss: 1.2519 - val_accuracy: 0.6025\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3174 - accuracy: 0.8473 - val_loss: 1.2989 - val_accuracy: 0.6058\n",
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3228 - accuracy: 0.8427 - val_loss: 1.3169 - val_accuracy: 0.6192\n",
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3241 - accuracy: 0.8489 - val_loss: 1.2626 - val_accuracy: 0.6037\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3199 - accuracy: 0.8493 - val_loss: 1.2765 - val_accuracy: 0.6021\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3206 - accuracy: 0.8470 - val_loss: 1.2805 - val_accuracy: 0.6004\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3366 - accuracy: 0.8421 - val_loss: 1.2653 - val_accuracy: 0.6125\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3460 - accuracy: 0.8325 - val_loss: 1.2758 - val_accuracy: 0.5892\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3293 - accuracy: 0.8445 - val_loss: 1.2807 - val_accuracy: 0.5921\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3144 - accuracy: 0.8545 - val_loss: 1.2706 - val_accuracy: 0.5925\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3091 - accuracy: 0.8571 - val_loss: 1.2995 - val_accuracy: 0.6112\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3179 - accuracy: 0.8512 - val_loss: 1.2914 - val_accuracy: 0.6092\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3085 - accuracy: 0.8586 - val_loss: 1.2881 - val_accuracy: 0.5925\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3094 - accuracy: 0.8550 - val_loss: 1.3041 - val_accuracy: 0.5929\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3052 - accuracy: 0.8573 - val_loss: 1.2605 - val_accuracy: 0.6121\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2996 - accuracy: 0.8645 - val_loss: 1.2855 - val_accuracy: 0.5908\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3024 - accuracy: 0.8607 - val_loss: 1.2881 - val_accuracy: 0.5904\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3016 - accuracy: 0.8609 - val_loss: 1.2886 - val_accuracy: 0.6000\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3051 - accuracy: 0.8607 - val_loss: 1.2984 - val_accuracy: 0.5850\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2988 - accuracy: 0.8607 - val_loss: 1.3381 - val_accuracy: 0.5917\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3055 - accuracy: 0.8573 - val_loss: 1.3526 - val_accuracy: 0.6075\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3052 - accuracy: 0.8552 - val_loss: 1.2981 - val_accuracy: 0.6017\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.3001 - accuracy: 0.8627 - val_loss: 1.3431 - val_accuracy: 0.6058\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.3002 - accuracy: 0.8621 - val_loss: 1.3245 - val_accuracy: 0.6029\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2975 - accuracy: 0.8618 - val_loss: 1.3186 - val_accuracy: 0.5917\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3073 - accuracy: 0.8611 - val_loss: 1.3526 - val_accuracy: 0.5850\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2949 - accuracy: 0.8645 - val_loss: 1.3294 - val_accuracy: 0.6008\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.2955 - accuracy: 0.8668 - val_loss: 1.3418 - val_accuracy: 0.6054\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2979 - accuracy: 0.8639 - val_loss: 1.3794 - val_accuracy: 0.6012\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.3040 - accuracy: 0.8582 - val_loss: 1.3166 - val_accuracy: 0.6050\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.3026 - accuracy: 0.8554 - val_loss: 1.3320 - val_accuracy: 0.5925\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2915 - accuracy: 0.8668 - val_loss: 1.3519 - val_accuracy: 0.6012\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2918 - accuracy: 0.8675 - val_loss: 1.3433 - val_accuracy: 0.5825\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2842 - accuracy: 0.8705 - val_loss: 1.3767 - val_accuracy: 0.5946\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2887 - accuracy: 0.8652 - val_loss: 1.3547 - val_accuracy: 0.5992\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2862 - accuracy: 0.8673 - val_loss: 1.3922 - val_accuracy: 0.5950\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2921 - accuracy: 0.8639 - val_loss: 1.3370 - val_accuracy: 0.6017\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2836 - accuracy: 0.8716 - val_loss: 1.3785 - val_accuracy: 0.5983\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2795 - accuracy: 0.8759 - val_loss: 1.4019 - val_accuracy: 0.6042\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2879 - accuracy: 0.8684 - val_loss: 1.3924 - val_accuracy: 0.6104\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2866 - accuracy: 0.8691 - val_loss: 1.4110 - val_accuracy: 0.6021\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2846 - accuracy: 0.8691 - val_loss: 1.3937 - val_accuracy: 0.6037\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2799 - accuracy: 0.8721 - val_loss: 1.4065 - val_accuracy: 0.6075\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2864 - accuracy: 0.8654 - val_loss: 1.4249 - val_accuracy: 0.5929\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2969 - accuracy: 0.8682 - val_loss: 1.3997 - val_accuracy: 0.5975\n",
      "Epoch 325/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2980 - accuracy: 0.8661 - val_loss: 1.3621 - val_accuracy: 0.6004\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2977 - accuracy: 0.8673 - val_loss: 1.4366 - val_accuracy: 0.5917\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2971 - accuracy: 0.8623 - val_loss: 1.4126 - val_accuracy: 0.5958\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2815 - accuracy: 0.8716 - val_loss: 1.3848 - val_accuracy: 0.5996\n",
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2781 - accuracy: 0.8750 - val_loss: 1.3796 - val_accuracy: 0.5925\n",
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2727 - accuracy: 0.8795 - val_loss: 1.3948 - val_accuracy: 0.5996\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2768 - accuracy: 0.8729 - val_loss: 1.4137 - val_accuracy: 0.5842\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2706 - accuracy: 0.8782 - val_loss: 1.4287 - val_accuracy: 0.6037\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2682 - accuracy: 0.8782 - val_loss: 1.4542 - val_accuracy: 0.6054\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2720 - accuracy: 0.8796 - val_loss: 1.3933 - val_accuracy: 0.5983\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2715 - accuracy: 0.8745 - val_loss: 1.4167 - val_accuracy: 0.6096\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2648 - accuracy: 0.8802 - val_loss: 1.4374 - val_accuracy: 0.6017\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2751 - accuracy: 0.8743 - val_loss: 1.4073 - val_accuracy: 0.6037\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2768 - accuracy: 0.8755 - val_loss: 1.4270 - val_accuracy: 0.5925\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2748 - accuracy: 0.8746 - val_loss: 1.4315 - val_accuracy: 0.5958\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2726 - accuracy: 0.8788 - val_loss: 1.4601 - val_accuracy: 0.5838\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2884 - accuracy: 0.8636 - val_loss: 1.4450 - val_accuracy: 0.6071\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2700 - accuracy: 0.8754 - val_loss: 1.4596 - val_accuracy: 0.5875\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2702 - accuracy: 0.8795 - val_loss: 1.4603 - val_accuracy: 0.6054\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2746 - accuracy: 0.8748 - val_loss: 1.4629 - val_accuracy: 0.6042\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2690 - accuracy: 0.8830 - val_loss: 1.4751 - val_accuracy: 0.6079\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2642 - accuracy: 0.8852 - val_loss: 1.4657 - val_accuracy: 0.6154\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2658 - accuracy: 0.8836 - val_loss: 1.4701 - val_accuracy: 0.6025\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2627 - accuracy: 0.8813 - val_loss: 1.4465 - val_accuracy: 0.5996\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2663 - accuracy: 0.8784 - val_loss: 1.4813 - val_accuracy: 0.5908\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2631 - accuracy: 0.8802 - val_loss: 1.4808 - val_accuracy: 0.6075\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2563 - accuracy: 0.8839 - val_loss: 1.4944 - val_accuracy: 0.5950\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2638 - accuracy: 0.8784 - val_loss: 1.4712 - val_accuracy: 0.6008\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2558 - accuracy: 0.8852 - val_loss: 1.4891 - val_accuracy: 0.6042\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2572 - accuracy: 0.8829 - val_loss: 1.5091 - val_accuracy: 0.6037\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2538 - accuracy: 0.8868 - val_loss: 1.5287 - val_accuracy: 0.6050\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2522 - accuracy: 0.8909 - val_loss: 1.4859 - val_accuracy: 0.5904\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2575 - accuracy: 0.8782 - val_loss: 1.4704 - val_accuracy: 0.5908\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2587 - accuracy: 0.8859 - val_loss: 1.4685 - val_accuracy: 0.6025\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2503 - accuracy: 0.8855 - val_loss: 1.4824 - val_accuracy: 0.5987\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2481 - accuracy: 0.8902 - val_loss: 1.5105 - val_accuracy: 0.5888\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2522 - accuracy: 0.8866 - val_loss: 1.5105 - val_accuracy: 0.5992\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2488 - accuracy: 0.8923 - val_loss: 1.5216 - val_accuracy: 0.5983\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2571 - accuracy: 0.8845 - val_loss: 1.5431 - val_accuracy: 0.5908\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.2586 - accuracy: 0.8859 - val_loss: 1.5242 - val_accuracy: 0.6012\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2553 - accuracy: 0.8852 - val_loss: 1.5359 - val_accuracy: 0.6029\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2594 - accuracy: 0.8850 - val_loss: 1.5538 - val_accuracy: 0.6104\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2678 - accuracy: 0.8809 - val_loss: 1.5161 - val_accuracy: 0.5938\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2540 - accuracy: 0.8845 - val_loss: 1.5082 - val_accuracy: 0.6000\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2453 - accuracy: 0.8889 - val_loss: 1.5769 - val_accuracy: 0.6000\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2474 - accuracy: 0.8914 - val_loss: 1.5848 - val_accuracy: 0.6008\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2421 - accuracy: 0.8889 - val_loss: 1.5527 - val_accuracy: 0.5896\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2426 - accuracy: 0.8934 - val_loss: 1.5710 - val_accuracy: 0.6042\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2435 - accuracy: 0.8934 - val_loss: 1.5624 - val_accuracy: 0.5992\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2409 - accuracy: 0.8938 - val_loss: 1.5611 - val_accuracy: 0.6029\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2404 - accuracy: 0.8934 - val_loss: 1.5507 - val_accuracy: 0.5929\n",
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2509 - accuracy: 0.8886 - val_loss: 1.5971 - val_accuracy: 0.5992\n",
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2488 - accuracy: 0.8898 - val_loss: 1.6221 - val_accuracy: 0.5992\n",
      "Epoch 378/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2459 - accuracy: 0.8943 - val_loss: 1.5752 - val_accuracy: 0.5971\n",
      "Epoch 379/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2605 - accuracy: 0.8913 - val_loss: 1.5995 - val_accuracy: 0.6017\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2671 - accuracy: 0.8775 - val_loss: 1.5472 - val_accuracy: 0.5875\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2704 - accuracy: 0.8802 - val_loss: 1.5842 - val_accuracy: 0.5854\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2624 - accuracy: 0.8841 - val_loss: 1.5542 - val_accuracy: 0.6021\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2398 - accuracy: 0.8893 - val_loss: 1.6563 - val_accuracy: 0.6008\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2497 - accuracy: 0.8889 - val_loss: 1.5862 - val_accuracy: 0.5992\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2501 - accuracy: 0.8895 - val_loss: 1.6215 - val_accuracy: 0.5721\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2464 - accuracy: 0.8907 - val_loss: 1.5841 - val_accuracy: 0.5979\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2410 - accuracy: 0.8907 - val_loss: 1.5873 - val_accuracy: 0.6037\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2331 - accuracy: 0.8957 - val_loss: 1.6072 - val_accuracy: 0.6012\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2329 - accuracy: 0.8977 - val_loss: 1.6372 - val_accuracy: 0.6108\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2318 - accuracy: 0.9020 - val_loss: 1.5990 - val_accuracy: 0.5979\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2323 - accuracy: 0.8945 - val_loss: 1.6163 - val_accuracy: 0.6042\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2282 - accuracy: 0.9014 - val_loss: 1.6211 - val_accuracy: 0.6008\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2380 - accuracy: 0.8929 - val_loss: 1.6231 - val_accuracy: 0.5938\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2318 - accuracy: 0.8993 - val_loss: 1.6533 - val_accuracy: 0.6087\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2321 - accuracy: 0.8964 - val_loss: 1.6402 - val_accuracy: 0.6087\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2230 - accuracy: 0.9002 - val_loss: 1.6309 - val_accuracy: 0.6042\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2266 - accuracy: 0.9043 - val_loss: 1.6166 - val_accuracy: 0.5975\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2324 - accuracy: 0.8959 - val_loss: 1.6139 - val_accuracy: 0.6083\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2314 - accuracy: 0.8982 - val_loss: 1.6610 - val_accuracy: 0.5825\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2303 - accuracy: 0.8986 - val_loss: 1.6369 - val_accuracy: 0.5975\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2277 - accuracy: 0.9030 - val_loss: 1.6490 - val_accuracy: 0.5929\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2245 - accuracy: 0.9054 - val_loss: 1.6363 - val_accuracy: 0.5892\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2267 - accuracy: 0.9052 - val_loss: 1.6439 - val_accuracy: 0.6075\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.2220 - accuracy: 0.9038 - val_loss: 1.6861 - val_accuracy: 0.6079\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2247 - accuracy: 0.9034 - val_loss: 1.6615 - val_accuracy: 0.5954\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2248 - accuracy: 0.8982 - val_loss: 1.6923 - val_accuracy: 0.5854\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2285 - accuracy: 0.9009 - val_loss: 1.6829 - val_accuracy: 0.5800\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2272 - accuracy: 0.9041 - val_loss: 1.6751 - val_accuracy: 0.5942\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2245 - accuracy: 0.9066 - val_loss: 1.6408 - val_accuracy: 0.6029\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2221 - accuracy: 0.9038 - val_loss: 1.6686 - val_accuracy: 0.6017\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2269 - accuracy: 0.9045 - val_loss: 1.6528 - val_accuracy: 0.5946\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2411 - accuracy: 0.8993 - val_loss: 1.6711 - val_accuracy: 0.5962\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2466 - accuracy: 0.8886 - val_loss: 1.6928 - val_accuracy: 0.5929\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2682 - accuracy: 0.8800 - val_loss: 1.7200 - val_accuracy: 0.5854\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2425 - accuracy: 0.8961 - val_loss: 1.6975 - val_accuracy: 0.6025\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.2345 - accuracy: 0.8968 - val_loss: 1.6734 - val_accuracy: 0.6042\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.2230 - accuracy: 0.9061 - val_loss: 1.6510 - val_accuracy: 0.6058\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.2174 - accuracy: 0.9093 - val_loss: 1.7074 - val_accuracy: 0.5900\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.2137 - accuracy: 0.9086 - val_loss: 1.6554 - val_accuracy: 0.5975\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.2109 - accuracy: 0.9100 - val_loss: 1.6555 - val_accuracy: 0.5871\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2121 - accuracy: 0.9107 - val_loss: 1.6755 - val_accuracy: 0.5913\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2064 - accuracy: 0.9148 - val_loss: 1.6688 - val_accuracy: 0.5900\n",
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2113 - accuracy: 0.9146 - val_loss: 1.6960 - val_accuracy: 0.5883\n",
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2155 - accuracy: 0.9082 - val_loss: 1.7281 - val_accuracy: 0.5992\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2095 - accuracy: 0.9130 - val_loss: 1.7099 - val_accuracy: 0.5821\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2129 - accuracy: 0.9084 - val_loss: 1.7359 - val_accuracy: 0.5967\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2089 - accuracy: 0.9100 - val_loss: 1.7168 - val_accuracy: 0.5992\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2121 - accuracy: 0.9082 - val_loss: 1.7059 - val_accuracy: 0.5950\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2096 - accuracy: 0.9107 - val_loss: 1.6825 - val_accuracy: 0.5904\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2092 - accuracy: 0.9120 - val_loss: 1.7250 - val_accuracy: 0.6062\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2094 - accuracy: 0.9095 - val_loss: 1.7216 - val_accuracy: 0.5921\n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2020 - accuracy: 0.9179 - val_loss: 1.7168 - val_accuracy: 0.6017\n",
      "Epoch 433/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2060 - accuracy: 0.9086 - val_loss: 1.7330 - val_accuracy: 0.5896\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2080 - accuracy: 0.9107 - val_loss: 1.7497 - val_accuracy: 0.6046\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2071 - accuracy: 0.9155 - val_loss: 1.7525 - val_accuracy: 0.6108\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2126 - accuracy: 0.9054 - val_loss: 1.7771 - val_accuracy: 0.6037\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2129 - accuracy: 0.9075 - val_loss: 1.7081 - val_accuracy: 0.5925\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2044 - accuracy: 0.9150 - val_loss: 1.7218 - val_accuracy: 0.5925\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2001 - accuracy: 0.9198 - val_loss: 1.7614 - val_accuracy: 0.5938\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1996 - accuracy: 0.9155 - val_loss: 1.7731 - val_accuracy: 0.5908\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2031 - accuracy: 0.9171 - val_loss: 1.7307 - val_accuracy: 0.5917\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.2114 - accuracy: 0.9105 - val_loss: 1.7575 - val_accuracy: 0.5962\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1994 - accuracy: 0.9170 - val_loss: 1.7324 - val_accuracy: 0.5825\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2005 - accuracy: 0.9164 - val_loss: 1.7565 - val_accuracy: 0.5962\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2040 - accuracy: 0.9109 - val_loss: 1.7904 - val_accuracy: 0.5817\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2064 - accuracy: 0.9143 - val_loss: 1.7486 - val_accuracy: 0.5888\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2057 - accuracy: 0.9148 - val_loss: 1.7638 - val_accuracy: 0.5992\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2036 - accuracy: 0.9159 - val_loss: 1.7602 - val_accuracy: 0.5721\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2087 - accuracy: 0.9136 - val_loss: 1.7562 - val_accuracy: 0.5917\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1988 - accuracy: 0.9173 - val_loss: 1.7788 - val_accuracy: 0.5917\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1985 - accuracy: 0.9189 - val_loss: 1.7779 - val_accuracy: 0.5892\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2154 - accuracy: 0.9112 - val_loss: 1.8244 - val_accuracy: 0.5975\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2474 - accuracy: 0.8950 - val_loss: 1.8365 - val_accuracy: 0.6075\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2619 - accuracy: 0.8868 - val_loss: 1.8133 - val_accuracy: 0.5996\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2216 - accuracy: 0.9082 - val_loss: 1.8165 - val_accuracy: 0.6004\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2076 - accuracy: 0.9114 - val_loss: 1.7611 - val_accuracy: 0.5929\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2009 - accuracy: 0.9112 - val_loss: 1.7988 - val_accuracy: 0.5892\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1960 - accuracy: 0.9157 - val_loss: 1.8088 - val_accuracy: 0.6000\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1888 - accuracy: 0.9214 - val_loss: 1.7930 - val_accuracy: 0.5929\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1998 - accuracy: 0.9184 - val_loss: 1.8115 - val_accuracy: 0.6017\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2033 - accuracy: 0.9159 - val_loss: 1.8152 - val_accuracy: 0.5938\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1899 - accuracy: 0.9196 - val_loss: 1.8179 - val_accuracy: 0.5967\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1886 - accuracy: 0.9248 - val_loss: 1.8280 - val_accuracy: 0.6025\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1842 - accuracy: 0.9264 - val_loss: 1.8345 - val_accuracy: 0.5875\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1887 - accuracy: 0.9261 - val_loss: 1.8132 - val_accuracy: 0.5950\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1788 - accuracy: 0.9266 - val_loss: 1.8510 - val_accuracy: 0.5904\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1832 - accuracy: 0.9261 - val_loss: 1.8352 - val_accuracy: 0.6046\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1854 - accuracy: 0.9264 - val_loss: 1.8347 - val_accuracy: 0.5925\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1864 - accuracy: 0.9236 - val_loss: 1.8250 - val_accuracy: 0.5933\n",
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1819 - accuracy: 0.9291 - val_loss: 1.8615 - val_accuracy: 0.5900\n",
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1874 - accuracy: 0.9239 - val_loss: 1.8523 - val_accuracy: 0.5962\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1894 - accuracy: 0.9187 - val_loss: 1.8435 - val_accuracy: 0.5925\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1854 - accuracy: 0.9245 - val_loss: 1.8585 - val_accuracy: 0.6000\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1828 - accuracy: 0.9286 - val_loss: 1.8397 - val_accuracy: 0.5971\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1808 - accuracy: 0.9271 - val_loss: 1.8682 - val_accuracy: 0.5996\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1803 - accuracy: 0.9270 - val_loss: 1.9014 - val_accuracy: 0.5967\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1928 - accuracy: 0.9236 - val_loss: 1.8850 - val_accuracy: 0.5917\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2028 - accuracy: 0.9170 - val_loss: 1.9363 - val_accuracy: 0.6050\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2221 - accuracy: 0.9080 - val_loss: 1.8194 - val_accuracy: 0.5962\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2142 - accuracy: 0.9123 - val_loss: 1.9275 - val_accuracy: 0.5971\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2202 - accuracy: 0.9089 - val_loss: 1.9037 - val_accuracy: 0.5938\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2104 - accuracy: 0.9139 - val_loss: 1.8349 - val_accuracy: 0.5817\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.2081 - accuracy: 0.9220 - val_loss: 1.8681 - val_accuracy: 0.5767\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2127 - accuracy: 0.9136 - val_loss: 1.8378 - val_accuracy: 0.5908\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.2133 - accuracy: 0.9139 - val_loss: 1.8511 - val_accuracy: 0.6029\n",
      "Epoch 486/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1961 - accuracy: 0.9212 - val_loss: 1.9004 - val_accuracy: 0.5954\n",
      "Epoch 487/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1909 - accuracy: 0.9202 - val_loss: 1.8783 - val_accuracy: 0.5996\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1875 - accuracy: 0.9221 - val_loss: 1.8900 - val_accuracy: 0.5871\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1849 - accuracy: 0.9230 - val_loss: 1.8602 - val_accuracy: 0.5879\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1798 - accuracy: 0.9246 - val_loss: 1.8699 - val_accuracy: 0.5917\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1741 - accuracy: 0.9314 - val_loss: 1.9322 - val_accuracy: 0.6100\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1771 - accuracy: 0.9300 - val_loss: 1.9147 - val_accuracy: 0.6129\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1769 - accuracy: 0.9293 - val_loss: 1.8872 - val_accuracy: 0.5933\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1726 - accuracy: 0.9300 - val_loss: 1.8857 - val_accuracy: 0.6029\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1716 - accuracy: 0.9323 - val_loss: 1.8819 - val_accuracy: 0.5842\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1760 - accuracy: 0.9325 - val_loss: 1.9022 - val_accuracy: 0.5942\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1705 - accuracy: 0.9343 - val_loss: 1.8838 - val_accuracy: 0.5933\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1700 - accuracy: 0.9309 - val_loss: 1.8711 - val_accuracy: 0.5975\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1717 - accuracy: 0.9293 - val_loss: 1.9360 - val_accuracy: 0.5983\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1695 - accuracy: 0.9343 - val_loss: 1.9312 - val_accuracy: 0.5975\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1725 - accuracy: 0.9325 - val_loss: 1.8905 - val_accuracy: 0.5896\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1720 - accuracy: 0.9316 - val_loss: 1.9066 - val_accuracy: 0.5979\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1707 - accuracy: 0.9279 - val_loss: 1.9136 - val_accuracy: 0.5883\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1754 - accuracy: 0.9286 - val_loss: 1.9261 - val_accuracy: 0.5925\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1686 - accuracy: 0.9345 - val_loss: 1.9233 - val_accuracy: 0.6000\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1638 - accuracy: 0.9354 - val_loss: 1.9119 - val_accuracy: 0.5942\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1705 - accuracy: 0.9316 - val_loss: 1.9356 - val_accuracy: 0.5879\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1641 - accuracy: 0.9339 - val_loss: 1.9326 - val_accuracy: 0.5821\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1675 - accuracy: 0.9336 - val_loss: 1.9214 - val_accuracy: 0.5913\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1714 - accuracy: 0.9318 - val_loss: 1.9794 - val_accuracy: 0.6004\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1637 - accuracy: 0.9350 - val_loss: 2.0052 - val_accuracy: 0.5996\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1705 - accuracy: 0.9325 - val_loss: 1.9428 - val_accuracy: 0.5892\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1711 - accuracy: 0.9323 - val_loss: 1.9926 - val_accuracy: 0.6079\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1795 - accuracy: 0.9234 - val_loss: 1.9809 - val_accuracy: 0.5942\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1760 - accuracy: 0.9323 - val_loss: 1.9865 - val_accuracy: 0.5846\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1857 - accuracy: 0.9230 - val_loss: 1.9721 - val_accuracy: 0.6054\n",
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1731 - accuracy: 0.9312 - val_loss: 1.9786 - val_accuracy: 0.6012\n",
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1781 - accuracy: 0.9312 - val_loss: 1.9800 - val_accuracy: 0.6008\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1714 - accuracy: 0.9305 - val_loss: 2.0128 - val_accuracy: 0.6004\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1689 - accuracy: 0.9321 - val_loss: 2.0025 - val_accuracy: 0.5867\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1667 - accuracy: 0.9330 - val_loss: 2.0054 - val_accuracy: 0.5971\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1620 - accuracy: 0.9350 - val_loss: 1.9923 - val_accuracy: 0.5825\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1631 - accuracy: 0.9359 - val_loss: 1.9699 - val_accuracy: 0.5933\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1599 - accuracy: 0.9348 - val_loss: 1.9709 - val_accuracy: 0.5896\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1610 - accuracy: 0.9361 - val_loss: 1.9986 - val_accuracy: 0.5804\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1595 - accuracy: 0.9377 - val_loss: 1.9803 - val_accuracy: 0.5983\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1647 - accuracy: 0.9332 - val_loss: 2.0003 - val_accuracy: 0.5788\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1681 - accuracy: 0.9337 - val_loss: 2.0471 - val_accuracy: 0.6008\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1603 - accuracy: 0.9345 - val_loss: 2.0538 - val_accuracy: 0.5813\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1649 - accuracy: 0.9364 - val_loss: 2.0460 - val_accuracy: 0.5896\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1716 - accuracy: 0.9321 - val_loss: 1.9526 - val_accuracy: 0.5871\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1576 - accuracy: 0.9386 - val_loss: 1.9700 - val_accuracy: 0.5871\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1676 - accuracy: 0.9355 - val_loss: 2.0712 - val_accuracy: 0.5879\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1753 - accuracy: 0.9314 - val_loss: 2.0039 - val_accuracy: 0.5867\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1971 - accuracy: 0.9204 - val_loss: 2.0302 - val_accuracy: 0.5967\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1713 - accuracy: 0.9337 - val_loss: 2.0288 - val_accuracy: 0.5871\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1825 - accuracy: 0.9279 - val_loss: 2.0782 - val_accuracy: 0.5908\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1893 - accuracy: 0.9280 - val_loss: 2.0255 - val_accuracy: 0.5896\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1941 - accuracy: 0.9180 - val_loss: 2.0512 - val_accuracy: 0.5871\n",
      "Epoch 540/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1919 - accuracy: 0.9196 - val_loss: 2.0205 - val_accuracy: 0.5858\n",
      "Epoch 541/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1661 - accuracy: 0.9336 - val_loss: 2.0570 - val_accuracy: 0.5946\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1576 - accuracy: 0.9423 - val_loss: 2.0440 - val_accuracy: 0.5933\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1679 - accuracy: 0.9359 - val_loss: 2.0463 - val_accuracy: 0.5983\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1579 - accuracy: 0.9438 - val_loss: 2.0495 - val_accuracy: 0.5954\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1608 - accuracy: 0.9391 - val_loss: 2.0091 - val_accuracy: 0.5854\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1517 - accuracy: 0.9448 - val_loss: 2.0793 - val_accuracy: 0.6004\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1539 - accuracy: 0.9430 - val_loss: 2.0996 - val_accuracy: 0.5942\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1530 - accuracy: 0.9446 - val_loss: 2.0811 - val_accuracy: 0.5875\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1493 - accuracy: 0.9421 - val_loss: 2.0672 - val_accuracy: 0.5879\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1547 - accuracy: 0.9362 - val_loss: 2.0945 - val_accuracy: 0.5996\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1520 - accuracy: 0.9421 - val_loss: 2.0709 - val_accuracy: 0.5967\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1495 - accuracy: 0.9452 - val_loss: 2.0665 - val_accuracy: 0.5829\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1477 - accuracy: 0.9439 - val_loss: 2.0723 - val_accuracy: 0.5829\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1507 - accuracy: 0.9405 - val_loss: 2.0660 - val_accuracy: 0.5975\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1505 - accuracy: 0.9430 - val_loss: 2.1022 - val_accuracy: 0.5871\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1466 - accuracy: 0.9446 - val_loss: 2.1067 - val_accuracy: 0.5954\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1453 - accuracy: 0.9452 - val_loss: 2.0853 - val_accuracy: 0.5954\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1480 - accuracy: 0.9441 - val_loss: 2.0720 - val_accuracy: 0.5883\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1429 - accuracy: 0.9463 - val_loss: 2.1351 - val_accuracy: 0.5992\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1483 - accuracy: 0.9432 - val_loss: 2.1350 - val_accuracy: 0.6104\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1513 - accuracy: 0.9375 - val_loss: 2.1197 - val_accuracy: 0.5858\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1458 - accuracy: 0.9430 - val_loss: 2.1228 - val_accuracy: 0.5917\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1495 - accuracy: 0.9404 - val_loss: 2.0739 - val_accuracy: 0.5942\n",
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1431 - accuracy: 0.9464 - val_loss: 2.0932 - val_accuracy: 0.5892\n",
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1518 - accuracy: 0.9395 - val_loss: 2.1737 - val_accuracy: 0.6017\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1609 - accuracy: 0.9386 - val_loss: 2.1331 - val_accuracy: 0.5917\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1517 - accuracy: 0.9420 - val_loss: 2.1431 - val_accuracy: 0.5913\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1429 - accuracy: 0.9445 - val_loss: 2.1093 - val_accuracy: 0.5775\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1466 - accuracy: 0.9420 - val_loss: 2.1313 - val_accuracy: 0.5842\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1447 - accuracy: 0.9450 - val_loss: 2.1423 - val_accuracy: 0.5854\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1401 - accuracy: 0.9486 - val_loss: 2.1739 - val_accuracy: 0.5946\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1433 - accuracy: 0.9448 - val_loss: 2.1497 - val_accuracy: 0.5888\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1460 - accuracy: 0.9446 - val_loss: 2.1633 - val_accuracy: 0.5900\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1452 - accuracy: 0.9423 - val_loss: 2.1273 - val_accuracy: 0.5875\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1436 - accuracy: 0.9439 - val_loss: 2.1476 - val_accuracy: 0.5838\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1432 - accuracy: 0.9418 - val_loss: 2.1239 - val_accuracy: 0.5933\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1405 - accuracy: 0.9482 - val_loss: 2.1232 - val_accuracy: 0.5892\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1413 - accuracy: 0.9461 - val_loss: 2.1617 - val_accuracy: 0.5871\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1379 - accuracy: 0.9468 - val_loss: 2.1669 - val_accuracy: 0.5950\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1331 - accuracy: 0.9525 - val_loss: 2.1356 - val_accuracy: 0.5867\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1386 - accuracy: 0.9471 - val_loss: 2.1888 - val_accuracy: 0.5983\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1455 - accuracy: 0.9445 - val_loss: 2.1312 - val_accuracy: 0.5833\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1598 - accuracy: 0.9362 - val_loss: 2.1809 - val_accuracy: 0.5888\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2160 - accuracy: 0.9187 - val_loss: 2.1779 - val_accuracy: 0.5721\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1955 - accuracy: 0.9171 - val_loss: 2.1671 - val_accuracy: 0.5879\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1525 - accuracy: 0.9414 - val_loss: 2.1174 - val_accuracy: 0.5896\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1457 - accuracy: 0.9418 - val_loss: 2.1620 - val_accuracy: 0.5925\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1391 - accuracy: 0.9491 - val_loss: 2.1750 - val_accuracy: 0.5938\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1319 - accuracy: 0.9525 - val_loss: 2.1627 - val_accuracy: 0.5950\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1395 - accuracy: 0.9450 - val_loss: 2.1025 - val_accuracy: 0.5821\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1441 - accuracy: 0.9423 - val_loss: 2.2022 - val_accuracy: 0.6000\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1298 - accuracy: 0.9518 - val_loss: 2.1813 - val_accuracy: 0.5867\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1289 - accuracy: 0.9541 - val_loss: 2.1941 - val_accuracy: 0.5983\n",
      "Epoch 594/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1285 - accuracy: 0.9541 - val_loss: 2.1851 - val_accuracy: 0.6004\n",
      "Epoch 595/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1415 - accuracy: 0.9450 - val_loss: 2.1685 - val_accuracy: 0.5883\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1359 - accuracy: 0.9466 - val_loss: 2.2002 - val_accuracy: 0.5971\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1274 - accuracy: 0.9550 - val_loss: 2.1687 - val_accuracy: 0.5896\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1274 - accuracy: 0.9543 - val_loss: 2.2175 - val_accuracy: 0.5975\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1383 - accuracy: 0.9463 - val_loss: 2.2065 - val_accuracy: 0.5962\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1249 - accuracy: 0.9564 - val_loss: 2.2403 - val_accuracy: 0.5950\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1257 - accuracy: 0.9539 - val_loss: 2.2836 - val_accuracy: 0.5938\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1402 - accuracy: 0.9463 - val_loss: 2.1738 - val_accuracy: 0.5800\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1794 - accuracy: 0.9311 - val_loss: 2.1543 - val_accuracy: 0.5933\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.2021 - accuracy: 0.9179 - val_loss: 2.1609 - val_accuracy: 0.5842\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1683 - accuracy: 0.9330 - val_loss: 2.2436 - val_accuracy: 0.5808\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1439 - accuracy: 0.9446 - val_loss: 2.2117 - val_accuracy: 0.5896\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1360 - accuracy: 0.9493 - val_loss: 2.2922 - val_accuracy: 0.5975\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1389 - accuracy: 0.9496 - val_loss: 2.2156 - val_accuracy: 0.5933\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1351 - accuracy: 0.9507 - val_loss: 2.2667 - val_accuracy: 0.6050\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1341 - accuracy: 0.9495 - val_loss: 2.2270 - val_accuracy: 0.5921\n",
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1218 - accuracy: 0.9571 - val_loss: 2.2493 - val_accuracy: 0.6017\n",
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1238 - accuracy: 0.9552 - val_loss: 2.2649 - val_accuracy: 0.5888\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1334 - accuracy: 0.9530 - val_loss: 2.2509 - val_accuracy: 0.5846\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1440 - accuracy: 0.9455 - val_loss: 2.2561 - val_accuracy: 0.5992\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1346 - accuracy: 0.9495 - val_loss: 2.3140 - val_accuracy: 0.5758\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1417 - accuracy: 0.9471 - val_loss: 2.2539 - val_accuracy: 0.5783\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1379 - accuracy: 0.9471 - val_loss: 2.2751 - val_accuracy: 0.5983\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1266 - accuracy: 0.9543 - val_loss: 2.2528 - val_accuracy: 0.5908\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1199 - accuracy: 0.9579 - val_loss: 2.2396 - val_accuracy: 0.5858\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1231 - accuracy: 0.9566 - val_loss: 2.2795 - val_accuracy: 0.5996\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1284 - accuracy: 0.9568 - val_loss: 2.2629 - val_accuracy: 0.5879\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1269 - accuracy: 0.9513 - val_loss: 2.2701 - val_accuracy: 0.5929\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1221 - accuracy: 0.9555 - val_loss: 2.2743 - val_accuracy: 0.5913\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1128 - accuracy: 0.9613 - val_loss: 2.3098 - val_accuracy: 0.5846\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1195 - accuracy: 0.9577 - val_loss: 2.2902 - val_accuracy: 0.5892\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1194 - accuracy: 0.9573 - val_loss: 2.2956 - val_accuracy: 0.5854\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1375 - accuracy: 0.9448 - val_loss: 2.2681 - val_accuracy: 0.5888\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1521 - accuracy: 0.9400 - val_loss: 2.3411 - val_accuracy: 0.5817\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1939 - accuracy: 0.9268 - val_loss: 2.2931 - val_accuracy: 0.5925\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1675 - accuracy: 0.9396 - val_loss: 2.3162 - val_accuracy: 0.6067\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1475 - accuracy: 0.9464 - val_loss: 2.2534 - val_accuracy: 0.5962\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1376 - accuracy: 0.9516 - val_loss: 2.2564 - val_accuracy: 0.5925\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1428 - accuracy: 0.9498 - val_loss: 2.2521 - val_accuracy: 0.5883\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1262 - accuracy: 0.9561 - val_loss: 2.3177 - val_accuracy: 0.5921\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1200 - accuracy: 0.9548 - val_loss: 2.2963 - val_accuracy: 0.5883\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1201 - accuracy: 0.9566 - val_loss: 2.3568 - val_accuracy: 0.5979\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1236 - accuracy: 0.9538 - val_loss: 2.2873 - val_accuracy: 0.5742\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1179 - accuracy: 0.9609 - val_loss: 2.3002 - val_accuracy: 0.5950\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1223 - accuracy: 0.9523 - val_loss: 2.3005 - val_accuracy: 0.5904\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1140 - accuracy: 0.9611 - val_loss: 2.3337 - val_accuracy: 0.5896\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1236 - accuracy: 0.9539 - val_loss: 2.3011 - val_accuracy: 0.5738\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1224 - accuracy: 0.9541 - val_loss: 2.3252 - val_accuracy: 0.5883\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1217 - accuracy: 0.9545 - val_loss: 2.3773 - val_accuracy: 0.5933\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1208 - accuracy: 0.9570 - val_loss: 2.3065 - val_accuracy: 0.5858\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1104 - accuracy: 0.9616 - val_loss: 2.3008 - val_accuracy: 0.5929\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1133 - accuracy: 0.9593 - val_loss: 2.3852 - val_accuracy: 0.5917\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1139 - accuracy: 0.9595 - val_loss: 2.3464 - val_accuracy: 0.5946\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1134 - accuracy: 0.9614 - val_loss: 2.2850 - val_accuracy: 0.5875\n",
      "Epoch 649/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1218 - accuracy: 0.9548 - val_loss: 2.4299 - val_accuracy: 0.6083\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1212 - accuracy: 0.9541 - val_loss: 2.3165 - val_accuracy: 0.5962\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1185 - accuracy: 0.9598 - val_loss: 2.3632 - val_accuracy: 0.5933\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1125 - accuracy: 0.9614 - val_loss: 2.3492 - val_accuracy: 0.5913\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1224 - accuracy: 0.9554 - val_loss: 2.3856 - val_accuracy: 0.6042\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1155 - accuracy: 0.9575 - val_loss: 2.3452 - val_accuracy: 0.6037\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1108 - accuracy: 0.9638 - val_loss: 2.3643 - val_accuracy: 0.5896\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1128 - accuracy: 0.9570 - val_loss: 2.2962 - val_accuracy: 0.5883\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1339 - accuracy: 0.9546 - val_loss: 2.4417 - val_accuracy: 0.5946\n",
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1488 - accuracy: 0.9402 - val_loss: 2.3766 - val_accuracy: 0.5917\n",
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1378 - accuracy: 0.9480 - val_loss: 2.4197 - val_accuracy: 0.5821\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1370 - accuracy: 0.9446 - val_loss: 2.3651 - val_accuracy: 0.5825\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1332 - accuracy: 0.9507 - val_loss: 2.3525 - val_accuracy: 0.5850\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1152 - accuracy: 0.9577 - val_loss: 2.3698 - val_accuracy: 0.5854\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1078 - accuracy: 0.9616 - val_loss: 2.3998 - val_accuracy: 0.5908\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1186 - accuracy: 0.9568 - val_loss: 2.4188 - val_accuracy: 0.5879\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1088 - accuracy: 0.9598 - val_loss: 2.4207 - val_accuracy: 0.5979\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1218 - accuracy: 0.9532 - val_loss: 2.3917 - val_accuracy: 0.5854\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1150 - accuracy: 0.9614 - val_loss: 2.4071 - val_accuracy: 0.6004\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1232 - accuracy: 0.9561 - val_loss: 2.3503 - val_accuracy: 0.5867\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1355 - accuracy: 0.9464 - val_loss: 2.3676 - val_accuracy: 0.5817\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1198 - accuracy: 0.9573 - val_loss: 2.3083 - val_accuracy: 0.5888\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1121 - accuracy: 0.9641 - val_loss: 2.3418 - val_accuracy: 0.5867\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1110 - accuracy: 0.9616 - val_loss: 2.3754 - val_accuracy: 0.5954\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1095 - accuracy: 0.9634 - val_loss: 2.3902 - val_accuracy: 0.5888\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1095 - accuracy: 0.9611 - val_loss: 2.4037 - val_accuracy: 0.5967\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1069 - accuracy: 0.9655 - val_loss: 2.4025 - val_accuracy: 0.5871\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1063 - accuracy: 0.9634 - val_loss: 2.3716 - val_accuracy: 0.5950\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1141 - accuracy: 0.9577 - val_loss: 2.4447 - val_accuracy: 0.5987\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1071 - accuracy: 0.9602 - val_loss: 2.4011 - val_accuracy: 0.5888\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1118 - accuracy: 0.9577 - val_loss: 2.4247 - val_accuracy: 0.5875\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1098 - accuracy: 0.9613 - val_loss: 2.3977 - val_accuracy: 0.5929\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1118 - accuracy: 0.9593 - val_loss: 2.4007 - val_accuracy: 0.5929\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1067 - accuracy: 0.9638 - val_loss: 2.4459 - val_accuracy: 0.5875\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.1031 - accuracy: 0.9673 - val_loss: 2.3980 - val_accuracy: 0.5846\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.1248 - accuracy: 0.9523 - val_loss: 2.3931 - val_accuracy: 0.5971\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1216 - accuracy: 0.9534 - val_loss: 2.4098 - val_accuracy: 0.5892\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1276 - accuracy: 0.9573 - val_loss: 2.4162 - val_accuracy: 0.5921\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1210 - accuracy: 0.9552 - val_loss: 2.4275 - val_accuracy: 0.5971\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1245 - accuracy: 0.9564 - val_loss: 2.4045 - val_accuracy: 0.5900\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1202 - accuracy: 0.9566 - val_loss: 2.4328 - val_accuracy: 0.5729\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1527 - accuracy: 0.9454 - val_loss: 2.4597 - val_accuracy: 0.5788\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1501 - accuracy: 0.9416 - val_loss: 2.4001 - val_accuracy: 0.5850\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1454 - accuracy: 0.9496 - val_loss: 2.4979 - val_accuracy: 0.5871\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1522 - accuracy: 0.9405 - val_loss: 2.4335 - val_accuracy: 0.5975\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1513 - accuracy: 0.9432 - val_loss: 2.4205 - val_accuracy: 0.5858\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1458 - accuracy: 0.9407 - val_loss: 2.3520 - val_accuracy: 0.5942\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1241 - accuracy: 0.9548 - val_loss: 2.3671 - val_accuracy: 0.5808\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1053 - accuracy: 0.9629 - val_loss: 2.4022 - val_accuracy: 0.5879\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.1007 - accuracy: 0.9657 - val_loss: 2.3841 - val_accuracy: 0.5863\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0963 - accuracy: 0.9657 - val_loss: 2.3937 - val_accuracy: 0.5967\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0977 - accuracy: 0.9679 - val_loss: 2.4167 - val_accuracy: 0.5854\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1060 - accuracy: 0.9611 - val_loss: 2.4696 - val_accuracy: 0.5983\n",
      "Epoch 702/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1066 - accuracy: 0.9625 - val_loss: 2.4142 - val_accuracy: 0.5871\n",
      "Epoch 703/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1007 - accuracy: 0.9659 - val_loss: 2.4335 - val_accuracy: 0.5821\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0992 - accuracy: 0.9645 - val_loss: 2.4448 - val_accuracy: 0.5846\n",
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0986 - accuracy: 0.9689 - val_loss: 2.4323 - val_accuracy: 0.5929\n",
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0971 - accuracy: 0.9668 - val_loss: 2.4185 - val_accuracy: 0.5950\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0964 - accuracy: 0.9686 - val_loss: 2.4332 - val_accuracy: 0.5867\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1056 - accuracy: 0.9614 - val_loss: 2.4755 - val_accuracy: 0.5875\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1149 - accuracy: 0.9564 - val_loss: 2.4761 - val_accuracy: 0.5875\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1018 - accuracy: 0.9657 - val_loss: 2.4591 - val_accuracy: 0.5896\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1052 - accuracy: 0.9645 - val_loss: 2.4619 - val_accuracy: 0.6000\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1067 - accuracy: 0.9616 - val_loss: 2.4377 - val_accuracy: 0.5867\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0925 - accuracy: 0.9693 - val_loss: 2.4403 - val_accuracy: 0.5933\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0993 - accuracy: 0.9682 - val_loss: 2.4280 - val_accuracy: 0.5908\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1026 - accuracy: 0.9666 - val_loss: 2.4594 - val_accuracy: 0.5983\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0967 - accuracy: 0.9684 - val_loss: 2.4425 - val_accuracy: 0.5921\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0950 - accuracy: 0.9680 - val_loss: 2.4528 - val_accuracy: 0.5925\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0997 - accuracy: 0.9645 - val_loss: 2.4552 - val_accuracy: 0.5933\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.1028 - accuracy: 0.9625 - val_loss: 2.5210 - val_accuracy: 0.5996\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1000 - accuracy: 0.9632 - val_loss: 2.4460 - val_accuracy: 0.5871\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0976 - accuracy: 0.9671 - val_loss: 2.4758 - val_accuracy: 0.5858\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1191 - accuracy: 0.9595 - val_loss: 2.4897 - val_accuracy: 0.5854\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1449 - accuracy: 0.9471 - val_loss: 2.4743 - val_accuracy: 0.5896\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1356 - accuracy: 0.9525 - val_loss: 2.4191 - val_accuracy: 0.6004\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1550 - accuracy: 0.9402 - val_loss: 2.5476 - val_accuracy: 0.5850\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1419 - accuracy: 0.9516 - val_loss: 2.4831 - val_accuracy: 0.5879\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1658 - accuracy: 0.9323 - val_loss: 2.5181 - val_accuracy: 0.5846\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1289 - accuracy: 0.9543 - val_loss: 2.4932 - val_accuracy: 0.5975\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1196 - accuracy: 0.9552 - val_loss: 2.4642 - val_accuracy: 0.5942\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0985 - accuracy: 0.9648 - val_loss: 2.4691 - val_accuracy: 0.5858\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1059 - accuracy: 0.9618 - val_loss: 2.5171 - val_accuracy: 0.5871\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1025 - accuracy: 0.9638 - val_loss: 2.4942 - val_accuracy: 0.5792\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0942 - accuracy: 0.9686 - val_loss: 2.4419 - val_accuracy: 0.5917\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0972 - accuracy: 0.9655 - val_loss: 2.4887 - val_accuracy: 0.5908\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0935 - accuracy: 0.9693 - val_loss: 2.4555 - val_accuracy: 0.5925\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0951 - accuracy: 0.9659 - val_loss: 2.4671 - val_accuracy: 0.5867\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0845 - accuracy: 0.9705 - val_loss: 2.5372 - val_accuracy: 0.5929\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0957 - accuracy: 0.9682 - val_loss: 2.5045 - val_accuracy: 0.5933\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0935 - accuracy: 0.9702 - val_loss: 2.4966 - val_accuracy: 0.5996\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1013 - accuracy: 0.9632 - val_loss: 2.5038 - val_accuracy: 0.5829\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0906 - accuracy: 0.9693 - val_loss: 2.4947 - val_accuracy: 0.5908\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0902 - accuracy: 0.9698 - val_loss: 2.5372 - val_accuracy: 0.5992\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0955 - accuracy: 0.9639 - val_loss: 2.5186 - val_accuracy: 0.5892\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0855 - accuracy: 0.9714 - val_loss: 2.5398 - val_accuracy: 0.5917\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0965 - accuracy: 0.9636 - val_loss: 2.5025 - val_accuracy: 0.5813\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0908 - accuracy: 0.9691 - val_loss: 2.5244 - val_accuracy: 0.5871\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0932 - accuracy: 0.9693 - val_loss: 2.5016 - val_accuracy: 0.5846\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0931 - accuracy: 0.9693 - val_loss: 2.5938 - val_accuracy: 0.5958\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0968 - accuracy: 0.9641 - val_loss: 2.5398 - val_accuracy: 0.5917\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0964 - accuracy: 0.9641 - val_loss: 2.5428 - val_accuracy: 0.6042\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0943 - accuracy: 0.9668 - val_loss: 2.5459 - val_accuracy: 0.5892\n",
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0989 - accuracy: 0.9625 - val_loss: 2.5479 - val_accuracy: 0.5925\n",
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1010 - accuracy: 0.9639 - val_loss: 2.5426 - val_accuracy: 0.5850\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0959 - accuracy: 0.9661 - val_loss: 2.5387 - val_accuracy: 0.5962\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0934 - accuracy: 0.9673 - val_loss: 2.5562 - val_accuracy: 0.5846\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0961 - accuracy: 0.9663 - val_loss: 2.5134 - val_accuracy: 0.5904\n",
      "Epoch 757/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0979 - accuracy: 0.9679 - val_loss: 2.5005 - val_accuracy: 0.5850\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1182 - accuracy: 0.9577 - val_loss: 2.4831 - val_accuracy: 0.6021\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0979 - accuracy: 0.9652 - val_loss: 2.5630 - val_accuracy: 0.5888\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0943 - accuracy: 0.9673 - val_loss: 2.5506 - val_accuracy: 0.5863\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0991 - accuracy: 0.9695 - val_loss: 2.5057 - val_accuracy: 0.5904\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0935 - accuracy: 0.9688 - val_loss: 2.5623 - val_accuracy: 0.5875\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0873 - accuracy: 0.9709 - val_loss: 2.6069 - val_accuracy: 0.5854\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1028 - accuracy: 0.9625 - val_loss: 2.5583 - val_accuracy: 0.5925\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1089 - accuracy: 0.9605 - val_loss: 2.5541 - val_accuracy: 0.5850\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0929 - accuracy: 0.9696 - val_loss: 2.5491 - val_accuracy: 0.6008\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1042 - accuracy: 0.9605 - val_loss: 2.5477 - val_accuracy: 0.5908\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0982 - accuracy: 0.9639 - val_loss: 2.6069 - val_accuracy: 0.5875\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0979 - accuracy: 0.9661 - val_loss: 2.5656 - val_accuracy: 0.5913\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1009 - accuracy: 0.9655 - val_loss: 2.5448 - val_accuracy: 0.5946\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1160 - accuracy: 0.9575 - val_loss: 2.5792 - val_accuracy: 0.5917\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1238 - accuracy: 0.9571 - val_loss: 2.5641 - val_accuracy: 0.5954\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1204 - accuracy: 0.9563 - val_loss: 2.5582 - val_accuracy: 0.5850\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1104 - accuracy: 0.9607 - val_loss: 2.5752 - val_accuracy: 0.5892\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0934 - accuracy: 0.9691 - val_loss: 2.5635 - val_accuracy: 0.5871\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0890 - accuracy: 0.9702 - val_loss: 2.5568 - val_accuracy: 0.5804\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0989 - accuracy: 0.9650 - val_loss: 2.6086 - val_accuracy: 0.5775\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0920 - accuracy: 0.9680 - val_loss: 2.6141 - val_accuracy: 0.5875\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0868 - accuracy: 0.9721 - val_loss: 2.5609 - val_accuracy: 0.5929\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0851 - accuracy: 0.9702 - val_loss: 2.5892 - val_accuracy: 0.5938\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0837 - accuracy: 0.9725 - val_loss: 2.5756 - val_accuracy: 0.5896\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0924 - accuracy: 0.9668 - val_loss: 2.5573 - val_accuracy: 0.5950\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0818 - accuracy: 0.9730 - val_loss: 2.5959 - val_accuracy: 0.5879\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0801 - accuracy: 0.9759 - val_loss: 2.5866 - val_accuracy: 0.5913\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0841 - accuracy: 0.9721 - val_loss: 2.5959 - val_accuracy: 0.5858\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0807 - accuracy: 0.9754 - val_loss: 2.5855 - val_accuracy: 0.5917\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0948 - accuracy: 0.9664 - val_loss: 2.5728 - val_accuracy: 0.5896\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1206 - accuracy: 0.9577 - val_loss: 2.6210 - val_accuracy: 0.5771\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1700 - accuracy: 0.9362 - val_loss: 2.5154 - val_accuracy: 0.5758\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1396 - accuracy: 0.9570 - val_loss: 2.4995 - val_accuracy: 0.5929\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1152 - accuracy: 0.9625 - val_loss: 2.6084 - val_accuracy: 0.5863\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1034 - accuracy: 0.9584 - val_loss: 2.5908 - val_accuracy: 0.5813\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0864 - accuracy: 0.9711 - val_loss: 2.5355 - val_accuracy: 0.5962\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0821 - accuracy: 0.9746 - val_loss: 2.5817 - val_accuracy: 0.5946\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0785 - accuracy: 0.9746 - val_loss: 2.5960 - val_accuracy: 0.5917\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0756 - accuracy: 0.9764 - val_loss: 2.5915 - val_accuracy: 0.5946\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0781 - accuracy: 0.9739 - val_loss: 2.6273 - val_accuracy: 0.5879\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0809 - accuracy: 0.9730 - val_loss: 2.6076 - val_accuracy: 0.5892\n",
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0798 - accuracy: 0.9734 - val_loss: 2.6623 - val_accuracy: 0.6062\n",
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0872 - accuracy: 0.9707 - val_loss: 2.6248 - val_accuracy: 0.5933\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0860 - accuracy: 0.9727 - val_loss: 2.6333 - val_accuracy: 0.5983\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0868 - accuracy: 0.9702 - val_loss: 2.6510 - val_accuracy: 0.5883\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0849 - accuracy: 0.9707 - val_loss: 2.5945 - val_accuracy: 0.5875\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0834 - accuracy: 0.9732 - val_loss: 2.6075 - val_accuracy: 0.5971\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0789 - accuracy: 0.9737 - val_loss: 2.6490 - val_accuracy: 0.5871\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0912 - accuracy: 0.9693 - val_loss: 2.5798 - val_accuracy: 0.5996\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0871 - accuracy: 0.9707 - val_loss: 2.5931 - val_accuracy: 0.5867\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0873 - accuracy: 0.9702 - val_loss: 2.5824 - val_accuracy: 0.5983\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0888 - accuracy: 0.9680 - val_loss: 2.6338 - val_accuracy: 0.5908\n",
      "Epoch 810/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0865 - accuracy: 0.9704 - val_loss: 2.6646 - val_accuracy: 0.5863\n",
      "Epoch 811/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1080 - accuracy: 0.9616 - val_loss: 2.7014 - val_accuracy: 0.6012\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1416 - accuracy: 0.9493 - val_loss: 2.6993 - val_accuracy: 0.6062\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1146 - accuracy: 0.9630 - val_loss: 2.6655 - val_accuracy: 0.5992\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1169 - accuracy: 0.9593 - val_loss: 2.6321 - val_accuracy: 0.5863\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1037 - accuracy: 0.9595 - val_loss: 2.6445 - val_accuracy: 0.5858\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.0881 - accuracy: 0.9727 - val_loss: 2.6258 - val_accuracy: 0.5875\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.1006 - accuracy: 0.9634 - val_loss: 2.6591 - val_accuracy: 0.5817\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0920 - accuracy: 0.9691 - val_loss: 2.6621 - val_accuracy: 0.6004\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0862 - accuracy: 0.9725 - val_loss: 2.6405 - val_accuracy: 0.5917\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0777 - accuracy: 0.9739 - val_loss: 2.6602 - val_accuracy: 0.5950\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0803 - accuracy: 0.9746 - val_loss: 2.6947 - val_accuracy: 0.5975\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0731 - accuracy: 0.9762 - val_loss: 2.6801 - val_accuracy: 0.5883\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0716 - accuracy: 0.9762 - val_loss: 2.6688 - val_accuracy: 0.5892\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0770 - accuracy: 0.9741 - val_loss: 2.6851 - val_accuracy: 0.5892\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0768 - accuracy: 0.9734 - val_loss: 2.6793 - val_accuracy: 0.5900\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0714 - accuracy: 0.9771 - val_loss: 2.6776 - val_accuracy: 0.5871\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0712 - accuracy: 0.9775 - val_loss: 2.6861 - val_accuracy: 0.5804\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0725 - accuracy: 0.9766 - val_loss: 2.7064 - val_accuracy: 0.5875\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0721 - accuracy: 0.9786 - val_loss: 2.6836 - val_accuracy: 0.5871\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0716 - accuracy: 0.9786 - val_loss: 2.6758 - val_accuracy: 0.5763\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0783 - accuracy: 0.9725 - val_loss: 2.6941 - val_accuracy: 0.6000\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0749 - accuracy: 0.9755 - val_loss: 2.6822 - val_accuracy: 0.5913\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0812 - accuracy: 0.9721 - val_loss: 2.7187 - val_accuracy: 0.5929\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0773 - accuracy: 0.9736 - val_loss: 2.6969 - val_accuracy: 0.5775\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0851 - accuracy: 0.9704 - val_loss: 2.6716 - val_accuracy: 0.5925\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0764 - accuracy: 0.9743 - val_loss: 2.6768 - val_accuracy: 0.5962\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0782 - accuracy: 0.9734 - val_loss: 2.6882 - val_accuracy: 0.5913\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0707 - accuracy: 0.9775 - val_loss: 2.6892 - val_accuracy: 0.5792\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0795 - accuracy: 0.9745 - val_loss: 2.6730 - val_accuracy: 0.5825\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0809 - accuracy: 0.9743 - val_loss: 2.7321 - val_accuracy: 0.5967\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0868 - accuracy: 0.9693 - val_loss: 2.6824 - val_accuracy: 0.5883\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0813 - accuracy: 0.9716 - val_loss: 2.6631 - val_accuracy: 0.5921\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0856 - accuracy: 0.9716 - val_loss: 2.7041 - val_accuracy: 0.5929\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0743 - accuracy: 0.9768 - val_loss: 2.7153 - val_accuracy: 0.5767\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0718 - accuracy: 0.9780 - val_loss: 2.7040 - val_accuracy: 0.5917\n",
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0740 - accuracy: 0.9761 - val_loss: 2.7150 - val_accuracy: 0.5958\n",
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.0871 - accuracy: 0.9734 - val_loss: 2.6934 - val_accuracy: 0.5958\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0986 - accuracy: 0.9668 - val_loss: 2.7744 - val_accuracy: 0.5896\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1123 - accuracy: 0.9584 - val_loss: 2.7127 - val_accuracy: 0.5850\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1160 - accuracy: 0.9514 - val_loss: 2.7256 - val_accuracy: 0.5821\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1613 - accuracy: 0.9418 - val_loss: 2.7011 - val_accuracy: 0.5713\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.2090 - accuracy: 0.9309 - val_loss: 2.7052 - val_accuracy: 0.5975\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1798 - accuracy: 0.9396 - val_loss: 2.6820 - val_accuracy: 0.6025\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.1406 - accuracy: 0.9534 - val_loss: 2.7141 - val_accuracy: 0.5987\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1232 - accuracy: 0.9532 - val_loss: 2.7289 - val_accuracy: 0.5908\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0976 - accuracy: 0.9657 - val_loss: 2.7104 - val_accuracy: 0.5888\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0799 - accuracy: 0.9721 - val_loss: 2.6867 - val_accuracy: 0.5825\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0713 - accuracy: 0.9768 - val_loss: 2.7046 - val_accuracy: 0.5900\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0721 - accuracy: 0.9752 - val_loss: 2.6841 - val_accuracy: 0.5888\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0756 - accuracy: 0.9739 - val_loss: 2.7081 - val_accuracy: 0.5758\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0747 - accuracy: 0.9782 - val_loss: 2.7100 - val_accuracy: 0.5867\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0675 - accuracy: 0.9796 - val_loss: 2.7365 - val_accuracy: 0.5996\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0694 - accuracy: 0.9807 - val_loss: 2.6996 - val_accuracy: 0.5938\n",
      "Epoch 864/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0707 - accuracy: 0.9771 - val_loss: 2.7220 - val_accuracy: 0.5913\n",
      "Epoch 865/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0725 - accuracy: 0.9771 - val_loss: 2.7486 - val_accuracy: 0.5729\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0757 - accuracy: 0.9761 - val_loss: 2.7133 - val_accuracy: 0.5896\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0643 - accuracy: 0.9802 - val_loss: 2.7338 - val_accuracy: 0.5933\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0645 - accuracy: 0.9796 - val_loss: 2.7227 - val_accuracy: 0.5925\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0724 - accuracy: 0.9732 - val_loss: 2.7199 - val_accuracy: 0.5929\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0649 - accuracy: 0.9816 - val_loss: 2.7642 - val_accuracy: 0.5950\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0717 - accuracy: 0.9784 - val_loss: 2.7463 - val_accuracy: 0.5817\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0636 - accuracy: 0.9820 - val_loss: 2.7470 - val_accuracy: 0.6000\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0763 - accuracy: 0.9752 - val_loss: 2.7283 - val_accuracy: 0.5863\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0766 - accuracy: 0.9729 - val_loss: 2.7410 - val_accuracy: 0.6058\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0737 - accuracy: 0.9761 - val_loss: 2.7175 - val_accuracy: 0.5913\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0700 - accuracy: 0.9757 - val_loss: 2.7411 - val_accuracy: 0.5962\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0704 - accuracy: 0.9770 - val_loss: 2.7518 - val_accuracy: 0.5783\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0688 - accuracy: 0.9789 - val_loss: 2.7595 - val_accuracy: 0.5783\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0902 - accuracy: 0.9664 - val_loss: 2.7408 - val_accuracy: 0.5896\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0762 - accuracy: 0.9752 - val_loss: 2.7289 - val_accuracy: 0.5921\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0693 - accuracy: 0.9770 - val_loss: 2.7086 - val_accuracy: 0.5871\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0645 - accuracy: 0.9805 - val_loss: 2.7945 - val_accuracy: 0.5938\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0726 - accuracy: 0.9757 - val_loss: 2.7457 - val_accuracy: 0.5854\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0676 - accuracy: 0.9791 - val_loss: 2.7906 - val_accuracy: 0.5842\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0702 - accuracy: 0.9784 - val_loss: 2.7253 - val_accuracy: 0.5921\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0682 - accuracy: 0.9754 - val_loss: 2.8186 - val_accuracy: 0.5896\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0717 - accuracy: 0.9787 - val_loss: 2.7463 - val_accuracy: 0.5821\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0685 - accuracy: 0.9775 - val_loss: 2.7288 - val_accuracy: 0.5804\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0699 - accuracy: 0.9766 - val_loss: 2.7495 - val_accuracy: 0.5904\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0745 - accuracy: 0.9757 - val_loss: 2.7455 - val_accuracy: 0.5883\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0718 - accuracy: 0.9750 - val_loss: 2.7658 - val_accuracy: 0.5796\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0750 - accuracy: 0.9761 - val_loss: 2.7432 - val_accuracy: 0.5888\n",
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0737 - accuracy: 0.9764 - val_loss: 2.7737 - val_accuracy: 0.5825\n",
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0738 - accuracy: 0.9759 - val_loss: 2.8016 - val_accuracy: 0.6008\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0717 - accuracy: 0.9761 - val_loss: 2.7983 - val_accuracy: 0.5996\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0911 - accuracy: 0.9680 - val_loss: 2.7734 - val_accuracy: 0.5900\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1198 - accuracy: 0.9621 - val_loss: 2.7826 - val_accuracy: 0.5888\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1664 - accuracy: 0.9425 - val_loss: 2.7325 - val_accuracy: 0.5875\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1825 - accuracy: 0.9321 - val_loss: 2.8942 - val_accuracy: 0.5875\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1500 - accuracy: 0.9455 - val_loss: 2.7676 - val_accuracy: 0.5867\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1161 - accuracy: 0.9575 - val_loss: 2.7886 - val_accuracy: 0.5950\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0949 - accuracy: 0.9673 - val_loss: 2.7867 - val_accuracy: 0.5792\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0891 - accuracy: 0.9695 - val_loss: 2.7486 - val_accuracy: 0.5883\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0819 - accuracy: 0.9727 - val_loss: 2.7836 - val_accuracy: 0.6004\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0800 - accuracy: 0.9768 - val_loss: 2.7748 - val_accuracy: 0.6033\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0785 - accuracy: 0.9730 - val_loss: 2.7570 - val_accuracy: 0.5817\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0636 - accuracy: 0.9812 - val_loss: 2.7787 - val_accuracy: 0.5813\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0628 - accuracy: 0.9812 - val_loss: 2.7832 - val_accuracy: 0.5962\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0654 - accuracy: 0.9786 - val_loss: 2.8345 - val_accuracy: 0.5879\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0631 - accuracy: 0.9816 - val_loss: 2.7860 - val_accuracy: 0.5908\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0604 - accuracy: 0.9818 - val_loss: 2.7928 - val_accuracy: 0.5925\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0628 - accuracy: 0.9804 - val_loss: 2.7966 - val_accuracy: 0.5821\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0601 - accuracy: 0.9807 - val_loss: 2.7808 - val_accuracy: 0.5846\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0626 - accuracy: 0.9814 - val_loss: 2.7803 - val_accuracy: 0.5921\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0632 - accuracy: 0.9786 - val_loss: 2.7960 - val_accuracy: 0.5875\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0630 - accuracy: 0.9789 - val_loss: 2.8152 - val_accuracy: 0.5879\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0674 - accuracy: 0.9752 - val_loss: 2.7981 - val_accuracy: 0.5888\n",
      "Epoch 918/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0656 - accuracy: 0.9777 - val_loss: 2.7935 - val_accuracy: 0.5942\n",
      "Epoch 919/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0588 - accuracy: 0.9809 - val_loss: 2.7996 - val_accuracy: 0.5938\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0625 - accuracy: 0.9814 - val_loss: 2.8334 - val_accuracy: 0.5842\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0612 - accuracy: 0.9814 - val_loss: 2.8098 - val_accuracy: 0.5888\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0644 - accuracy: 0.9787 - val_loss: 2.8052 - val_accuracy: 0.5946\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.0595 - accuracy: 0.9804 - val_loss: 2.8082 - val_accuracy: 0.5842\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0673 - accuracy: 0.9793 - val_loss: 2.8232 - val_accuracy: 0.5896\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0685 - accuracy: 0.9773 - val_loss: 2.8143 - val_accuracy: 0.5875\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0635 - accuracy: 0.9811 - val_loss: 2.8235 - val_accuracy: 0.5954\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0666 - accuracy: 0.9793 - val_loss: 2.8105 - val_accuracy: 0.5954\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0598 - accuracy: 0.9823 - val_loss: 2.8174 - val_accuracy: 0.5871\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0621 - accuracy: 0.9812 - val_loss: 2.8651 - val_accuracy: 0.5954\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.0638 - accuracy: 0.9805 - val_loss: 2.8284 - val_accuracy: 0.5846\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0697 - accuracy: 0.9780 - val_loss: 2.8230 - val_accuracy: 0.5854\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0722 - accuracy: 0.9764 - val_loss: 2.8113 - val_accuracy: 0.5913\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0706 - accuracy: 0.9773 - val_loss: 2.8782 - val_accuracy: 0.5875\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0706 - accuracy: 0.9780 - val_loss: 2.9081 - val_accuracy: 0.5754\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.1172 - accuracy: 0.9564 - val_loss: 2.8169 - val_accuracy: 0.5829\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1452 - accuracy: 0.9586 - val_loss: 2.8600 - val_accuracy: 0.5771\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.2652 - accuracy: 0.9098 - val_loss: 2.7622 - val_accuracy: 0.6017\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1505 - accuracy: 0.9459 - val_loss: 2.8512 - val_accuracy: 0.5888\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.1113 - accuracy: 0.9627 - val_loss: 2.8534 - val_accuracy: 0.5904\n",
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.1111 - accuracy: 0.9627 - val_loss: 2.8281 - val_accuracy: 0.5892\n",
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0829 - accuracy: 0.9718 - val_loss: 2.8520 - val_accuracy: 0.5813\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1033 - accuracy: 0.9679 - val_loss: 2.8346 - val_accuracy: 0.5971\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0751 - accuracy: 0.9737 - val_loss: 2.8456 - val_accuracy: 0.5854\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0690 - accuracy: 0.9773 - val_loss: 2.8205 - val_accuracy: 0.5867\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0650 - accuracy: 0.9796 - val_loss: 2.8568 - val_accuracy: 0.5950\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0565 - accuracy: 0.9820 - val_loss: 2.8716 - val_accuracy: 0.5871\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0515 - accuracy: 0.9870 - val_loss: 2.8423 - val_accuracy: 0.5942\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0574 - accuracy: 0.9818 - val_loss: 2.8328 - val_accuracy: 0.5825\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0561 - accuracy: 0.9832 - val_loss: 2.8800 - val_accuracy: 0.5904\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0572 - accuracy: 0.9830 - val_loss: 2.8327 - val_accuracy: 0.5879\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0577 - accuracy: 0.9823 - val_loss: 2.8460 - val_accuracy: 0.5875\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0616 - accuracy: 0.9814 - val_loss: 2.8301 - val_accuracy: 0.5904\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0540 - accuracy: 0.9850 - val_loss: 2.8715 - val_accuracy: 0.5821\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0612 - accuracy: 0.9807 - val_loss: 2.8817 - val_accuracy: 0.5804\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0576 - accuracy: 0.9829 - val_loss: 2.8727 - val_accuracy: 0.5817\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0605 - accuracy: 0.9809 - val_loss: 2.8420 - val_accuracy: 0.5867\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0635 - accuracy: 0.9795 - val_loss: 2.8636 - val_accuracy: 0.5917\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0607 - accuracy: 0.9807 - val_loss: 2.8706 - val_accuracy: 0.5908\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0578 - accuracy: 0.9820 - val_loss: 2.8591 - val_accuracy: 0.5942\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0605 - accuracy: 0.9829 - val_loss: 2.9296 - val_accuracy: 0.5892\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0767 - accuracy: 0.9732 - val_loss: 2.9023 - val_accuracy: 0.5842\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.0658 - accuracy: 0.9795 - val_loss: 2.8911 - val_accuracy: 0.5871\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.0622 - accuracy: 0.9802 - val_loss: 2.8622 - val_accuracy: 0.5971\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0670 - accuracy: 0.9793 - val_loss: 2.8584 - val_accuracy: 0.5967\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0676 - accuracy: 0.9779 - val_loss: 2.8783 - val_accuracy: 0.5888\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0534 - accuracy: 0.9848 - val_loss: 2.8745 - val_accuracy: 0.5850\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0599 - accuracy: 0.9821 - val_loss: 2.9312 - val_accuracy: 0.5813\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.0636 - accuracy: 0.9791 - val_loss: 2.9063 - val_accuracy: 0.5792\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0610 - accuracy: 0.9823 - val_loss: 2.9260 - val_accuracy: 0.5929\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0693 - accuracy: 0.9748 - val_loss: 2.9023 - val_accuracy: 0.5933\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0688 - accuracy: 0.9754 - val_loss: 2.9354 - val_accuracy: 0.5900\n",
      "Epoch 972/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0648 - accuracy: 0.9802 - val_loss: 2.9248 - val_accuracy: 0.5746\n",
      "Epoch 973/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0619 - accuracy: 0.9796 - val_loss: 2.8927 - val_accuracy: 0.5938\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.0602 - accuracy: 0.9816 - val_loss: 2.9062 - val_accuracy: 0.5858\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0568 - accuracy: 0.9850 - val_loss: 2.8897 - val_accuracy: 0.5892\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0502 - accuracy: 0.9859 - val_loss: 2.9288 - val_accuracy: 0.5850\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0570 - accuracy: 0.9830 - val_loss: 2.9122 - val_accuracy: 0.5967\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0541 - accuracy: 0.9834 - val_loss: 2.9075 - val_accuracy: 0.5788\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0590 - accuracy: 0.9814 - val_loss: 2.9268 - val_accuracy: 0.5979\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0645 - accuracy: 0.9780 - val_loss: 2.9063 - val_accuracy: 0.5933\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.0601 - accuracy: 0.9812 - val_loss: 2.9125 - val_accuracy: 0.5867\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0590 - accuracy: 0.9809 - val_loss: 2.9528 - val_accuracy: 0.5992\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0591 - accuracy: 0.9832 - val_loss: 2.8996 - val_accuracy: 0.5838\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0601 - accuracy: 0.9825 - val_loss: 2.9124 - val_accuracy: 0.5942\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.0564 - accuracy: 0.9834 - val_loss: 2.9170 - val_accuracy: 0.5921\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0650 - accuracy: 0.9804 - val_loss: 2.9339 - val_accuracy: 0.5871\n",
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.0555 - accuracy: 0.9841 - val_loss: 2.9279 - val_accuracy: 0.5904\n",
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0705 - accuracy: 0.9784 - val_loss: 2.9181 - val_accuracy: 0.5946\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0811 - accuracy: 0.9723 - val_loss: 2.8413 - val_accuracy: 0.5821\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0765 - accuracy: 0.9741 - val_loss: 2.9866 - val_accuracy: 0.5833\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1094 - accuracy: 0.9613 - val_loss: 2.9333 - val_accuracy: 0.5917\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1930 - accuracy: 0.9445 - val_loss: 2.9804 - val_accuracy: 0.5842\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1576 - accuracy: 0.9513 - val_loss: 2.9231 - val_accuracy: 0.5896\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.1062 - accuracy: 0.9643 - val_loss: 2.9096 - val_accuracy: 0.5879\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.1127 - accuracy: 0.9604 - val_loss: 2.8685 - val_accuracy: 0.5838\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0940 - accuracy: 0.9684 - val_loss: 2.8973 - val_accuracy: 0.5962\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0625 - accuracy: 0.9809 - val_loss: 2.9088 - val_accuracy: 0.5888\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.0670 - accuracy: 0.9791 - val_loss: 2.9054 - val_accuracy: 0.5946\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0642 - accuracy: 0.9805 - val_loss: 2.9496 - val_accuracy: 0.5896\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.0537 - accuracy: 0.9861 - val_loss: 2.8965 - val_accuracy: 0.5971\n"
     ]
    }
   ],
   "source": [
    "# specify network layers\n",
    "binary_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (13, )),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# compile and fit network\n",
    "binary_ann.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "history = binary_ann.fit(X_train, y_train, epochs = 1000, batch_size = 128, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs. Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 0.7570 - accuracy: 0.5245 - val_loss: 0.6523 - val_accuracy: 0.6454\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6573 - accuracy: 0.6402 - val_loss: 0.6477 - val_accuracy: 0.6454\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6531 - accuracy: 0.6407 - val_loss: 0.6456 - val_accuracy: 0.6454\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6518 - accuracy: 0.6416 - val_loss: 0.6441 - val_accuracy: 0.6454\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6508 - accuracy: 0.6414 - val_loss: 0.6438 - val_accuracy: 0.6454\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6501 - accuracy: 0.6414 - val_loss: 0.6424 - val_accuracy: 0.6454\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6499 - accuracy: 0.6418 - val_loss: 0.6419 - val_accuracy: 0.6454\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 0s 41us/sample - loss: 0.6492 - accuracy: 0.6414 - val_loss: 0.6413 - val_accuracy: 0.6454\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6471 - accuracy: 0.6418 - val_loss: 0.6405 - val_accuracy: 0.6454\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6479 - accuracy: 0.6418 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6481 - accuracy: 0.6418 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6497 - accuracy: 0.6418 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6485 - accuracy: 0.6414 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6479 - accuracy: 0.6418 - val_loss: 0.6424 - val_accuracy: 0.6454\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6483 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6410 - val_accuracy: 0.6454\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6481 - accuracy: 0.6416 - val_loss: 0.6403 - val_accuracy: 0.6454\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6471 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6477 - accuracy: 0.6416 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6473 - accuracy: 0.6416 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6480 - accuracy: 0.6416 - val_loss: 0.6430 - val_accuracy: 0.6454\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6466 - accuracy: 0.6413 - val_loss: 0.6437 - val_accuracy: 0.6454\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6499 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6456 - accuracy: 0.6413 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6486 - accuracy: 0.6418 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6478 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6476 - accuracy: 0.6414 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 0s 44us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6480 - accuracy: 0.6416 - val_loss: 0.6422 - val_accuracy: 0.6454\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6468 - accuracy: 0.6418 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6473 - accuracy: 0.6414 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6479 - accuracy: 0.6416 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6437 - val_accuracy: 0.6454\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6449 - accuracy: 0.6418 - val_loss: 0.6501 - val_accuracy: 0.6408\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6464 - accuracy: 0.6420 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6484 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6468 - accuracy: 0.6413 - val_loss: 0.6429 - val_accuracy: 0.6454\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6418 - val_accuracy: 0.6454\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6488 - accuracy: 0.6416 - val_loss: 0.6417 - val_accuracy: 0.6454\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6475 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6473 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6462 - accuracy: 0.6416 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6454 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6467 - accuracy: 0.6416 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6463 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 54/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6463 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6428 - val_accuracy: 0.6454\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 0s 54us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6455 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6399 - val_accuracy: 0.6454\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6447 - accuracy: 0.6416 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6457 - accuracy: 0.6414 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6458 - accuracy: 0.6414 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6441 - val_accuracy: 0.6454\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6461 - accuracy: 0.6418 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6410 - val_accuracy: 0.6454\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6464 - accuracy: 0.6418 - val_loss: 0.6403 - val_accuracy: 0.6454\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6448 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6464 - accuracy: 0.6416 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6468 - accuracy: 0.6418 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6465 - accuracy: 0.6416 - val_loss: 0.6418 - val_accuracy: 0.6454\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6455 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6408 - val_accuracy: 0.6454\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6400 - val_accuracy: 0.6454\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6450 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6462 - accuracy: 0.6409 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6484 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6451 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 94/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 95/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6458 - accuracy: 0.6414 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6475 - accuracy: 0.6411 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6461 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6414 - val_loss: 0.6406 - val_accuracy: 0.6454\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6452 - accuracy: 0.6414 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6442 - accuracy: 0.6414 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6457 - accuracy: 0.6416 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6450 - accuracy: 0.6413 - val_loss: 0.6456 - val_accuracy: 0.6446\n",
      "Epoch 108/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6475 - accuracy: 0.6411 - val_loss: 0.6384 - val_accuracy: 0.6454\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6459 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6451 - accuracy: 0.6416 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6443 - accuracy: 0.6418 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6468 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6453 - accuracy: 0.6416 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6445 - accuracy: 0.6418 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6449 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6468 - accuracy: 0.6414 - val_loss: 0.6411 - val_accuracy: 0.6454\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - 0s 36us/sample - loss: 0.6437 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6471 - accuracy: 0.6416 - val_loss: 0.6401 - val_accuracy: 0.6454\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6440 - accuracy: 0.6420 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6471 - accuracy: 0.6420 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6475 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6433 - val_accuracy: 0.6454\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6466 - accuracy: 0.6423 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6434 - accuracy: 0.6405 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6449 - accuracy: 0.6416 - val_loss: 0.6386 - val_accuracy: 0.6454\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6451 - accuracy: 0.6423 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 39us/sample - loss: 0.6452 - accuracy: 0.6416 - val_loss: 0.6379 - val_accuracy: 0.6454\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6435 - accuracy: 0.6414 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6441 - accuracy: 0.6418 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6454 - accuracy: 0.6414 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6442 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6429 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6443 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6444 - accuracy: 0.6418 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6439 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6433 - accuracy: 0.6416 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6469 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6445 - accuracy: 0.6416 - val_loss: 0.6390 - val_accuracy: 0.6454\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6446 - accuracy: 0.6416 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6441 - accuracy: 0.6413 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6391 - val_accuracy: 0.6454\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6444 - accuracy: 0.6416 - val_loss: 0.6411 - val_accuracy: 0.6454\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6441 - accuracy: 0.6416 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6431 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 162/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 163/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6422 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6426 - accuracy: 0.6414 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6450 - accuracy: 0.6418 - val_loss: 0.6392 - val_accuracy: 0.6454\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6445 - accuracy: 0.6414 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6439 - accuracy: 0.6418 - val_loss: 0.6378 - val_accuracy: 0.6454\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6460 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6435 - accuracy: 0.6420 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6435 - accuracy: 0.6416 - val_loss: 0.6379 - val_accuracy: 0.6454\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6436 - accuracy: 0.6416 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6444 - accuracy: 0.6409 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6384 - val_accuracy: 0.6454\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6456 - accuracy: 0.6416 - val_loss: 0.6396 - val_accuracy: 0.6454\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6422 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6450 - accuracy: 0.6416 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6430 - accuracy: 0.6416 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6456 - accuracy: 0.6418 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6425 - accuracy: 0.6409 - val_loss: 0.6450 - val_accuracy: 0.6442\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6418 - accuracy: 0.6421 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6427 - accuracy: 0.6418 - val_loss: 0.6412 - val_accuracy: 0.6454\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6420 - accuracy: 0.6414 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6420 - accuracy: 0.6420 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6435 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6394 - val_accuracy: 0.6454\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6458 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6438 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6416 - accuracy: 0.6413 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6438 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6425 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6406 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6440 - accuracy: 0.6416 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6426 - accuracy: 0.6416 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6439 - accuracy: 0.6418 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6420 - accuracy: 0.6423 - val_loss: 0.6385 - val_accuracy: 0.6454\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6382 - val_accuracy: 0.6454\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6428 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6428 - accuracy: 0.6416 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6447 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6409 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6454\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6430 - accuracy: 0.6404 - val_loss: 0.6424 - val_accuracy: 0.6463\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6417 - accuracy: 0.6413 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6419 - accuracy: 0.6418 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6434 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6406 - accuracy: 0.6416 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6420 - accuracy: 0.6423 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 216/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6425 - accuracy: 0.6411 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 217/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6415 - accuracy: 0.6413 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6426 - accuracy: 0.6416 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6417 - accuracy: 0.6418 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6442 - accuracy: 0.6414 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6428 - accuracy: 0.6420 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6421 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6389 - val_accuracy: 0.6454\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6419 - accuracy: 0.6414 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6404 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6429 - accuracy: 0.6413 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6420 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6425 - accuracy: 0.6418 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6413 - accuracy: 0.6414 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6421 - accuracy: 0.6414 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6409 - accuracy: 0.6420 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6432 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6418 - accuracy: 0.6418 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6428 - accuracy: 0.6414 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6423 - accuracy: 0.6416 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6421 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6412 - accuracy: 0.6416 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6417 - accuracy: 0.6414 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6433 - accuracy: 0.6409 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6410 - accuracy: 0.6420 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6401 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6415 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6416 - accuracy: 0.6416 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6412 - accuracy: 0.6414 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6398 - accuracy: 0.6414 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6424 - accuracy: 0.6416 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6407 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6390 - accuracy: 0.6420 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6413 - accuracy: 0.6416 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6405 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6417 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6390 - accuracy: 0.6418 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6398 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6408 - accuracy: 0.6416 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6414 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6409 - accuracy: 0.6421 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6421 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6420 - accuracy: 0.6418 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6407 - accuracy: 0.6413 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6412 - accuracy: 0.6416 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6386 - accuracy: 0.6413 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6395 - accuracy: 0.6414 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6389 - accuracy: 0.6413 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6405 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6373 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6399 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 270/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6407 - accuracy: 0.6411 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 271/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6425 - accuracy: 0.6416 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6400 - accuracy: 0.6418 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6401 - accuracy: 0.6427 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6379 - accuracy: 0.6421 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6378 - accuracy: 0.6418 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6394 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6394 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6394 - accuracy: 0.6421 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6402 - accuracy: 0.6418 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6410 - accuracy: 0.6416 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6400 - accuracy: 0.6416 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6403 - accuracy: 0.6416 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6384 - accuracy: 0.6413 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6414 - accuracy: 0.6418 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6401 - accuracy: 0.6430 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6407 - accuracy: 0.6416 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6394 - accuracy: 0.6416 - val_loss: 0.6376 - val_accuracy: 0.6450\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6410 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6400 - accuracy: 0.6416 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6404 - accuracy: 0.6407 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6401 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6396 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6408 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6399 - accuracy: 0.6411 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6416 - accuracy: 0.6421 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6392 - accuracy: 0.6418 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6407 - accuracy: 0.6396 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6403 - accuracy: 0.6411 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6423 - accuracy: 0.6409 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6404 - accuracy: 0.6416 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6423 - accuracy: 0.6413 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6384 - accuracy: 0.6421 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6404 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6387 - accuracy: 0.6414 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6391 - accuracy: 0.6413 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6380 - accuracy: 0.6421 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6400 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6393 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 53us/sample - loss: 0.6404 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6406 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6397 - accuracy: 0.6414 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6378 - accuracy: 0.6427 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 51us/sample - loss: 0.6413 - accuracy: 0.6414 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6384 - accuracy: 0.6407 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6355 - accuracy: 0.6407 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6393 - accuracy: 0.6416 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6402 - accuracy: 0.6411 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6410 - accuracy: 0.6373 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6391 - accuracy: 0.6413 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6401 - accuracy: 0.6407 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6401 - accuracy: 0.6409 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 324/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6385 - accuracy: 0.6421 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 325/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6384 - accuracy: 0.6413 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6389 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6380 - accuracy: 0.6420 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6374 - accuracy: 0.6418 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6391 - accuracy: 0.6434 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6398 - accuracy: 0.6432 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6395 - accuracy: 0.6409 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6414 - accuracy: 0.6407 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6386 - accuracy: 0.6425 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6388 - accuracy: 0.6423 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6383 - accuracy: 0.6413 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6400 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6395 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6391 - accuracy: 0.6421 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6358 - accuracy: 0.6405 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6394 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6401 - accuracy: 0.6407 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6397 - accuracy: 0.6414 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6391 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6388 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6369 - accuracy: 0.6432 - val_loss: 0.6377 - val_accuracy: 0.6454\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6412 - accuracy: 0.6402 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6384 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6376 - accuracy: 0.6418 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6380 - accuracy: 0.6418 - val_loss: 0.6370 - val_accuracy: 0.6454\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6381 - accuracy: 0.6413 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6376 - accuracy: 0.6416 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6374 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6375 - accuracy: 0.6425 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6363 - accuracy: 0.6432 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6372 - accuracy: 0.6404 - val_loss: 0.6346 - val_accuracy: 0.6450\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6384 - accuracy: 0.6409 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6396 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6394 - accuracy: 0.6409 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6371 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6405 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6379 - accuracy: 0.6416 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6388 - accuracy: 0.6416 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6371 - accuracy: 0.6423 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6374 - accuracy: 0.6427 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6383 - accuracy: 0.6421 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6367 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6369 - accuracy: 0.6425 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6401 - accuracy: 0.6402 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6385 - accuracy: 0.6423 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 34us/sample - loss: 0.6376 - accuracy: 0.6427 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6375 - accuracy: 0.6407 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6393 - accuracy: 0.6423 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6411 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6386 - accuracy: 0.6414 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 378/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6370 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6389 - accuracy: 0.6407 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6390 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6358 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6382 - accuracy: 0.6414 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6370 - accuracy: 0.6427 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6392 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6378 - accuracy: 0.6413 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6384 - accuracy: 0.6418 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6377 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6379 - accuracy: 0.6411 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6371 - accuracy: 0.6427 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6387 - accuracy: 0.6413 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6393 - accuracy: 0.6413 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6413 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6368 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6398 - accuracy: 0.6416 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6389 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6349 - accuracy: 0.6420 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6367 - accuracy: 0.6414 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6377 - accuracy: 0.6418 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6383 - accuracy: 0.6414 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6373 - accuracy: 0.6425 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6371 - accuracy: 0.6416 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6375 - accuracy: 0.6429 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6376 - accuracy: 0.6421 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6378 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6371 - accuracy: 0.6418 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6395 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6350 - accuracy: 0.6420 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6383 - accuracy: 0.6423 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 0.6348 - accuracy: 0.6407 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6380 - accuracy: 0.6421 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 37us/sample - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6371 - accuracy: 0.6398 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6367 - accuracy: 0.6400 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6369 - accuracy: 0.6391 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6356 - accuracy: 0.6438 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 0.6375 - accuracy: 0.6391 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 0.6352 - accuracy: 0.6411 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 43us/sample - loss: 0.6362 - accuracy: 0.6402 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6398 - accuracy: 0.6414 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6396 - accuracy: 0.6420 - val_loss: 0.6322 - val_accuracy: 0.6454\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6392 - accuracy: 0.6429 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6379 - accuracy: 0.6418 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6377 - accuracy: 0.6416 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6357 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6340 - accuracy: 0.6434 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6357 - accuracy: 0.6400 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6373 - accuracy: 0.6418 - val_loss: 0.6321 - val_accuracy: 0.6454\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 0.6374 - accuracy: 0.6438 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6391 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 0s 46us/sample - loss: 0.6367 - accuracy: 0.6413 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 432/1000\n",
      "5600/5600 [==============================] - 0s 50us/sample - loss: 0.6334 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 433/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 62us/sample - loss: 0.6365 - accuracy: 0.6420 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6399 - accuracy: 0.6400 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 42us/sample - loss: 0.6384 - accuracy: 0.6416 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6359 - accuracy: 0.6413 - val_loss: 0.6365 - val_accuracy: 0.6454\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6404 - accuracy: 0.6402 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 46us/sample - loss: 0.6365 - accuracy: 0.6421 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 47us/sample - loss: 0.6366 - accuracy: 0.6418 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 0s 50us/sample - loss: 0.6337 - accuracy: 0.6429 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 36us/sample - loss: 0.6360 - accuracy: 0.6438 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6364 - accuracy: 0.6407 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6374 - accuracy: 0.6409 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6373 - accuracy: 0.6416 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6378 - accuracy: 0.6407 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6377 - accuracy: 0.6411 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6372 - accuracy: 0.6395 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6361 - accuracy: 0.6416 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6350 - accuracy: 0.6414 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6393 - accuracy: 0.6418 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6367 - accuracy: 0.6413 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6370 - accuracy: 0.6400 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6370 - accuracy: 0.6423 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6380 - accuracy: 0.6436 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6351 - accuracy: 0.6429 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6368 - accuracy: 0.6411 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6326 - accuracy: 0.6434 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6371 - accuracy: 0.6396 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6363 - accuracy: 0.6425 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6365 - accuracy: 0.6452 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6347 - accuracy: 0.6420 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 0.6396 - accuracy: 0.6427 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 32us/sample - loss: 0.6369 - accuracy: 0.6432 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6352 - accuracy: 0.6405 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6386 - accuracy: 0.6420 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 30us/sample - loss: 0.6373 - accuracy: 0.6421 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6327 - accuracy: 0.6430 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6395 - accuracy: 0.6421 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6362 - accuracy: 0.6405 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 35us/sample - loss: 0.6340 - accuracy: 0.6398 - val_loss: 0.6376 - val_accuracy: 0.6454\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6358 - accuracy: 0.6421 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6368 - accuracy: 0.6421 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6371 - accuracy: 0.6400 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6379 - accuracy: 0.6413 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6348 - accuracy: 0.6430 - val_loss: 0.6346 - val_accuracy: 0.6454\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6390 - accuracy: 0.6414 - val_loss: 0.6388 - val_accuracy: 0.6454\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6362 - accuracy: 0.6420 - val_loss: 0.6346 - val_accuracy: 0.6458\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6356 - accuracy: 0.6389 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6414 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6363 - accuracy: 0.6423 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6367 - accuracy: 0.6398 - val_loss: 0.6374 - val_accuracy: 0.6454\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 27us/sample - loss: 0.6368 - accuracy: 0.6423 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 486/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6379 - accuracy: 0.6411 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 487/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 28us/sample - loss: 0.6366 - accuracy: 0.6407 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6346 - accuracy: 0.6416 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6390 - accuracy: 0.6416 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6361 - accuracy: 0.6434 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6388 - accuracy: 0.6405 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6375 - accuracy: 0.6416 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6365 - accuracy: 0.6427 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6402 - val_loss: 0.6319 - val_accuracy: 0.6454\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6362 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6356 - accuracy: 0.6414 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 29us/sample - loss: 0.6353 - accuracy: 0.6405 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6377 - accuracy: 0.6413 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6418 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6434 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6365 - accuracy: 0.6405 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6436 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6407 - val_loss: 0.6340 - val_accuracy: 0.6463\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6379 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6348 - accuracy: 0.6416 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6418 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6357 - accuracy: 0.6423 - val_loss: 0.6333 - val_accuracy: 0.6458\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6348 - accuracy: 0.6414 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6346 - accuracy: 0.6441 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6367 - accuracy: 0.6427 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6378 - accuracy: 0.6421 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6369 - accuracy: 0.6414 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6361 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6409 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6413 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6414 - val_loss: 0.6414 - val_accuracy: 0.6454\n",
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6420 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6420 - val_loss: 0.6320 - val_accuracy: 0.6454\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6375 - accuracy: 0.6405 - val_loss: 0.6371 - val_accuracy: 0.6454\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6378 - accuracy: 0.6405 - val_loss: 0.6354 - val_accuracy: 0.6454\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6423 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6383 - accuracy: 0.6407 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6411 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6432 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6372 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6432 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6425 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6418 - val_loss: 0.6368 - val_accuracy: 0.6454\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6425 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6429 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6372 - accuracy: 0.6416 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6409 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6425 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6414 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6375 - accuracy: 0.6418 - val_loss: 0.6352 - val_accuracy: 0.6454\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6413 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6360 - accuracy: 0.6423 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6418 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 540/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6365 - accuracy: 0.6404 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 541/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6349 - accuracy: 0.6421 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6445 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6404 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6420 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6404 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6381 - accuracy: 0.6405 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6360 - accuracy: 0.6407 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6349 - accuracy: 0.6432 - val_loss: 0.6334 - val_accuracy: 0.6454\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6414 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6356 - accuracy: 0.6436 - val_loss: 0.6331 - val_accuracy: 0.6454\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6421 - val_loss: 0.6317 - val_accuracy: 0.6454\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6405 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6416 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6357 - accuracy: 0.6404 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6421 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6358 - accuracy: 0.6409 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6335 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6355 - accuracy: 0.6396 - val_loss: 0.6328 - val_accuracy: 0.6450\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6359 - accuracy: 0.6438 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6407 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6373 - accuracy: 0.6382 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6351 - accuracy: 0.6413 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6416 - val_loss: 0.6332 - val_accuracy: 0.6454\n",
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6372 - accuracy: 0.6439 - val_loss: 0.6336 - val_accuracy: 0.6454\n",
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6354 - accuracy: 0.6425 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6420 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6427 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6328 - accuracy: 0.6413 - val_loss: 0.6343 - val_accuracy: 0.6454\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6414 - val_loss: 0.6366 - val_accuracy: 0.6454\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6445 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6381 - accuracy: 0.6436 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6371 - accuracy: 0.6414 - val_loss: 0.6336 - val_accuracy: 0.6458\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6377 - accuracy: 0.6407 - val_loss: 0.6361 - val_accuracy: 0.6454\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6356 - accuracy: 0.6420 - val_loss: 0.6316 - val_accuracy: 0.6454\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6421 - val_loss: 0.6329 - val_accuracy: 0.6454\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6370 - accuracy: 0.6405 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6349 - accuracy: 0.6429 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6421 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6432 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6438 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6365 - accuracy: 0.6427 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6421 - val_loss: 0.6353 - val_accuracy: 0.6454\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6393 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6358 - accuracy: 0.6423 - val_loss: 0.6404 - val_accuracy: 0.6454\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6413 - val_loss: 0.6380 - val_accuracy: 0.6454\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6330 - accuracy: 0.6441 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6339 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6420 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6427 - val_loss: 0.6344 - val_accuracy: 0.6454\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6382 - val_loss: 0.6318 - val_accuracy: 0.6458\n",
      "Epoch 594/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6425 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6438 - val_loss: 0.6339 - val_accuracy: 0.6450\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6430 - val_loss: 0.6354 - val_accuracy: 0.6450\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6420 - val_loss: 0.6378 - val_accuracy: 0.6450\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6411 - val_loss: 0.6372 - val_accuracy: 0.6454\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6352 - accuracy: 0.6421 - val_loss: 0.6350 - val_accuracy: 0.6454\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6367 - accuracy: 0.6402 - val_loss: 0.6331 - val_accuracy: 0.6450\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6423 - val_loss: 0.6351 - val_accuracy: 0.6454\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6425 - val_loss: 0.6326 - val_accuracy: 0.6450\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6421 - val_loss: 0.6389 - val_accuracy: 0.6450\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6429 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6325 - val_accuracy: 0.6450\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6402 - val_loss: 0.6335 - val_accuracy: 0.6450\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6355 - accuracy: 0.6443 - val_loss: 0.6337 - val_accuracy: 0.6458\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6452 - val_loss: 0.6318 - val_accuracy: 0.6454\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6427 - val_loss: 0.6342 - val_accuracy: 0.6450\n",
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6409 - val_loss: 0.6363 - val_accuracy: 0.6454\n",
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6416 - val_loss: 0.6348 - val_accuracy: 0.6450\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6405 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6434 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6400 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6393 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6423 - val_loss: 0.6324 - val_accuracy: 0.6454\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6425 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6400 - val_loss: 0.6347 - val_accuracy: 0.6450\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6342 - val_accuracy: 0.6454\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6340 - accuracy: 0.6411 - val_loss: 0.6358 - val_accuracy: 0.6450\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6418 - val_loss: 0.6358 - val_accuracy: 0.6454\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6421 - val_loss: 0.6341 - val_accuracy: 0.6454\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6359 - accuracy: 0.6379 - val_loss: 0.6347 - val_accuracy: 0.6454\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6427 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6317 - val_accuracy: 0.6454\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6427 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6423 - val_loss: 0.6320 - val_accuracy: 0.6450\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6343 - accuracy: 0.6402 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6396 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6330 - accuracy: 0.6421 - val_loss: 0.6323 - val_accuracy: 0.6454\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6454\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6441 - val_loss: 0.6327 - val_accuracy: 0.6454\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6425 - val_loss: 0.6347 - val_accuracy: 0.6450\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6441 - val_loss: 0.6360 - val_accuracy: 0.6454\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6441 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6441 - val_loss: 0.6340 - val_accuracy: 0.6454\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6370 - accuracy: 0.6418 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6446 - val_loss: 0.6349 - val_accuracy: 0.6450\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6425 - val_loss: 0.6330 - val_accuracy: 0.6446\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6395 - val_loss: 0.6381 - val_accuracy: 0.6454\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6363 - accuracy: 0.6404 - val_loss: 0.6364 - val_accuracy: 0.6454\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6366 - accuracy: 0.6416 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6446 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6405 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6409 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 648/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6356 - accuracy: 0.6429 - val_loss: 0.6338 - val_accuracy: 0.6454\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6320 - accuracy: 0.6421 - val_loss: 0.6325 - val_accuracy: 0.6454\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6411 - val_loss: 0.6363 - val_accuracy: 0.6450\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6405 - val_loss: 0.6334 - val_accuracy: 0.6450\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6429 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6393 - val_loss: 0.6350 - val_accuracy: 0.6450\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6398 - val_loss: 0.6359 - val_accuracy: 0.6454\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6436 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6350 - accuracy: 0.6423 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.6326 - val_accuracy: 0.6454\n",
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6314 - accuracy: 0.6429 - val_loss: 0.6350 - val_accuracy: 0.6450\n",
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6330 - accuracy: 0.6405 - val_loss: 0.6348 - val_accuracy: 0.6454\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6405 - val_loss: 0.6326 - val_accuracy: 0.6442\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6355 - accuracy: 0.6407 - val_loss: 0.6364 - val_accuracy: 0.6450\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6413 - val_loss: 0.6327 - val_accuracy: 0.6450\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6405 - val_loss: 0.6329 - val_accuracy: 0.6450\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6386 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6427 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6380 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6353 - accuracy: 0.6413 - val_loss: 0.6324 - val_accuracy: 0.6450\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6334 - accuracy: 0.6427 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6416 - val_loss: 0.6327 - val_accuracy: 0.6450\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6393 - val_loss: 0.6345 - val_accuracy: 0.6450\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6346 - accuracy: 0.6425 - val_loss: 0.6373 - val_accuracy: 0.6454\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6359 - accuracy: 0.6404 - val_loss: 0.6324 - val_accuracy: 0.6450\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6418 - val_loss: 0.6319 - val_accuracy: 0.6454\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6425 - val_loss: 0.6345 - val_accuracy: 0.6454\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6438 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6361 - accuracy: 0.6430 - val_loss: 0.6337 - val_accuracy: 0.6454\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6356 - accuracy: 0.6379 - val_loss: 0.6378 - val_accuracy: 0.6450\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6423 - val_loss: 0.6395 - val_accuracy: 0.6454\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6423 - val_loss: 0.6398 - val_accuracy: 0.6454\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6427 - val_loss: 0.6339 - val_accuracy: 0.6454\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6353 - accuracy: 0.6429 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6350 - accuracy: 0.6425 - val_loss: 0.6348 - val_accuracy: 0.6450\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6342 - accuracy: 0.6438 - val_loss: 0.6397 - val_accuracy: 0.6454\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6342 - accuracy: 0.6416 - val_loss: 0.6320 - val_accuracy: 0.6446\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6331 - accuracy: 0.6434 - val_loss: 0.6340 - val_accuracy: 0.6450\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6351 - accuracy: 0.6425 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6324 - accuracy: 0.6457 - val_loss: 0.6369 - val_accuracy: 0.6454\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6343 - accuracy: 0.6405 - val_loss: 0.6351 - val_accuracy: 0.6446\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6450 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6338 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6413 - val_loss: 0.6368 - val_accuracy: 0.6454\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6359 - accuracy: 0.6438 - val_loss: 0.6411 - val_accuracy: 0.6450\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6352 - accuracy: 0.6396 - val_loss: 0.6338 - val_accuracy: 0.6450\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6413 - val_loss: 0.6331 - val_accuracy: 0.6450\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6423 - val_loss: 0.6341 - val_accuracy: 0.6450\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6439 - val_loss: 0.6335 - val_accuracy: 0.6450\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6420 - val_loss: 0.6356 - val_accuracy: 0.6454\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6321 - val_accuracy: 0.6446\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6332 - accuracy: 0.6425 - val_loss: 0.6456 - val_accuracy: 0.6450\n",
      "Epoch 702/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6353 - accuracy: 0.6427 - val_loss: 0.6390 - val_accuracy: 0.6450\n",
      "Epoch 703/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6352 - accuracy: 0.6421 - val_loss: 0.6403 - val_accuracy: 0.6450\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6354 - accuracy: 0.6418 - val_loss: 0.6359 - val_accuracy: 0.6450\n",
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6364 - accuracy: 0.6368 - val_loss: 0.6337 - val_accuracy: 0.6446\n",
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6411 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6436 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6420 - val_loss: 0.6336 - val_accuracy: 0.6446\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6402 - val_loss: 0.6332 - val_accuracy: 0.6442\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6404 - val_loss: 0.6378 - val_accuracy: 0.6446\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6379 - accuracy: 0.6382 - val_loss: 0.6337 - val_accuracy: 0.6450\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6439 - val_loss: 0.6359 - val_accuracy: 0.6446\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6427 - val_loss: 0.6354 - val_accuracy: 0.6450\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6352 - accuracy: 0.6411 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6436 - val_loss: 0.6319 - val_accuracy: 0.6446\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6334 - accuracy: 0.6405 - val_loss: 0.6317 - val_accuracy: 0.6446\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6464 - val_loss: 0.6318 - val_accuracy: 0.6450\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6427 - val_loss: 0.6330 - val_accuracy: 0.6450\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6365 - accuracy: 0.6407 - val_loss: 0.6335 - val_accuracy: 0.6454\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6347 - accuracy: 0.6409 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6328 - accuracy: 0.6409 - val_loss: 0.6309 - val_accuracy: 0.6446\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6366 - accuracy: 0.6382 - val_loss: 0.6313 - val_accuracy: 0.6446\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6452 - val_loss: 0.6316 - val_accuracy: 0.6450\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6357 - accuracy: 0.6405 - val_loss: 0.6321 - val_accuracy: 0.6446\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6429 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6357 - accuracy: 0.6430 - val_loss: 0.6350 - val_accuracy: 0.6446\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6404 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6347 - accuracy: 0.6421 - val_loss: 0.6352 - val_accuracy: 0.6450\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6429 - val_loss: 0.6340 - val_accuracy: 0.6446\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6303 - accuracy: 0.6425 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6348 - accuracy: 0.6425 - val_loss: 0.6374 - val_accuracy: 0.6450\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6425 - val_loss: 0.6323 - val_accuracy: 0.6446\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6425 - val_loss: 0.6328 - val_accuracy: 0.6446\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6332 - accuracy: 0.6448 - val_loss: 0.6322 - val_accuracy: 0.6446\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6434 - val_loss: 0.6318 - val_accuracy: 0.6446\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6405 - val_loss: 0.6333 - val_accuracy: 0.6450\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6425 - val_loss: 0.6365 - val_accuracy: 0.6450\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6441 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6356 - accuracy: 0.6423 - val_loss: 0.6327 - val_accuracy: 0.6446\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6322 - accuracy: 0.6438 - val_loss: 0.6372 - val_accuracy: 0.6450\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6420 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6315 - accuracy: 0.6446 - val_loss: 0.6366 - val_accuracy: 0.6450\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6351 - accuracy: 0.6404 - val_loss: 0.6323 - val_accuracy: 0.6450\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6320 - accuracy: 0.6423 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6404 - val_loss: 0.6307 - val_accuracy: 0.6442\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6345 - accuracy: 0.6418 - val_loss: 0.6351 - val_accuracy: 0.6450\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6311 - accuracy: 0.6427 - val_loss: 0.6387 - val_accuracy: 0.6450\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6329 - accuracy: 0.6448 - val_loss: 0.6394 - val_accuracy: 0.6450\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6414 - val_loss: 0.6334 - val_accuracy: 0.6450\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6409 - val_loss: 0.6320 - val_accuracy: 0.6450\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6430 - val_loss: 0.6336 - val_accuracy: 0.6446\n",
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6343 - accuracy: 0.6436 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6317 - accuracy: 0.6430 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6398 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6427 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 756/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6389 - val_loss: 0.6358 - val_accuracy: 0.6450\n",
      "Epoch 757/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6320 - accuracy: 0.6411 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6425 - val_loss: 0.6360 - val_accuracy: 0.6442\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6326 - accuracy: 0.6446 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6429 - val_loss: 0.6357 - val_accuracy: 0.6442\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6360 - accuracy: 0.6434 - val_loss: 0.6353 - val_accuracy: 0.6442\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6420 - val_loss: 0.6334 - val_accuracy: 0.6442\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6414 - val_loss: 0.6342 - val_accuracy: 0.6446\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6457 - val_loss: 0.6333 - val_accuracy: 0.6446\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6452 - val_loss: 0.6397 - val_accuracy: 0.6450\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6357 - accuracy: 0.6450 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6405 - val_loss: 0.6322 - val_accuracy: 0.6454\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6441 - val_loss: 0.6343 - val_accuracy: 0.6450\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6389 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6420 - val_loss: 0.6377 - val_accuracy: 0.6450\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6328 - accuracy: 0.6418 - val_loss: 0.6317 - val_accuracy: 0.6446\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6434 - val_loss: 0.6383 - val_accuracy: 0.6446\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6400 - val_loss: 0.6357 - val_accuracy: 0.6446\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6407 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6420 - val_loss: 0.6332 - val_accuracy: 0.6446\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6443 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6334 - accuracy: 0.6463 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6407 - val_loss: 0.6347 - val_accuracy: 0.6446\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6344 - accuracy: 0.6421 - val_loss: 0.6343 - val_accuracy: 0.6446\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6445 - val_loss: 0.6327 - val_accuracy: 0.6446\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6411 - val_loss: 0.6440 - val_accuracy: 0.6450\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6436 - val_loss: 0.6330 - val_accuracy: 0.6442\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6340 - accuracy: 0.6427 - val_loss: 0.6355 - val_accuracy: 0.6446\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6296 - accuracy: 0.6450 - val_loss: 0.6322 - val_accuracy: 0.6442\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6348 - accuracy: 0.6438 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6321 - accuracy: 0.6420 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6334 - accuracy: 0.6418 - val_loss: 0.6325 - val_accuracy: 0.6442\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6351 - accuracy: 0.6409 - val_loss: 0.6370 - val_accuracy: 0.6446\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6356 - accuracy: 0.6430 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6327 - accuracy: 0.6400 - val_loss: 0.6366 - val_accuracy: 0.6450\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6350 - accuracy: 0.6398 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6379 - val_accuracy: 0.6446\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6353 - accuracy: 0.6405 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6425 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6386 - val_loss: 0.6376 - val_accuracy: 0.6450\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6346 - accuracy: 0.6423 - val_loss: 0.6355 - val_accuracy: 0.6450\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6441 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6327 - accuracy: 0.6416 - val_loss: 0.6373 - val_accuracy: 0.6442\n",
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6441 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6421 - val_loss: 0.6328 - val_accuracy: 0.6442\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6446\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6333 - accuracy: 0.6438 - val_loss: 0.6328 - val_accuracy: 0.6442\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6318 - accuracy: 0.6445 - val_loss: 0.6371 - val_accuracy: 0.6446\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6309 - accuracy: 0.6411 - val_loss: 0.6334 - val_accuracy: 0.6442\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6427 - val_loss: 0.6379 - val_accuracy: 0.6446\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6418 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6353 - accuracy: 0.6448 - val_loss: 0.6361 - val_accuracy: 0.6446\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6398 - val_loss: 0.6374 - val_accuracy: 0.6450\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.6448 - val_loss: 0.6425 - val_accuracy: 0.6446\n",
      "Epoch 810/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6434 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 811/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6322 - accuracy: 0.6441 - val_loss: 0.6322 - val_accuracy: 0.6442\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6470 - val_loss: 0.6417 - val_accuracy: 0.6450\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6429 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6432 - val_loss: 0.6340 - val_accuracy: 0.6438\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6438 - val_loss: 0.6360 - val_accuracy: 0.6446\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6434 - val_loss: 0.6458 - val_accuracy: 0.6450\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6337 - accuracy: 0.6413 - val_loss: 0.6321 - val_accuracy: 0.6442\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6396 - val_loss: 0.6381 - val_accuracy: 0.6446\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6398 - val_loss: 0.6350 - val_accuracy: 0.6446\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6310 - accuracy: 0.6421 - val_loss: 0.6368 - val_accuracy: 0.6446\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6331 - accuracy: 0.6432 - val_loss: 0.6393 - val_accuracy: 0.6450\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6405 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6358 - accuracy: 0.6395 - val_loss: 0.6417 - val_accuracy: 0.6442\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6363 - accuracy: 0.6430 - val_loss: 0.6409 - val_accuracy: 0.6446\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6429 - val_loss: 0.6364 - val_accuracy: 0.6446\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6418 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6414 - val_loss: 0.6417 - val_accuracy: 0.6446\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6312 - accuracy: 0.6432 - val_loss: 0.6357 - val_accuracy: 0.6446\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6393 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6340 - accuracy: 0.6398 - val_loss: 0.6354 - val_accuracy: 0.6438\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6320 - accuracy: 0.6395 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6414 - val_loss: 0.6369 - val_accuracy: 0.6438\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6347 - accuracy: 0.6434 - val_loss: 0.6332 - val_accuracy: 0.6438\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6420 - val_loss: 0.6384 - val_accuracy: 0.6442\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6306 - accuracy: 0.6438 - val_loss: 0.6320 - val_accuracy: 0.6442\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6318 - accuracy: 0.6420 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6308 - accuracy: 0.6445 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6306 - accuracy: 0.6452 - val_loss: 0.6339 - val_accuracy: 0.6442\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6423 - val_loss: 0.6359 - val_accuracy: 0.6446\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6404 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6350 - accuracy: 0.6404 - val_loss: 0.6341 - val_accuracy: 0.6446\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6454 - val_loss: 0.6365 - val_accuracy: 0.6438\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6341 - accuracy: 0.6430 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6326 - accuracy: 0.6382 - val_loss: 0.6335 - val_accuracy: 0.6446\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6342 - accuracy: 0.6373 - val_loss: 0.6390 - val_accuracy: 0.6446\n",
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6362 - accuracy: 0.6414 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6285 - accuracy: 0.6436 - val_loss: 0.6328 - val_accuracy: 0.6454\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6413 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6413 - val_loss: 0.6395 - val_accuracy: 0.6446\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6430 - val_loss: 0.6403 - val_accuracy: 0.6446\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6322 - accuracy: 0.6421 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6427 - val_loss: 0.6429 - val_accuracy: 0.6446\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6342 - accuracy: 0.6438 - val_loss: 0.6312 - val_accuracy: 0.6442\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6413 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6395 - val_loss: 0.6337 - val_accuracy: 0.6442\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6327 - accuracy: 0.6480 - val_loss: 0.6482 - val_accuracy: 0.6450\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6353 - accuracy: 0.6423 - val_loss: 0.6348 - val_accuracy: 0.6446\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6339 - accuracy: 0.6409 - val_loss: 0.6379 - val_accuracy: 0.6442\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6337 - accuracy: 0.6420 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6329 - accuracy: 0.6430 - val_loss: 0.6352 - val_accuracy: 0.6450\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6432 - val_loss: 0.6348 - val_accuracy: 0.6438\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6425 - val_loss: 0.6356 - val_accuracy: 0.6446\n",
      "Epoch 864/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6421 - val_loss: 0.6390 - val_accuracy: 0.6442\n",
      "Epoch 865/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6322 - accuracy: 0.6416 - val_loss: 0.6353 - val_accuracy: 0.6442\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6313 - accuracy: 0.6432 - val_loss: 0.6356 - val_accuracy: 0.6438\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6423 - val_loss: 0.6325 - val_accuracy: 0.6438\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6454 - val_loss: 0.6341 - val_accuracy: 0.6438\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6432 - val_loss: 0.6409 - val_accuracy: 0.6446\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6325 - accuracy: 0.6409 - val_loss: 0.6375 - val_accuracy: 0.6442\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6363 - accuracy: 0.6418 - val_loss: 0.6378 - val_accuracy: 0.6442\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6336 - accuracy: 0.6423 - val_loss: 0.6352 - val_accuracy: 0.6442\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6338 - accuracy: 0.6409 - val_loss: 0.6327 - val_accuracy: 0.6442\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6438 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6325 - accuracy: 0.6414 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6430 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6328 - accuracy: 0.6439 - val_loss: 0.6370 - val_accuracy: 0.6446\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6319 - accuracy: 0.6421 - val_loss: 0.6338 - val_accuracy: 0.6442\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6438 - val_loss: 0.6358 - val_accuracy: 0.6442\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6346 - accuracy: 0.6404 - val_loss: 0.6374 - val_accuracy: 0.6442\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6334 - accuracy: 0.6420 - val_loss: 0.6341 - val_accuracy: 0.6442\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6321 - accuracy: 0.6436 - val_loss: 0.6344 - val_accuracy: 0.6446\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6340 - accuracy: 0.6434 - val_loss: 0.6426 - val_accuracy: 0.6442\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6322 - accuracy: 0.6443 - val_loss: 0.6331 - val_accuracy: 0.6446\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6322 - accuracy: 0.6438 - val_loss: 0.6331 - val_accuracy: 0.6442\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6441 - val_loss: 0.6333 - val_accuracy: 0.6446\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6313 - accuracy: 0.6452 - val_loss: 0.6364 - val_accuracy: 0.6442\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6315 - accuracy: 0.6471 - val_loss: 0.6349 - val_accuracy: 0.6450\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6332 - accuracy: 0.6405 - val_loss: 0.6325 - val_accuracy: 0.6438\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6319 - accuracy: 0.6434 - val_loss: 0.6364 - val_accuracy: 0.6442\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6302 - accuracy: 0.6464 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6333 - accuracy: 0.6427 - val_loss: 0.6377 - val_accuracy: 0.6446\n",
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6342 - accuracy: 0.6423 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 23us/sample - loss: 0.6316 - accuracy: 0.6427 - val_loss: 0.6362 - val_accuracy: 0.6442\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 0s 33us/sample - loss: 0.6314 - accuracy: 0.6421 - val_loss: 0.6345 - val_accuracy: 0.6446\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6340 - accuracy: 0.6421 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6326 - accuracy: 0.6425 - val_loss: 0.6365 - val_accuracy: 0.6446\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6345 - accuracy: 0.6416 - val_loss: 0.6373 - val_accuracy: 0.6450\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6337 - accuracy: 0.6452 - val_loss: 0.6353 - val_accuracy: 0.6446\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6325 - accuracy: 0.6409 - val_loss: 0.6412 - val_accuracy: 0.6446\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6343 - accuracy: 0.6396 - val_loss: 0.6329 - val_accuracy: 0.6446\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6329 - accuracy: 0.6416 - val_loss: 0.6374 - val_accuracy: 0.6446\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6335 - accuracy: 0.6420 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 0s 31us/sample - loss: 0.6311 - accuracy: 0.6411 - val_loss: 0.6413 - val_accuracy: 0.6446\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6308 - accuracy: 0.6427 - val_loss: 0.6365 - val_accuracy: 0.6442\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6429 - val_loss: 0.6415 - val_accuracy: 0.6446\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6329 - accuracy: 0.6427 - val_loss: 0.6359 - val_accuracy: 0.6450\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6295 - accuracy: 0.6418 - val_loss: 0.6396 - val_accuracy: 0.6446\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6305 - accuracy: 0.6429 - val_loss: 0.6382 - val_accuracy: 0.6446\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6330 - accuracy: 0.6411 - val_loss: 0.6367 - val_accuracy: 0.6446\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 24us/sample - loss: 0.6307 - accuracy: 0.6464 - val_loss: 0.6385 - val_accuracy: 0.6446\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6300 - accuracy: 0.6432 - val_loss: 0.6372 - val_accuracy: 0.6442\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6314 - accuracy: 0.6438 - val_loss: 0.6400 - val_accuracy: 0.6446\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6341 - accuracy: 0.6402 - val_loss: 0.6345 - val_accuracy: 0.6442\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6423 - val_loss: 0.6394 - val_accuracy: 0.6442\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6322 - accuracy: 0.6430 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 918/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6445 - val_loss: 0.6413 - val_accuracy: 0.6446\n",
      "Epoch 919/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6323 - accuracy: 0.6427 - val_loss: 0.6420 - val_accuracy: 0.6446\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6307 - accuracy: 0.6423 - val_loss: 0.6350 - val_accuracy: 0.6442\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6407 - val_loss: 0.6381 - val_accuracy: 0.6446\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6330 - accuracy: 0.6450 - val_loss: 0.6436 - val_accuracy: 0.6446\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6413 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6309 - accuracy: 0.6452 - val_loss: 0.6397 - val_accuracy: 0.6438\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6300 - accuracy: 0.6434 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6402 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6443 - val_loss: 0.6340 - val_accuracy: 0.6438\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6306 - accuracy: 0.6414 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6338 - accuracy: 0.6434 - val_loss: 0.6406 - val_accuracy: 0.6442\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6335 - accuracy: 0.6438 - val_loss: 0.6354 - val_accuracy: 0.6446\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6348 - accuracy: 0.6404 - val_loss: 0.6366 - val_accuracy: 0.6442\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6324 - accuracy: 0.6443 - val_loss: 0.6351 - val_accuracy: 0.6438\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6302 - accuracy: 0.6450 - val_loss: 0.6355 - val_accuracy: 0.6442\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6319 - accuracy: 0.6457 - val_loss: 0.6428 - val_accuracy: 0.6446\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6314 - accuracy: 0.6425 - val_loss: 0.6333 - val_accuracy: 0.6442\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6404 - val_loss: 0.6388 - val_accuracy: 0.6442\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6427 - val_loss: 0.6415 - val_accuracy: 0.6442\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6438 - val_loss: 0.6356 - val_accuracy: 0.6442\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6311 - accuracy: 0.6423 - val_loss: 0.6421 - val_accuracy: 0.6446\n",
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6302 - accuracy: 0.6389 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6325 - accuracy: 0.6427 - val_loss: 0.6323 - val_accuracy: 0.6442\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6445 - val_loss: 0.6446 - val_accuracy: 0.6446\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6336 - accuracy: 0.6421 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6327 - accuracy: 0.6416 - val_loss: 0.6324 - val_accuracy: 0.6446\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6434 - val_loss: 0.6417 - val_accuracy: 0.6446\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6303 - accuracy: 0.6427 - val_loss: 0.6343 - val_accuracy: 0.6442\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6328 - accuracy: 0.6413 - val_loss: 0.6348 - val_accuracy: 0.6442\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6284 - accuracy: 0.6432 - val_loss: 0.6335 - val_accuracy: 0.6442\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6343 - accuracy: 0.6420 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6308 - accuracy: 0.6432 - val_loss: 0.6375 - val_accuracy: 0.6450\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6315 - accuracy: 0.6505 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6288 - accuracy: 0.6455 - val_loss: 0.6364 - val_accuracy: 0.6446\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6324 - accuracy: 0.6457 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6310 - accuracy: 0.6430 - val_loss: 0.6369 - val_accuracy: 0.6446\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6318 - accuracy: 0.6420 - val_loss: 0.6360 - val_accuracy: 0.6446\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6327 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6446\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6301 - accuracy: 0.6430 - val_loss: 0.6383 - val_accuracy: 0.6446\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6344 - accuracy: 0.6414 - val_loss: 0.6333 - val_accuracy: 0.6438\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6318 - accuracy: 0.6430 - val_loss: 0.6342 - val_accuracy: 0.6442\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6316 - accuracy: 0.6429 - val_loss: 0.6401 - val_accuracy: 0.6450\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6315 - accuracy: 0.6423 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6326 - accuracy: 0.6432 - val_loss: 0.6409 - val_accuracy: 0.6450\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6331 - accuracy: 0.6439 - val_loss: 0.6416 - val_accuracy: 0.6442\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 18us/sample - loss: 0.6333 - accuracy: 0.6414 - val_loss: 0.6340 - val_accuracy: 0.6442\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6461 - val_loss: 0.6372 - val_accuracy: 0.6442\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6308 - accuracy: 0.6425 - val_loss: 0.6354 - val_accuracy: 0.6442\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6321 - accuracy: 0.6448 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6316 - accuracy: 0.6461 - val_loss: 0.6411 - val_accuracy: 0.6446\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6304 - accuracy: 0.6441 - val_loss: 0.6349 - val_accuracy: 0.6442\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6304 - accuracy: 0.6441 - val_loss: 0.6330 - val_accuracy: 0.6442\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6323 - accuracy: 0.6411 - val_loss: 0.6357 - val_accuracy: 0.6442\n",
      "Epoch 972/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6298 - accuracy: 0.6427 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 973/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6324 - accuracy: 0.6425 - val_loss: 0.6441 - val_accuracy: 0.6446\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6299 - accuracy: 0.6466 - val_loss: 0.6362 - val_accuracy: 0.6446\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6339 - accuracy: 0.6407 - val_loss: 0.6394 - val_accuracy: 0.6446\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6282 - accuracy: 0.6457 - val_loss: 0.6408 - val_accuracy: 0.6446\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6312 - accuracy: 0.6438 - val_loss: 0.6381 - val_accuracy: 0.6442\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 17us/sample - loss: 0.6311 - accuracy: 0.6407 - val_loss: 0.6396 - val_accuracy: 0.6446\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6309 - accuracy: 0.6425 - val_loss: 0.6374 - val_accuracy: 0.6442\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6352 - accuracy: 0.6446 - val_loss: 0.6390 - val_accuracy: 0.6446\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 22us/sample - loss: 0.6324 - accuracy: 0.6413 - val_loss: 0.6401 - val_accuracy: 0.6446\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6332 - accuracy: 0.6427 - val_loss: 0.6403 - val_accuracy: 0.6450\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 16us/sample - loss: 0.6331 - accuracy: 0.6429 - val_loss: 0.6398 - val_accuracy: 0.6446\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6348 - accuracy: 0.6427 - val_loss: 0.6362 - val_accuracy: 0.6438\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6330 - accuracy: 0.6436 - val_loss: 0.6399 - val_accuracy: 0.6442\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6327 - accuracy: 0.6413 - val_loss: 0.6382 - val_accuracy: 0.6446\n",
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 0s 19us/sample - loss: 0.6324 - accuracy: 0.6439 - val_loss: 0.6418 - val_accuracy: 0.6446\n",
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6302 - accuracy: 0.6425 - val_loss: 0.6344 - val_accuracy: 0.6442\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6301 - accuracy: 0.6482 - val_loss: 0.6367 - val_accuracy: 0.6446\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6317 - accuracy: 0.6446 - val_loss: 0.6504 - val_accuracy: 0.6446\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.64 - 0s 20us/sample - loss: 0.6299 - accuracy: 0.6459 - val_loss: 0.6439 - val_accuracy: 0.6442\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6332 - accuracy: 0.6414 - val_loss: 0.6516 - val_accuracy: 0.6446\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 21us/sample - loss: 0.6303 - accuracy: 0.6434 - val_loss: 0.6329 - val_accuracy: 0.6433\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6301 - accuracy: 0.6443 - val_loss: 0.6404 - val_accuracy: 0.6438\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 20us/sample - loss: 0.6292 - accuracy: 0.6432 - val_loss: 0.6359 - val_accuracy: 0.6438\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 45us/sample - loss: 0.6322 - accuracy: 0.6411 - val_loss: 0.6445 - val_accuracy: 0.6442\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 38us/sample - loss: 0.6316 - accuracy: 0.6409 - val_loss: 0.6392 - val_accuracy: 0.6446\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 40us/sample - loss: 0.6296 - accuracy: 0.6466 - val_loss: 0.6395 - val_accuracy: 0.6442\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 25us/sample - loss: 0.6307 - accuracy: 0.6425 - val_loss: 0.6408 - val_accuracy: 0.6446\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 26us/sample - loss: 0.6319 - accuracy: 0.6438 - val_loss: 0.6414 - val_accuracy: 0.6446\n"
     ]
    }
   ],
   "source": [
    "# specify network layers\n",
    "binary_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (13, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# compile and fit network\n",
    "binary_ann.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "history = binary_ann.fit(X_train, y_train, epochs = 1000, batch_size = 128, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs. Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = pd.read_csv('data/songs_10000.csv')\n",
    "\n",
    "# drop additional index column\n",
    "songs_df = songs_df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "songs_df_clean = songs_df.drop(columns = ['Artist', 'Track Name', 'Track ID'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(songs_df_clean.loc[:, songs_df_clean.columns != 'Popularity'], \n",
    "                                                    songs_df_clean.Popularity, test_size = 0.2, \n",
    "                                                    random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# cat_cols = ['Key', 'Time Signature']\n",
    "# X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "# X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "# num_features = X_train_num.columns.tolist()\n",
    "# num_index_train = X_train.index.tolist()\n",
    "# num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "\n",
    "# scaler = MinMaxScaler().fit(X_train_num)\n",
    "# X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "# X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "# X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)\n",
    "\n",
    "cat_cols = ['Key', 'Time Signature', 'Mode']\n",
    "X_train_num = X_train.drop(cat_cols, axis = 1)\n",
    "X_test_num = X_test.drop(cat_cols, axis = 1)\n",
    "num_features = X_train_num.columns.tolist()\n",
    "num_index_train = X_train.index.tolist()\n",
    "num_index_test = X_test.index.tolist()\n",
    "\n",
    "# X_train_dum = pd.get_dummies(X_train[cat_cols], columns = cat_cols)\n",
    "# X_test_dum = pd.get_dummies(X_test[cat_cols], columns = cat_cols)\n",
    "X_train_dum = X_train[cat_cols]\n",
    "X_test_dum = X_test[cat_cols]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_num), index = num_index_train, columns = num_features)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_num), index = num_index_test, columns = num_features)\n",
    "\n",
    "X_train = pd.concat([X_train_dum, X_train_scaled], axis = 1)\n",
    "X_test = pd.concat([X_test_dum, X_test_scaled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Time Signature</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Duration_ms</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.239920</td>\n",
       "      <td>0.485962</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.833175</td>\n",
       "      <td>0.265010</td>\n",
       "      <td>0.568203</td>\n",
       "      <td>0.432074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724899</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.105684</td>\n",
       "      <td>0.462915</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.061393</td>\n",
       "      <td>0.821867</td>\n",
       "      <td>0.283644</td>\n",
       "      <td>0.430502</td>\n",
       "      <td>0.222676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764056</td>\n",
       "      <td>0.626396</td>\n",
       "      <td>0.301020</td>\n",
       "      <td>0.248482</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.098395</td>\n",
       "      <td>0.670088</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>0.632278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045681</td>\n",
       "      <td>0.829442</td>\n",
       "      <td>0.268965</td>\n",
       "      <td>0.665324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048676</td>\n",
       "      <td>0.825085</td>\n",
       "      <td>0.059627</td>\n",
       "      <td>0.445949</td>\n",
       "      <td>0.208376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462850</td>\n",
       "      <td>0.948223</td>\n",
       "      <td>0.246726</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.066046</td>\n",
       "      <td>0.522910</td>\n",
       "      <td>0.554648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027909</td>\n",
       "      <td>0.626396</td>\n",
       "      <td>0.270068</td>\n",
       "      <td>0.733462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109860</td>\n",
       "      <td>0.857285</td>\n",
       "      <td>0.028778</td>\n",
       "      <td>0.386085</td>\n",
       "      <td>0.738509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.788832</td>\n",
       "      <td>0.296175</td>\n",
       "      <td>0.658310</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.076715</td>\n",
       "      <td>0.864151</td>\n",
       "      <td>0.245342</td>\n",
       "      <td>0.863212</td>\n",
       "      <td>0.250255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086243</td>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.360517</td>\n",
       "      <td>0.834666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.857444</td>\n",
       "      <td>0.124224</td>\n",
       "      <td>0.495577</td>\n",
       "      <td>0.718080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099195</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.193043</td>\n",
       "      <td>0.659312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>0.893950</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>0.635977</td>\n",
       "      <td>0.331971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.890355</td>\n",
       "      <td>0.296823</td>\n",
       "      <td>0.559109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057223</td>\n",
       "      <td>0.899003</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.713438</td>\n",
       "      <td>0.503575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.494416</td>\n",
       "      <td>0.314166</td>\n",
       "      <td>0.953907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147384</td>\n",
       "      <td>0.909585</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>0.572438</td>\n",
       "      <td>0.336057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.422335</td>\n",
       "      <td>0.328477</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.089014</td>\n",
       "      <td>0.849241</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.680230</td>\n",
       "      <td>0.134831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.113452</td>\n",
       "      <td>0.861929</td>\n",
       "      <td>0.190791</td>\n",
       "      <td>0.583158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138003</td>\n",
       "      <td>0.864763</td>\n",
       "      <td>0.221532</td>\n",
       "      <td>0.454564</td>\n",
       "      <td>0.527068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026905</td>\n",
       "      <td>0.748223</td>\n",
       "      <td>0.220057</td>\n",
       "      <td>0.908816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027621</td>\n",
       "      <td>0.911240</td>\n",
       "      <td>0.042961</td>\n",
       "      <td>0.581516</td>\n",
       "      <td>0.849847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385541</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.335932</td>\n",
       "      <td>0.655304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379821</td>\n",
       "      <td>0.855404</td>\n",
       "      <td>0.308489</td>\n",
       "      <td>0.681280</td>\n",
       "      <td>0.689479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507027</td>\n",
       "      <td>0.498477</td>\n",
       "      <td>0.340111</td>\n",
       "      <td>0.384757</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.836370</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.445645</td>\n",
       "      <td>0.113381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145580</td>\n",
       "      <td>0.699492</td>\n",
       "      <td>0.226768</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.457995</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.070186</td>\n",
       "      <td>0.568449</td>\n",
       "      <td>0.664964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511043</td>\n",
       "      <td>0.536041</td>\n",
       "      <td>0.341775</td>\n",
       "      <td>0.751498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.912848</td>\n",
       "      <td>0.112836</td>\n",
       "      <td>0.798391</td>\n",
       "      <td>0.546476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.846701</td>\n",
       "      <td>0.324166</td>\n",
       "      <td>0.622237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058683</td>\n",
       "      <td>0.870451</td>\n",
       "      <td>0.084990</td>\n",
       "      <td>0.636305</td>\n",
       "      <td>0.626149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112448</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.294249</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119241</td>\n",
       "      <td>0.930093</td>\n",
       "      <td>0.079089</td>\n",
       "      <td>0.636459</td>\n",
       "      <td>0.402451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266063</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>0.268431</td>\n",
       "      <td>0.438867</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.065145</td>\n",
       "      <td>0.823182</td>\n",
       "      <td>0.122153</td>\n",
       "      <td>0.654333</td>\n",
       "      <td>0.157303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655622</td>\n",
       "      <td>0.746193</td>\n",
       "      <td>0.192221</td>\n",
       "      <td>0.526043</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.943349</td>\n",
       "      <td>0.045652</td>\n",
       "      <td>0.727613</td>\n",
       "      <td>0.376915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752007</td>\n",
       "      <td>0.339086</td>\n",
       "      <td>0.279566</td>\n",
       "      <td>0.440871</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.225558</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.044928</td>\n",
       "      <td>0.365227</td>\n",
       "      <td>0.384065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112448</td>\n",
       "      <td>0.703553</td>\n",
       "      <td>0.352779</td>\n",
       "      <td>0.489970</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.787627</td>\n",
       "      <td>0.093271</td>\n",
       "      <td>0.399416</td>\n",
       "      <td>0.444331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530119</td>\n",
       "      <td>0.760406</td>\n",
       "      <td>0.244590</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>0.074431</td>\n",
       "      <td>0.545963</td>\n",
       "      <td>0.284985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895582</td>\n",
       "      <td>0.450761</td>\n",
       "      <td>0.438794</td>\n",
       "      <td>0.210405</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.854075</td>\n",
       "      <td>0.704034</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.640344</td>\n",
       "      <td>0.251277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.162437</td>\n",
       "      <td>0.048646</td>\n",
       "      <td>0.407804</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.596623</td>\n",
       "      <td>0.221867</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.597395</td>\n",
       "      <td>0.028703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946787</td>\n",
       "      <td>0.189848</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.820638</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.365942</td>\n",
       "      <td>0.049068</td>\n",
       "      <td>0.600189</td>\n",
       "      <td>0.029213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.470882</td>\n",
       "      <td>0.785787</td>\n",
       "      <td>0.189842</td>\n",
       "      <td>0.514018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246404</td>\n",
       "      <td>0.841650</td>\n",
       "      <td>0.091304</td>\n",
       "      <td>0.363468</td>\n",
       "      <td>0.365679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.717766</td>\n",
       "      <td>0.248573</td>\n",
       "      <td>0.138259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>0.781033</td>\n",
       "      <td>0.044410</td>\n",
       "      <td>0.385845</td>\n",
       "      <td>0.603677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047689</td>\n",
       "      <td>0.669036</td>\n",
       "      <td>0.297758</td>\n",
       "      <td>0.498988</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.789452</td>\n",
       "      <td>0.821845</td>\n",
       "      <td>0.061801</td>\n",
       "      <td>0.375436</td>\n",
       "      <td>0.813075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097990</td>\n",
       "      <td>0.502538</td>\n",
       "      <td>0.300819</td>\n",
       "      <td>0.751498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067855</td>\n",
       "      <td>0.906322</td>\n",
       "      <td>0.068944</td>\n",
       "      <td>0.718422</td>\n",
       "      <td>0.517875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851405</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.179819</td>\n",
       "      <td>0.283553</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.182823</td>\n",
       "      <td>0.791072</td>\n",
       "      <td>0.034369</td>\n",
       "      <td>0.349970</td>\n",
       "      <td>0.280899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142568</td>\n",
       "      <td>0.747208</td>\n",
       "      <td>0.237775</td>\n",
       "      <td>0.484960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426725</td>\n",
       "      <td>0.815613</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.470243</td>\n",
       "      <td>0.218590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.842640</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.356890</td>\n",
       "      <td>0.682257</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.476831</td>\n",
       "      <td>0.193054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.273739</td>\n",
       "      <td>0.737470</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.883141</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>0.536177</td>\n",
       "      <td>0.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968875</td>\n",
       "      <td>0.414213</td>\n",
       "      <td>0.161808</td>\n",
       "      <td>0.197379</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.602674</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.608726</td>\n",
       "      <td>0.639428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.522087</td>\n",
       "      <td>0.581726</td>\n",
       "      <td>0.280920</td>\n",
       "      <td>0.398786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.797575</td>\n",
       "      <td>0.118012</td>\n",
       "      <td>0.563715</td>\n",
       "      <td>0.445352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588352</td>\n",
       "      <td>0.658883</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.493978</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.049093</td>\n",
       "      <td>0.836710</td>\n",
       "      <td>0.113872</td>\n",
       "      <td>0.474977</td>\n",
       "      <td>0.282942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893574</td>\n",
       "      <td>0.696447</td>\n",
       "      <td>0.312803</td>\n",
       "      <td>0.332652</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.743485</td>\n",
       "      <td>0.046998</td>\n",
       "      <td>0.367989</td>\n",
       "      <td>0.076098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.617259</td>\n",
       "      <td>0.300821</td>\n",
       "      <td>0.157298</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.114030</td>\n",
       "      <td>0.739089</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.704665</td>\n",
       "      <td>0.153218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.372589</td>\n",
       "      <td>0.445712</td>\n",
       "      <td>0.272531</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>0.685565</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.544087</td>\n",
       "      <td>0.088662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.589848</td>\n",
       "      <td>0.278201</td>\n",
       "      <td>0.197379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.835531</td>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.592311</td>\n",
       "      <td>0.356486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072488</td>\n",
       "      <td>0.649746</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>0.676346</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.194288</td>\n",
       "      <td>0.896397</td>\n",
       "      <td>0.096894</td>\n",
       "      <td>0.367657</td>\n",
       "      <td>0.712972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.780711</td>\n",
       "      <td>0.164941</td>\n",
       "      <td>0.808613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>0.942737</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.374477</td>\n",
       "      <td>0.371808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.414213</td>\n",
       "      <td>0.101696</td>\n",
       "      <td>0.506002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151553</td>\n",
       "      <td>0.698754</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.819817</td>\n",
       "      <td>0.650664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056022</td>\n",
       "      <td>0.504569</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.210965</td>\n",
       "      <td>0.730138</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.793316</td>\n",
       "      <td>0.527068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405621</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.237512</td>\n",
       "      <td>0.510010</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.164061</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.063665</td>\n",
       "      <td>0.455695</td>\n",
       "      <td>0.330950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056122</td>\n",
       "      <td>0.862944</td>\n",
       "      <td>0.265895</td>\n",
       "      <td>0.552095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147384</td>\n",
       "      <td>0.846862</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>0.476908</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.248994</td>\n",
       "      <td>0.672081</td>\n",
       "      <td>0.144127</td>\n",
       "      <td>0.606205</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.130707</td>\n",
       "      <td>0.779968</td>\n",
       "      <td>0.671843</td>\n",
       "      <td>0.346998</td>\n",
       "      <td>0.708887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.281757</td>\n",
       "      <td>0.736468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.196687</td>\n",
       "      <td>0.685964</td>\n",
       "      <td>0.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.267324</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415260</td>\n",
       "      <td>0.896012</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.818023</td>\n",
       "      <td>0.876404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.089255</td>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.253191</td>\n",
       "      <td>0.547085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056077</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.156315</td>\n",
       "      <td>0.817119</td>\n",
       "      <td>0.410623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107428</td>\n",
       "      <td>0.705584</td>\n",
       "      <td>0.240472</td>\n",
       "      <td>0.640273</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.853660</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.409030</td>\n",
       "      <td>0.351379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964859</td>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.265317</td>\n",
       "      <td>0.396781</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.065875</td>\n",
       "      <td>0.771697</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>0.527176</td>\n",
       "      <td>0.686415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.531980</td>\n",
       "      <td>0.201019</td>\n",
       "      <td>0.140263</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.148426</td>\n",
       "      <td>0.506979</td>\n",
       "      <td>0.141822</td>\n",
       "      <td>0.612802</td>\n",
       "      <td>0.265577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.219878</td>\n",
       "      <td>0.467005</td>\n",
       "      <td>0.245797</td>\n",
       "      <td>0.498988</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.082760</td>\n",
       "      <td>0.866916</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.349397</td>\n",
       "      <td>0.157303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7960</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045881</td>\n",
       "      <td>0.722843</td>\n",
       "      <td>0.321652</td>\n",
       "      <td>0.661316</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.067125</td>\n",
       "      <td>0.810605</td>\n",
       "      <td>0.052795</td>\n",
       "      <td>0.620593</td>\n",
       "      <td>0.407559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.970558</td>\n",
       "      <td>0.253548</td>\n",
       "      <td>0.545081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>0.864695</td>\n",
       "      <td>0.085093</td>\n",
       "      <td>0.472565</td>\n",
       "      <td>0.129724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.405076</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.162308</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.086929</td>\n",
       "      <td>0.475074</td>\n",
       "      <td>0.046170</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.318693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Key  Time Signature  Mode  Acousticness  Danceability  Duration_ms    Energy  Instrumentalness  Liveness  Loudness  Speechiness     Tempo   Valence\n",
       "8018   11               4     0      0.010138      0.800000     0.239920  0.485962          0.000524  0.009589  0.833175     0.265010  0.568203  0.432074\n",
       "9225    6               4     1      0.724899      0.680203     0.105684  0.462915          0.000009  0.061393  0.821867     0.283644  0.430502  0.222676\n",
       "3854   11               4     1      0.764056      0.626396     0.301020  0.248482          0.599000  0.098395  0.670088     0.039130  0.735592  0.632278\n",
       "2029    1               4     1      0.045681      0.829442     0.268965  0.665324          0.000000  0.048676  0.825085     0.059627  0.445949  0.208376\n",
       "3539   11               4     1      0.462850      0.948223     0.246726  0.565122          0.000018  0.084845  0.901042     0.066046  0.522910  0.554648\n",
       "1942   11               4     0      0.027909      0.626396     0.270068  0.733462          0.000000  0.109860  0.857285     0.028778  0.386085  0.738509\n",
       "1250    1               4     1      0.000739      0.788832     0.296175  0.658310          0.000003  0.076715  0.864151     0.245342  0.863212  0.250255\n",
       "2817    1               4     0      0.086243      0.817259     0.360517  0.834666          0.000000  0.039817  0.857444     0.124224  0.495577  0.718080\n",
       "4211    2               4     1      0.099195      0.786802     0.193043  0.659312          0.000000  0.095268  0.893950     0.181159  0.635977  0.331971\n",
       "477     1               4     1      0.066263      0.890355     0.296823  0.559109          0.000000  0.057223  0.899003     0.354037  0.713438  0.503575\n",
       "7904    1               4     0      0.000227      0.494416     0.314166  0.953907          0.000000  0.147384  0.909585     0.063458  0.572438  0.336057\n",
       "4322    6               4     0      0.013853      0.422335     0.328477  0.565122          0.005670  0.089014  0.849241     0.033126  0.680230  0.134831\n",
       "770     8               4     1      0.113452      0.861929     0.190791  0.583158          0.000000  0.138003  0.864763     0.221532  0.454564  0.527068\n",
       "8272    8               4     1      0.026905      0.748223     0.220057  0.908816          0.000000  0.027621  0.911240     0.042961  0.581516  0.849847\n",
       "540     0               4     1      0.385541      0.734010     0.335932  0.655304          0.000000  0.379821  0.855404     0.308489  0.681280  0.689479\n",
       "1519    9               4     1      0.507027      0.498477     0.340111  0.384757          0.000051  0.096310  0.836370     0.026708  0.445645  0.113381\n",
       "5315    7               4     1      0.145580      0.699492     0.226768  0.902804          0.000010  0.457995  0.977521     0.070186  0.568449  0.664964\n",
       "7728    2               4     1      0.511043      0.536041     0.341775  0.751498          0.000000  0.268293  0.912848     0.112836  0.798391  0.546476\n",
       "61      7               4     1      0.092167      0.846701     0.324166  0.622237          0.000000  0.058683  0.870451     0.084990  0.636305  0.626149\n",
       "9851    9               4     0      0.112448      0.667005     0.294249  0.753502          0.000000  0.119241  0.930093     0.079089  0.636459  0.402451\n",
       "2987    1               4     0      0.266063      0.769543     0.268431  0.438867          0.002980  0.065145  0.823182     0.122153  0.654333  0.157303\n",
       "258     6               4     0      0.655622      0.746193     0.192221  0.526043          0.918000  0.084845  0.943349     0.045652  0.727613  0.376915\n",
       "2933    0               4     1      0.752007      0.339086     0.279566  0.440871          0.000607  0.225558  0.786585     0.044928  0.365227  0.384065\n",
       "6996    4               4     0      0.112448      0.703553     0.352779  0.489970          0.000465  0.064936  0.787627     0.093271  0.399416  0.444331\n",
       "3308    5               4     1      0.530119      0.760406     0.244590  0.419828          0.000008  0.097353  0.796714     0.074431  0.545963  0.284985\n",
       "3149    3               3     1      0.895582      0.450761     0.438794  0.210405          0.000021  0.854075  0.704034     0.036439  0.640344  0.251277\n",
       "8571    5               4     1      0.001383      0.162437     0.048646  0.407804          0.778000  0.596623  0.221867     0.053313  0.597395  0.028703\n",
       "4751    9               5     1      0.946787      0.189848     0.051920  0.820638          0.989000  0.096310  0.365942     0.049068  0.600189  0.029213\n",
       "4564   11               4     0      0.470882      0.785787     0.189842  0.514018          0.000000  0.246404  0.841650     0.091304  0.363468  0.365679\n",
       "7694    4               4     1      0.823293      0.717766     0.248573  0.138259          0.000000  0.120284  0.781033     0.044410  0.385845  0.603677\n",
       "...   ...             ...   ...           ...           ...          ...       ...               ...       ...       ...          ...       ...       ...\n",
       "9057    4               4     0      0.047689      0.669036     0.297758  0.498988          0.855000  0.789452  0.821845     0.061801  0.375436  0.813075\n",
       "8302    6               4     0      0.097990      0.502538     0.300819  0.751498          0.000000  0.067855  0.906322     0.068944  0.718422  0.517875\n",
       "8550    9               4     1      0.851405      0.659898     0.179819  0.283553          0.912000  0.182823  0.791072     0.034369  0.349970  0.280899\n",
       "9890    7               4     1      0.142568      0.747208     0.237775  0.484960          0.000000  0.426725  0.815613     0.452381  0.470243  0.218590\n",
       "2131    1               4     1      0.949799      0.842640     0.267715  0.159302          0.000020  0.356890  0.682257     0.039648  0.476831  0.193054\n",
       "1093   10               4     0      0.006102      0.613198     0.273739  0.737470          0.109000  0.094226  0.883141     0.057453  0.536177  0.352400\n",
       "8719   11               4     1      0.968875      0.414213     0.161808  0.197379          0.791000  0.094226  0.602674     0.054141  0.608726  0.639428\n",
       "5624    5               5     0      0.522087      0.581726     0.280920  0.398786          0.000000  0.097353  0.797575     0.118012  0.563715  0.445352\n",
       "303    11               4     0      0.588352      0.658883     0.305085  0.493978          0.000167  0.049093  0.836710     0.113872  0.474977  0.282942\n",
       "4066    9               4     1      0.893574      0.696447     0.312803  0.332652          0.342000  0.091099  0.743485     0.046998  0.367989  0.076098\n",
       "5056    0               3     1      0.843373      0.617259     0.300821  0.157298          0.000060  0.114030  0.739089     0.035300  0.704665  0.153218\n",
       "9664    7               3     0      0.925703      0.372589     0.445712  0.272531          0.580000  0.096310  0.685565     0.054141  0.544087  0.088662\n",
       "244     3               4     1      0.883534      0.589848     0.278201  0.197379          0.000000  0.091099  0.835531     0.034058  0.592311  0.356486\n",
       "3266    9               4     1      0.072488      0.649746     0.246092  0.676346          0.000001  0.194288  0.896397     0.096894  0.367657  0.712972\n",
       "707    11               4     1      0.051705      0.780711     0.164941  0.808613          0.000000  0.095268  0.942737     0.316770  0.374477  0.371808\n",
       "5358    2               4     1      0.262047      0.414213     0.101696  0.506002          0.000000  0.151553  0.698754     0.219462  0.819817  0.650664\n",
       "6980    9               3     0      0.056022      0.504569     0.268810  0.499990          0.730000  0.210965  0.730138     0.057971  0.793316  0.527068\n",
       "6408    5               4     0      0.405621      0.560406     0.237512  0.510010          0.000016  0.164061  0.805031     0.063665  0.455695  0.330950\n",
       "1664    0               4     1      0.056122      0.862944     0.265895  0.552095          0.000000  0.147384  0.846862     0.059213  0.476908  0.647600\n",
       "5655    6               4     1      0.248994      0.672081     0.144127  0.606205          0.013200  0.130707  0.779968     0.671843  0.346998  0.708887\n",
       "8628   11               4     0      0.008612      0.898477     0.281757  0.736468          0.000000  0.081301  0.854294     0.196687  0.685964  0.911134\n",
       "3053    8               4     0      0.100400      0.707614     0.267324  0.773543          0.000000  0.415260  0.896012     0.319876  0.818023  0.876404\n",
       "1275    8               4     1      0.089255      0.613198     0.253191  0.547085          0.000000  0.056077  0.880988     0.156315  0.817119  0.410623\n",
       "2602    7               4     0      0.107428      0.705584     0.240472  0.640273          0.000657  0.102564  0.853660     0.158385  0.409030  0.351379\n",
       "6654    0               4     1      0.964859      0.750254     0.265317  0.396781          0.320000  0.065875  0.771697     0.045342  0.527176  0.686415\n",
       "6923    4               4     0      0.897590      0.531980     0.201019  0.140263          0.000238  0.148426  0.506979     0.141822  0.612802  0.265577\n",
       "1207    5               4     1      0.219878      0.467005     0.245797  0.498988          0.003290  0.082760  0.866916     0.029193  0.349397  0.157303\n",
       "7960    1               3     1      0.045881      0.722843     0.321652  0.661316          0.000293  0.067125  0.810605     0.052795  0.620593  0.407559\n",
       "2339    1               4     1      0.002950      0.970558     0.253548  0.545081          0.000000  0.087972  0.864695     0.085093  0.472565  0.129724\n",
       "6637    7               4     0      0.993976      0.405076     0.248009  0.162308          0.903000  0.086929  0.475074     0.046170  0.891363  0.318693\n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 2726.4436 - mean_squared_error: 2726.4438 - val_loss: 1948.4693 - val_mean_squared_error: 1948.4694\n",
      "Epoch 2/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 1403.8449 - mean_squared_error: 1403.8446 - val_loss: 896.8678 - val_mean_squared_error: 896.8677\n",
      "Epoch 3/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 578.6293 - mean_squared_error: 578.6293 - val_loss: 320.3426 - val_mean_squared_error: 320.3426\n",
      "Epoch 4/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 204.4830 - mean_squared_error: 204.4830 - val_loss: 113.8485 - val_mean_squared_error: 113.8485\n",
      "Epoch 5/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 89.2697 - mean_squared_error: 89.2697 - val_loss: 67.1371 - val_mean_squared_error: 67.1371\n",
      "Epoch 6/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 65.8605 - mean_squared_error: 65.8605 - val_loss: 59.8493 - val_mean_squared_error: 59.8493\n",
      "Epoch 7/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 62.4462 - mean_squared_error: 62.4462 - val_loss: 59.1847 - val_mean_squared_error: 59.1847\n",
      "Epoch 8/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 62.0255 - mean_squared_error: 62.0255 - val_loss: 59.1610 - val_mean_squared_error: 59.1610\n",
      "Epoch 9/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 61.9554 - mean_squared_error: 61.9554 - val_loss: 59.1298 - val_mean_squared_error: 59.1298\n",
      "Epoch 10/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 61.8923 - mean_squared_error: 61.8924 - val_loss: 59.0471 - val_mean_squared_error: 59.0471\n",
      "Epoch 11/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 61.8337 - mean_squared_error: 61.8337 - val_loss: 59.0288 - val_mean_squared_error: 59.0288\n",
      "Epoch 12/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 61.7743 - mean_squared_error: 61.7744 - val_loss: 58.9756 - val_mean_squared_error: 58.9756\n",
      "Epoch 13/1000\n",
      "5600/5600 [==============================] - 0s 80us/sample - loss: 61.7282 - mean_squared_error: 61.7282 - val_loss: 58.9381 - val_mean_squared_error: 58.9381\n",
      "Epoch 14/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 61.6699 - mean_squared_error: 61.6700 - val_loss: 58.8680 - val_mean_squared_error: 58.8680\n",
      "Epoch 15/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 61.5991 - mean_squared_error: 61.5991 - val_loss: 58.8309 - val_mean_squared_error: 58.8309\n",
      "Epoch 16/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 61.5579 - mean_squared_error: 61.5579 - val_loss: 58.7977 - val_mean_squared_error: 58.7977\n",
      "Epoch 17/1000\n",
      "5600/5600 [==============================] - 0s 75us/sample - loss: 61.4807 - mean_squared_error: 61.4807 - val_loss: 58.6893 - val_mean_squared_error: 58.6893\n",
      "Epoch 18/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 61.4108 - mean_squared_error: 61.4108 - val_loss: 58.7038 - val_mean_squared_error: 58.7039\n",
      "Epoch 19/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 61.3458 - mean_squared_error: 61.3459 - val_loss: 58.5575 - val_mean_squared_error: 58.5575\n",
      "Epoch 20/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 61.2377 - mean_squared_error: 61.2377 - val_loss: 58.4982 - val_mean_squared_error: 58.4982\n",
      "Epoch 21/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 61.1210 - mean_squared_error: 61.1210 - val_loss: 58.5167 - val_mean_squared_error: 58.5167\n",
      "Epoch 22/1000\n",
      "5600/5600 [==============================] - 1s 105us/sample - loss: 61.0378 - mean_squared_error: 61.0378 - val_loss: 58.2432 - val_mean_squared_error: 58.2432\n",
      "Epoch 23/1000\n",
      "5600/5600 [==============================] - 1s 99us/sample - loss: 60.8402 - mean_squared_error: 60.8402 - val_loss: 58.1517 - val_mean_squared_error: 58.1517\n",
      "Epoch 24/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 60.7331 - mean_squared_error: 60.7331 - val_loss: 58.2532 - val_mean_squared_error: 58.2532\n",
      "Epoch 25/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 60.6396 - mean_squared_error: 60.6396 - val_loss: 58.0011 - val_mean_squared_error: 58.0011\n",
      "Epoch 26/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 60.5404 - mean_squared_error: 60.5404 - val_loss: 57.9714 - val_mean_squared_error: 57.9714\n",
      "Epoch 27/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 60.4714 - mean_squared_error: 60.4714 - val_loss: 57.7933 - val_mean_squared_error: 57.7933\n",
      "Epoch 28/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 60.3620 - mean_squared_error: 60.3619 - val_loss: 57.8436 - val_mean_squared_error: 57.8436\n",
      "Epoch 29/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 60.2719 - mean_squared_error: 60.2719 - val_loss: 57.6521 - val_mean_squared_error: 57.6521\n",
      "Epoch 30/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 60.1890 - mean_squared_error: 60.1890 - val_loss: 57.6744 - val_mean_squared_error: 57.6744\n",
      "Epoch 31/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 60.1581 - mean_squared_error: 60.1581 - val_loss: 57.6460 - val_mean_squared_error: 57.6460\n",
      "Epoch 32/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 60.0750 - mean_squared_error: 60.0750 - val_loss: 57.5372 - val_mean_squared_error: 57.5372\n",
      "Epoch 33/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 60.0218 - mean_squared_error: 60.0218 - val_loss: 57.4517 - val_mean_squared_error: 57.4517\n",
      "Epoch 34/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 59.9434 - mean_squared_error: 59.9434 - val_loss: 57.4985 - val_mean_squared_error: 57.4985\n",
      "Epoch 35/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.9113 - mean_squared_error: 59.9113 - val_loss: 57.4497 - val_mean_squared_error: 57.4497\n",
      "Epoch 36/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.8384 - mean_squared_error: 59.8384 - val_loss: 57.3504 - val_mean_squared_error: 57.3504\n",
      "Epoch 37/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 59.8638 - mean_squared_error: 59.8638 - val_loss: 57.3090 - val_mean_squared_error: 57.3090\n",
      "Epoch 38/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.7664 - mean_squared_error: 59.7664 - val_loss: 57.4313 - val_mean_squared_error: 57.4313\n",
      "Epoch 39/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.7445 - mean_squared_error: 59.7445 - val_loss: 57.5044 - val_mean_squared_error: 57.5044\n",
      "Epoch 40/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.7207 - mean_squared_error: 59.7207 - val_loss: 57.2180 - val_mean_squared_error: 57.2180\n",
      "Epoch 41/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.6620 - mean_squared_error: 59.6620 - val_loss: 57.2316 - val_mean_squared_error: 57.2316\n",
      "Epoch 42/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.5973 - mean_squared_error: 59.5973 - val_loss: 57.1985 - val_mean_squared_error: 57.1985\n",
      "Epoch 43/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.5743 - mean_squared_error: 59.5743 - val_loss: 57.1807 - val_mean_squared_error: 57.1807\n",
      "Epoch 44/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.5726 - mean_squared_error: 59.5726 - val_loss: 57.1896 - val_mean_squared_error: 57.1896\n",
      "Epoch 45/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.5474 - mean_squared_error: 59.5474 - val_loss: 57.0940 - val_mean_squared_error: 57.0940\n",
      "Epoch 46/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.5424 - mean_squared_error: 59.5424 - val_loss: 57.0609 - val_mean_squared_error: 57.0609\n",
      "Epoch 47/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.4930 - mean_squared_error: 59.4930 - val_loss: 57.0311 - val_mean_squared_error: 57.0311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.4878 - mean_squared_error: 59.4878 - val_loss: 57.0219 - val_mean_squared_error: 57.0219\n",
      "Epoch 49/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 59.4529 - mean_squared_error: 59.4529 - val_loss: 57.4685 - val_mean_squared_error: 57.4685\n",
      "Epoch 50/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.4765 - mean_squared_error: 59.4765 - val_loss: 57.0233 - val_mean_squared_error: 57.0233\n",
      "Epoch 51/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.4293 - mean_squared_error: 59.4293 - val_loss: 56.9664 - val_mean_squared_error: 56.9664\n",
      "Epoch 52/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 59.4153 - mean_squared_error: 59.4153 - val_loss: 57.0964 - val_mean_squared_error: 57.0963\n",
      "Epoch 53/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.4326 - mean_squared_error: 59.4326 - val_loss: 56.9455 - val_mean_squared_error: 56.9455\n",
      "Epoch 54/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.3967 - mean_squared_error: 59.3967 - val_loss: 56.9779 - val_mean_squared_error: 56.9779\n",
      "Epoch 55/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.3302 - mean_squared_error: 59.3302 - val_loss: 57.5251 - val_mean_squared_error: 57.5251\n",
      "Epoch 56/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.3419 - mean_squared_error: 59.3419 - val_loss: 56.8698 - val_mean_squared_error: 56.8698\n",
      "Epoch 57/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.3221 - mean_squared_error: 59.3221 - val_loss: 57.1916 - val_mean_squared_error: 57.1916\n",
      "Epoch 58/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.3557 - mean_squared_error: 59.3557 - val_loss: 56.8966 - val_mean_squared_error: 56.8966\n",
      "Epoch 59/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.3049 - mean_squared_error: 59.3049 - val_loss: 57.2101 - val_mean_squared_error: 57.2101\n",
      "Epoch 60/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.2438 - mean_squared_error: 59.2437 - val_loss: 56.8228 - val_mean_squared_error: 56.8228\n",
      "Epoch 61/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 59.2716 - mean_squared_error: 59.2716 - val_loss: 56.9589 - val_mean_squared_error: 56.9589\n",
      "Epoch 62/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 59.3210 - mean_squared_error: 59.3210 - val_loss: 56.8866 - val_mean_squared_error: 56.8866\n",
      "Epoch 63/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.2120 - mean_squared_error: 59.2120 - val_loss: 57.2567 - val_mean_squared_error: 57.2566\n",
      "Epoch 64/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.1908 - mean_squared_error: 59.1908 - val_loss: 56.8260 - val_mean_squared_error: 56.8260\n",
      "Epoch 65/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.1801 - mean_squared_error: 59.1801 - val_loss: 56.7205 - val_mean_squared_error: 56.7205\n",
      "Epoch 66/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.1941 - mean_squared_error: 59.1941 - val_loss: 56.7198 - val_mean_squared_error: 56.7198\n",
      "Epoch 67/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.1720 - mean_squared_error: 59.1720 - val_loss: 56.9019 - val_mean_squared_error: 56.9019\n",
      "Epoch 68/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.2049 - mean_squared_error: 59.2049 - val_loss: 56.6727 - val_mean_squared_error: 56.6727\n",
      "Epoch 69/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.1812 - mean_squared_error: 59.1812 - val_loss: 57.0647 - val_mean_squared_error: 57.0647\n",
      "Epoch 70/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.1406 - mean_squared_error: 59.1406 - val_loss: 56.6611 - val_mean_squared_error: 56.6610\n",
      "Epoch 71/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.1744 - mean_squared_error: 59.1744 - val_loss: 56.7182 - val_mean_squared_error: 56.7182\n",
      "Epoch 72/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.0798 - mean_squared_error: 59.0798 - val_loss: 56.7034 - val_mean_squared_error: 56.7034\n",
      "Epoch 73/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.0870 - mean_squared_error: 59.0870 - val_loss: 56.6151 - val_mean_squared_error: 56.6151\n",
      "Epoch 74/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.1457 - mean_squared_error: 59.1457 - val_loss: 56.6556 - val_mean_squared_error: 56.6556\n",
      "Epoch 75/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 59.0462 - mean_squared_error: 59.0462 - val_loss: 56.6254 - val_mean_squared_error: 56.6254\n",
      "Epoch 76/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 59.1350 - mean_squared_error: 59.1350 - val_loss: 56.6787 - val_mean_squared_error: 56.6787\n",
      "Epoch 77/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 59.1588 - mean_squared_error: 59.1588 - val_loss: 56.6961 - val_mean_squared_error: 56.6961\n",
      "Epoch 78/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 59.0371 - mean_squared_error: 59.0371 - val_loss: 56.6099 - val_mean_squared_error: 56.6099\n",
      "Epoch 79/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 59.0354 - mean_squared_error: 59.0354 - val_loss: 56.5242 - val_mean_squared_error: 56.5242\n",
      "Epoch 80/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.9844 - mean_squared_error: 58.9844 - val_loss: 56.5452 - val_mean_squared_error: 56.5452\n",
      "Epoch 81/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.9902 - mean_squared_error: 58.9902 - val_loss: 56.4856 - val_mean_squared_error: 56.4856\n",
      "Epoch 82/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.9396 - mean_squared_error: 58.9396 - val_loss: 56.8300 - val_mean_squared_error: 56.8300\n",
      "Epoch 83/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.9362 - mean_squared_error: 58.9362 - val_loss: 56.8886 - val_mean_squared_error: 56.8886\n",
      "Epoch 84/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.9436 - mean_squared_error: 58.9436 - val_loss: 56.8141 - val_mean_squared_error: 56.8141\n",
      "Epoch 85/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.9934 - mean_squared_error: 58.9934 - val_loss: 56.5259 - val_mean_squared_error: 56.5259\n",
      "Epoch 86/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.9784 - mean_squared_error: 58.9785 - val_loss: 56.4763 - val_mean_squared_error: 56.4763\n",
      "Epoch 87/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.8425 - mean_squared_error: 58.8424 - val_loss: 57.1639 - val_mean_squared_error: 57.1639\n",
      "Epoch 88/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.9668 - mean_squared_error: 58.9668 - val_loss: 56.5219 - val_mean_squared_error: 56.5219\n",
      "Epoch 89/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.9128 - mean_squared_error: 58.9129 - val_loss: 56.4461 - val_mean_squared_error: 56.4461\n",
      "Epoch 90/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.9407 - mean_squared_error: 58.9407 - val_loss: 56.3718 - val_mean_squared_error: 56.3718\n",
      "Epoch 91/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.8185 - mean_squared_error: 58.8185 - val_loss: 56.3598 - val_mean_squared_error: 56.3598\n",
      "Epoch 92/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 58.7863 - mean_squared_error: 58.7863 - val_loss: 56.6517 - val_mean_squared_error: 56.6517\n",
      "Epoch 93/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.7901 - mean_squared_error: 58.7901 - val_loss: 56.3376 - val_mean_squared_error: 56.3376\n",
      "Epoch 94/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.8335 - mean_squared_error: 58.8335 - val_loss: 56.4126 - val_mean_squared_error: 56.4126\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.8861 - mean_squared_error: 58.8861 - val_loss: 56.3274 - val_mean_squared_error: 56.3274\n",
      "Epoch 96/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.8099 - mean_squared_error: 58.8099 - val_loss: 56.3460 - val_mean_squared_error: 56.3460\n",
      "Epoch 97/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.7852 - mean_squared_error: 58.7852 - val_loss: 56.4448 - val_mean_squared_error: 56.4448\n",
      "Epoch 98/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.8175 - mean_squared_error: 58.8175 - val_loss: 56.3379 - val_mean_squared_error: 56.3379\n",
      "Epoch 99/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.8095 - mean_squared_error: 58.8095 - val_loss: 56.2811 - val_mean_squared_error: 56.2811\n",
      "Epoch 100/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.8247 - mean_squared_error: 58.8247 - val_loss: 56.4212 - val_mean_squared_error: 56.4212\n",
      "Epoch 101/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.7134 - mean_squared_error: 58.7134 - val_loss: 56.2865 - val_mean_squared_error: 56.2865\n",
      "Epoch 102/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 58.7174 - mean_squared_error: 58.7174 - val_loss: 56.3613 - val_mean_squared_error: 56.3613\n",
      "Epoch 103/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.7340 - mean_squared_error: 58.7340 - val_loss: 56.3704 - val_mean_squared_error: 56.3704\n",
      "Epoch 104/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.6978 - mean_squared_error: 58.6978 - val_loss: 56.2232 - val_mean_squared_error: 56.2232\n",
      "Epoch 105/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.6912 - mean_squared_error: 58.6912 - val_loss: 56.2151 - val_mean_squared_error: 56.2151\n",
      "Epoch 106/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.6740 - mean_squared_error: 58.6740 - val_loss: 56.3185 - val_mean_squared_error: 56.3185\n",
      "Epoch 107/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.7178 - mean_squared_error: 58.7178 - val_loss: 56.2174 - val_mean_squared_error: 56.2174\n",
      "Epoch 108/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.6615 - mean_squared_error: 58.6615 - val_loss: 56.6475 - val_mean_squared_error: 56.6475\n",
      "Epoch 109/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.6367 - mean_squared_error: 58.6367 - val_loss: 56.1656 - val_mean_squared_error: 56.1656\n",
      "Epoch 110/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.6259 - mean_squared_error: 58.6259 - val_loss: 56.1693 - val_mean_squared_error: 56.1693\n",
      "Epoch 111/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.6790 - mean_squared_error: 58.6790 - val_loss: 56.2002 - val_mean_squared_error: 56.2002\n",
      "Epoch 112/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.6198 - mean_squared_error: 58.6198 - val_loss: 56.1621 - val_mean_squared_error: 56.1622\n",
      "Epoch 113/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.5677 - mean_squared_error: 58.5677 - val_loss: 56.4272 - val_mean_squared_error: 56.4271\n",
      "Epoch 114/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.6435 - mean_squared_error: 58.6435 - val_loss: 56.1533 - val_mean_squared_error: 56.1533\n",
      "Epoch 115/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.5614 - mean_squared_error: 58.5614 - val_loss: 56.1943 - val_mean_squared_error: 56.1943\n",
      "Epoch 116/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 58.5924 - mean_squared_error: 58.5924 - val_loss: 56.1114 - val_mean_squared_error: 56.1114\n",
      "Epoch 117/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.6462 - mean_squared_error: 58.6462 - val_loss: 56.1200 - val_mean_squared_error: 56.1200\n",
      "Epoch 118/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.5252 - mean_squared_error: 58.5252 - val_loss: 56.2146 - val_mean_squared_error: 56.2146\n",
      "Epoch 119/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 58.5411 - mean_squared_error: 58.5411 - val_loss: 56.1146 - val_mean_squared_error: 56.1146\n",
      "Epoch 120/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.5690 - mean_squared_error: 58.5690 - val_loss: 56.0890 - val_mean_squared_error: 56.0890\n",
      "Epoch 121/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.4827 - mean_squared_error: 58.4827 - val_loss: 56.1131 - val_mean_squared_error: 56.1131\n",
      "Epoch 122/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.5924 - mean_squared_error: 58.5924 - val_loss: 56.5463 - val_mean_squared_error: 56.5463\n",
      "Epoch 123/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.4780 - mean_squared_error: 58.4780 - val_loss: 56.0451 - val_mean_squared_error: 56.0451\n",
      "Epoch 124/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 58.4067 - mean_squared_error: 58.4067 - val_loss: 56.3306 - val_mean_squared_error: 56.3306\n",
      "Epoch 125/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.5979 - mean_squared_error: 58.5979 - val_loss: 56.0698 - val_mean_squared_error: 56.0698\n",
      "Epoch 126/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.4531 - mean_squared_error: 58.4531 - val_loss: 56.1683 - val_mean_squared_error: 56.1684\n",
      "Epoch 127/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.4316 - mean_squared_error: 58.4316 - val_loss: 56.0119 - val_mean_squared_error: 56.0119\n",
      "Epoch 128/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.4974 - mean_squared_error: 58.4974 - val_loss: 56.3614 - val_mean_squared_error: 56.3614\n",
      "Epoch 129/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3542 - mean_squared_error: 58.3542 - val_loss: 56.2172 - val_mean_squared_error: 56.2172\n",
      "Epoch 130/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.3997 - mean_squared_error: 58.3997 - val_loss: 56.0055 - val_mean_squared_error: 56.0055\n",
      "Epoch 131/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.4021 - mean_squared_error: 58.4022 - val_loss: 56.0457 - val_mean_squared_error: 56.0457\n",
      "Epoch 132/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.4671 - mean_squared_error: 58.4671 - val_loss: 56.5027 - val_mean_squared_error: 56.5027\n",
      "Epoch 133/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.4429 - mean_squared_error: 58.4429 - val_loss: 56.2188 - val_mean_squared_error: 56.2188\n",
      "Epoch 134/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.3685 - mean_squared_error: 58.3685 - val_loss: 56.2985 - val_mean_squared_error: 56.2985\n",
      "Epoch 135/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.4622 - mean_squared_error: 58.4622 - val_loss: 55.9418 - val_mean_squared_error: 55.9417\n",
      "Epoch 136/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3562 - mean_squared_error: 58.3562 - val_loss: 56.0307 - val_mean_squared_error: 56.0307\n",
      "Epoch 137/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.3254 - mean_squared_error: 58.3254 - val_loss: 56.4491 - val_mean_squared_error: 56.4491\n",
      "Epoch 138/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.3495 - mean_squared_error: 58.3495 - val_loss: 55.9429 - val_mean_squared_error: 55.9429\n",
      "Epoch 139/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.3348 - mean_squared_error: 58.3348 - val_loss: 55.9593 - val_mean_squared_error: 55.9593\n",
      "Epoch 140/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.4319 - mean_squared_error: 58.4319 - val_loss: 55.9361 - val_mean_squared_error: 55.9361\n",
      "Epoch 141/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.3437 - mean_squared_error: 58.3437 - val_loss: 55.9093 - val_mean_squared_error: 55.9093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.2714 - mean_squared_error: 58.2714 - val_loss: 55.9358 - val_mean_squared_error: 55.9358\n",
      "Epoch 143/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.4023 - mean_squared_error: 58.4023 - val_loss: 55.9621 - val_mean_squared_error: 55.9621\n",
      "Epoch 144/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2812 - mean_squared_error: 58.2812 - val_loss: 55.9838 - val_mean_squared_error: 55.9838\n",
      "Epoch 145/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.3720 - mean_squared_error: 58.3720 - val_loss: 55.8970 - val_mean_squared_error: 55.8970\n",
      "Epoch 146/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.3196 - mean_squared_error: 58.3196 - val_loss: 55.8702 - val_mean_squared_error: 55.8702\n",
      "Epoch 147/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.2565 - mean_squared_error: 58.2565 - val_loss: 55.8815 - val_mean_squared_error: 55.8815\n",
      "Epoch 148/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.1873 - mean_squared_error: 58.1873 - val_loss: 55.9660 - val_mean_squared_error: 55.9660\n",
      "Epoch 149/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 58.2720 - mean_squared_error: 58.2720 - val_loss: 55.9310 - val_mean_squared_error: 55.9310\n",
      "Epoch 150/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.1861 - mean_squared_error: 58.1861 - val_loss: 55.8676 - val_mean_squared_error: 55.8676\n",
      "Epoch 151/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 58.2926 - mean_squared_error: 58.2927 - val_loss: 55.9549 - val_mean_squared_error: 55.9549\n",
      "Epoch 152/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.1981 - mean_squared_error: 58.1981 - val_loss: 55.8770 - val_mean_squared_error: 55.8770\n",
      "Epoch 153/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.2197 - mean_squared_error: 58.2197 - val_loss: 55.8291 - val_mean_squared_error: 55.8291\n",
      "Epoch 154/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.2191 - mean_squared_error: 58.2191 - val_loss: 56.1152 - val_mean_squared_error: 56.1152\n",
      "Epoch 155/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.1514 - mean_squared_error: 58.1514 - val_loss: 55.8268 - val_mean_squared_error: 55.8268\n",
      "Epoch 156/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 58.2803 - mean_squared_error: 58.2803 - val_loss: 56.3285 - val_mean_squared_error: 56.3285\n",
      "Epoch 157/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.1558 - mean_squared_error: 58.1558 - val_loss: 55.8530 - val_mean_squared_error: 55.8530\n",
      "Epoch 158/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.2117 - mean_squared_error: 58.2117 - val_loss: 56.5560 - val_mean_squared_error: 56.5560\n",
      "Epoch 159/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1224 - mean_squared_error: 58.1224 - val_loss: 56.6537 - val_mean_squared_error: 56.6537\n",
      "Epoch 160/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.2437 - mean_squared_error: 58.2437 - val_loss: 56.2587 - val_mean_squared_error: 56.2587\n",
      "Epoch 161/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.2154 - mean_squared_error: 58.2154 - val_loss: 55.8160 - val_mean_squared_error: 55.8160\n",
      "Epoch 162/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1042 - mean_squared_error: 58.1042 - val_loss: 56.2106 - val_mean_squared_error: 56.2106\n",
      "Epoch 163/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.1703 - mean_squared_error: 58.1703 - val_loss: 55.8060 - val_mean_squared_error: 55.8060\n",
      "Epoch 164/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.1118 - mean_squared_error: 58.1118 - val_loss: 55.9127 - val_mean_squared_error: 55.9127\n",
      "Epoch 165/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.1782 - mean_squared_error: 58.1782 - val_loss: 55.7873 - val_mean_squared_error: 55.7873\n",
      "Epoch 166/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.1013 - mean_squared_error: 58.1013 - val_loss: 56.6487 - val_mean_squared_error: 56.6488\n",
      "Epoch 167/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.1406 - mean_squared_error: 58.1406 - val_loss: 55.8981 - val_mean_squared_error: 55.8981\n",
      "Epoch 168/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 58.1550 - mean_squared_error: 58.1550 - val_loss: 55.7392 - val_mean_squared_error: 55.7392\n",
      "Epoch 169/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.1026 - mean_squared_error: 58.1026 - val_loss: 55.7888 - val_mean_squared_error: 55.7888\n",
      "Epoch 170/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0638 - mean_squared_error: 58.0638 - val_loss: 55.8867 - val_mean_squared_error: 55.8867\n",
      "Epoch 171/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.2867 - mean_squared_error: 58.2866 - val_loss: 55.7408 - val_mean_squared_error: 55.7408\n",
      "Epoch 172/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0416 - mean_squared_error: 58.0416 - val_loss: 55.9121 - val_mean_squared_error: 55.9121\n",
      "Epoch 173/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0805 - mean_squared_error: 58.0805 - val_loss: 56.1131 - val_mean_squared_error: 56.1131\n",
      "Epoch 174/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.0923 - mean_squared_error: 58.0923 - val_loss: 55.7744 - val_mean_squared_error: 55.7744\n",
      "Epoch 175/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.1364 - mean_squared_error: 58.1364 - val_loss: 55.7026 - val_mean_squared_error: 55.7026\n",
      "Epoch 176/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0671 - mean_squared_error: 58.0671 - val_loss: 56.1674 - val_mean_squared_error: 56.1674\n",
      "Epoch 177/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0521 - mean_squared_error: 58.0521 - val_loss: 55.7410 - val_mean_squared_error: 55.7410\n",
      "Epoch 178/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.1004 - mean_squared_error: 58.1004 - val_loss: 55.7113 - val_mean_squared_error: 55.7113\n",
      "Epoch 179/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9783 - mean_squared_error: 57.9783 - val_loss: 55.7514 - val_mean_squared_error: 55.7514\n",
      "Epoch 180/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9597 - mean_squared_error: 57.9597 - val_loss: 55.7277 - val_mean_squared_error: 55.7277\n",
      "Epoch 181/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.0677 - mean_squared_error: 58.0678 - val_loss: 55.9303 - val_mean_squared_error: 55.9303\n",
      "Epoch 182/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.0782 - mean_squared_error: 58.0782 - val_loss: 55.6760 - val_mean_squared_error: 55.6760\n",
      "Epoch 183/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.0464 - mean_squared_error: 58.0464 - val_loss: 55.6520 - val_mean_squared_error: 55.6520\n",
      "Epoch 184/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9931 - mean_squared_error: 57.9931 - val_loss: 55.6817 - val_mean_squared_error: 55.6817\n",
      "Epoch 185/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 58.0249 - mean_squared_error: 58.0249 - val_loss: 55.6766 - val_mean_squared_error: 55.6766\n",
      "Epoch 186/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9720 - mean_squared_error: 57.9721 - val_loss: 56.0253 - val_mean_squared_error: 56.0253\n",
      "Epoch 187/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 58.0583 - mean_squared_error: 58.0583 - val_loss: 55.6569 - val_mean_squared_error: 55.6569\n",
      "Epoch 188/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0825 - mean_squared_error: 58.0825 - val_loss: 55.9093 - val_mean_squared_error: 55.9093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 58.0358 - mean_squared_error: 58.0358 - val_loss: 55.7971 - val_mean_squared_error: 55.7971\n",
      "Epoch 190/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9695 - mean_squared_error: 57.9695 - val_loss: 55.9119 - val_mean_squared_error: 55.9119\n",
      "Epoch 191/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.8752 - mean_squared_error: 57.8752 - val_loss: 55.7580 - val_mean_squared_error: 55.7580\n",
      "Epoch 192/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9779 - mean_squared_error: 57.9779 - val_loss: 55.8984 - val_mean_squared_error: 55.8984\n",
      "Epoch 193/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9957 - mean_squared_error: 57.9957 - val_loss: 55.9245 - val_mean_squared_error: 55.9245\n",
      "Epoch 194/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.9526 - mean_squared_error: 57.9526 - val_loss: 55.6312 - val_mean_squared_error: 55.6312\n",
      "Epoch 195/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9696 - mean_squared_error: 57.9696 - val_loss: 55.6926 - val_mean_squared_error: 55.6926\n",
      "Epoch 196/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.8920 - mean_squared_error: 57.8920 - val_loss: 55.6177 - val_mean_squared_error: 55.6177\n",
      "Epoch 197/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.9182 - mean_squared_error: 57.9182 - val_loss: 55.6743 - val_mean_squared_error: 55.6743\n",
      "Epoch 198/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.8662 - mean_squared_error: 57.8662 - val_loss: 55.8482 - val_mean_squared_error: 55.8482\n",
      "Epoch 199/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 58.0501 - mean_squared_error: 58.0501 - val_loss: 55.6972 - val_mean_squared_error: 55.6972\n",
      "Epoch 200/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.8256 - mean_squared_error: 57.8256 - val_loss: 55.5705 - val_mean_squared_error: 55.5705\n",
      "Epoch 201/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.9669 - mean_squared_error: 57.9669 - val_loss: 55.6490 - val_mean_squared_error: 55.6490\n",
      "Epoch 202/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.8856 - mean_squared_error: 57.8856 - val_loss: 55.6244 - val_mean_squared_error: 55.6244\n",
      "Epoch 203/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.9179 - mean_squared_error: 57.9179 - val_loss: 55.6172 - val_mean_squared_error: 55.6172\n",
      "Epoch 204/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8675 - mean_squared_error: 57.8674 - val_loss: 55.5817 - val_mean_squared_error: 55.5817\n",
      "Epoch 205/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.9597 - mean_squared_error: 57.9597 - val_loss: 55.5572 - val_mean_squared_error: 55.5572\n",
      "Epoch 206/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.9063 - mean_squared_error: 57.9063 - val_loss: 55.7794 - val_mean_squared_error: 55.7794\n",
      "Epoch 207/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8174 - mean_squared_error: 57.8174 - val_loss: 56.0425 - val_mean_squared_error: 56.0425\n",
      "Epoch 208/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8795 - mean_squared_error: 57.8795 - val_loss: 55.7013 - val_mean_squared_error: 55.7013\n",
      "Epoch 209/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8561 - mean_squared_error: 57.8561 - val_loss: 55.6039 - val_mean_squared_error: 55.6039\n",
      "Epoch 210/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8568 - mean_squared_error: 57.8568 - val_loss: 55.9728 - val_mean_squared_error: 55.9728\n",
      "Epoch 211/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.8839 - mean_squared_error: 57.8839 - val_loss: 55.5654 - val_mean_squared_error: 55.5654\n",
      "Epoch 212/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7998 - mean_squared_error: 57.7998 - val_loss: 55.5758 - val_mean_squared_error: 55.5758\n",
      "Epoch 213/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8236 - mean_squared_error: 57.8236 - val_loss: 55.6447 - val_mean_squared_error: 55.6447\n",
      "Epoch 214/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.9800 - mean_squared_error: 57.9800 - val_loss: 55.8868 - val_mean_squared_error: 55.8868\n",
      "Epoch 215/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8355 - mean_squared_error: 57.8355 - val_loss: 55.5382 - val_mean_squared_error: 55.5382\n",
      "Epoch 216/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 57.8925 - mean_squared_error: 57.8925 - val_loss: 55.6351 - val_mean_squared_error: 55.6351\n",
      "Epoch 217/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.8093 - mean_squared_error: 57.8093 - val_loss: 55.5912 - val_mean_squared_error: 55.5912\n",
      "Epoch 218/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.8200 - mean_squared_error: 57.8200 - val_loss: 55.5047 - val_mean_squared_error: 55.5047\n",
      "Epoch 219/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.8900 - mean_squared_error: 57.8900 - val_loss: 55.7990 - val_mean_squared_error: 55.7990\n",
      "Epoch 220/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.7971 - mean_squared_error: 57.7971 - val_loss: 55.5622 - val_mean_squared_error: 55.5622\n",
      "Epoch 221/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.8014 - mean_squared_error: 57.8015 - val_loss: 55.6129 - val_mean_squared_error: 55.6129\n",
      "Epoch 222/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8821 - mean_squared_error: 57.8820 - val_loss: 55.8515 - val_mean_squared_error: 55.8515\n",
      "Epoch 223/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8273 - mean_squared_error: 57.8274 - val_loss: 55.5028 - val_mean_squared_error: 55.5028\n",
      "Epoch 224/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.7576 - mean_squared_error: 57.7576 - val_loss: 55.5043 - val_mean_squared_error: 55.5043\n",
      "Epoch 225/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7984 - mean_squared_error: 57.7984 - val_loss: 55.4886 - val_mean_squared_error: 55.4886\n",
      "Epoch 226/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7874 - mean_squared_error: 57.7874 - val_loss: 55.4823 - val_mean_squared_error: 55.4823\n",
      "Epoch 227/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.7794 - mean_squared_error: 57.7794 - val_loss: 55.5487 - val_mean_squared_error: 55.5487\n",
      "Epoch 228/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.7367 - mean_squared_error: 57.7367 - val_loss: 55.4943 - val_mean_squared_error: 55.4943\n",
      "Epoch 229/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.7289 - mean_squared_error: 57.7289 - val_loss: 55.5872 - val_mean_squared_error: 55.5873\n",
      "Epoch 230/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.8177 - mean_squared_error: 57.8177 - val_loss: 55.5971 - val_mean_squared_error: 55.5972\n",
      "Epoch 231/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.7813 - mean_squared_error: 57.7813 - val_loss: 55.5586 - val_mean_squared_error: 55.5586\n",
      "Epoch 232/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.8219 - mean_squared_error: 57.8219 - val_loss: 55.4995 - val_mean_squared_error: 55.4995\n",
      "Epoch 233/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7285 - mean_squared_error: 57.7285 - val_loss: 55.5593 - val_mean_squared_error: 55.5593\n",
      "Epoch 234/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7838 - mean_squared_error: 57.7838 - val_loss: 55.7532 - val_mean_squared_error: 55.7532\n",
      "Epoch 235/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.6344 - mean_squared_error: 57.6344 - val_loss: 55.7607 - val_mean_squared_error: 55.7607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8100 - mean_squared_error: 57.8100 - val_loss: 55.5260 - val_mean_squared_error: 55.5260\n",
      "Epoch 237/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.8226 - mean_squared_error: 57.8226 - val_loss: 55.4337 - val_mean_squared_error: 55.4337\n",
      "Epoch 238/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.7035 - mean_squared_error: 57.7035 - val_loss: 55.4902 - val_mean_squared_error: 55.4902\n",
      "Epoch 239/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.6333 - mean_squared_error: 57.6333 - val_loss: 55.4724 - val_mean_squared_error: 55.4724\n",
      "Epoch 240/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.7539 - mean_squared_error: 57.7540 - val_loss: 55.4304 - val_mean_squared_error: 55.4304\n",
      "Epoch 241/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7016 - mean_squared_error: 57.7016 - val_loss: 55.5783 - val_mean_squared_error: 55.5783\n",
      "Epoch 242/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.6616 - mean_squared_error: 57.6616 - val_loss: 55.4737 - val_mean_squared_error: 55.4737\n",
      "Epoch 243/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6595 - mean_squared_error: 57.6595 - val_loss: 55.6093 - val_mean_squared_error: 55.6093\n",
      "Epoch 244/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7169 - mean_squared_error: 57.7169 - val_loss: 55.7242 - val_mean_squared_error: 55.7242\n",
      "Epoch 245/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7073 - mean_squared_error: 57.7073 - val_loss: 55.6985 - val_mean_squared_error: 55.6985\n",
      "Epoch 246/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.6561 - mean_squared_error: 57.6561 - val_loss: 55.4587 - val_mean_squared_error: 55.4588\n",
      "Epoch 247/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7678 - mean_squared_error: 57.7678 - val_loss: 55.7384 - val_mean_squared_error: 55.7384\n",
      "Epoch 248/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.6677 - mean_squared_error: 57.6677 - val_loss: 55.4516 - val_mean_squared_error: 55.4516\n",
      "Epoch 249/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.6455 - mean_squared_error: 57.6455 - val_loss: 55.8442 - val_mean_squared_error: 55.8442\n",
      "Epoch 250/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.6777 - mean_squared_error: 57.6777 - val_loss: 55.4436 - val_mean_squared_error: 55.4436\n",
      "Epoch 251/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5874 - mean_squared_error: 57.5874 - val_loss: 55.4232 - val_mean_squared_error: 55.4232\n",
      "Epoch 252/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5779 - mean_squared_error: 57.5779 - val_loss: 55.4607 - val_mean_squared_error: 55.4607\n",
      "Epoch 253/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.6413 - mean_squared_error: 57.6413 - val_loss: 55.4258 - val_mean_squared_error: 55.4258\n",
      "Epoch 254/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.6277 - mean_squared_error: 57.6277 - val_loss: 55.4616 - val_mean_squared_error: 55.4616\n",
      "Epoch 255/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.5839 - mean_squared_error: 57.5839 - val_loss: 55.4786 - val_mean_squared_error: 55.4786\n",
      "Epoch 256/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5388 - mean_squared_error: 57.5388 - val_loss: 55.4584 - val_mean_squared_error: 55.4584\n",
      "Epoch 257/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.6009 - mean_squared_error: 57.6008 - val_loss: 55.3680 - val_mean_squared_error: 55.3680\n",
      "Epoch 258/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5927 - mean_squared_error: 57.5927 - val_loss: 55.5523 - val_mean_squared_error: 55.5523\n",
      "Epoch 259/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.6064 - mean_squared_error: 57.6064 - val_loss: 55.4835 - val_mean_squared_error: 55.4835\n",
      "Epoch 260/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5868 - mean_squared_error: 57.5868 - val_loss: 55.3919 - val_mean_squared_error: 55.3919\n",
      "Epoch 261/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.7206 - mean_squared_error: 57.7206 - val_loss: 55.3539 - val_mean_squared_error: 55.3539\n",
      "Epoch 262/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5401 - mean_squared_error: 57.5401 - val_loss: 55.4136 - val_mean_squared_error: 55.4136\n",
      "Epoch 263/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.4420 - mean_squared_error: 57.4420 - val_loss: 55.6718 - val_mean_squared_error: 55.6718\n",
      "Epoch 264/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5274 - mean_squared_error: 57.5274 - val_loss: 55.3373 - val_mean_squared_error: 55.3373\n",
      "Epoch 265/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.7635 - mean_squared_error: 57.7635 - val_loss: 55.3423 - val_mean_squared_error: 55.3423\n",
      "Epoch 266/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5630 - mean_squared_error: 57.5630 - val_loss: 56.0204 - val_mean_squared_error: 56.0204\n",
      "Epoch 267/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5659 - mean_squared_error: 57.5659 - val_loss: 55.5424 - val_mean_squared_error: 55.5424\n",
      "Epoch 268/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.5536 - mean_squared_error: 57.5536 - val_loss: 55.3348 - val_mean_squared_error: 55.3348\n",
      "Epoch 269/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5513 - mean_squared_error: 57.5513 - val_loss: 55.3619 - val_mean_squared_error: 55.3619\n",
      "Epoch 270/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.5499 - mean_squared_error: 57.5499 - val_loss: 55.3961 - val_mean_squared_error: 55.3961\n",
      "Epoch 271/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5953 - mean_squared_error: 57.5953 - val_loss: 55.4710 - val_mean_squared_error: 55.4710\n",
      "Epoch 272/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5173 - mean_squared_error: 57.5173 - val_loss: 55.3756 - val_mean_squared_error: 55.3756\n",
      "Epoch 273/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.6790 - mean_squared_error: 57.6790 - val_loss: 55.7674 - val_mean_squared_error: 55.7674\n",
      "Epoch 274/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5556 - mean_squared_error: 57.5556 - val_loss: 55.3799 - val_mean_squared_error: 55.3799\n",
      "Epoch 275/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.5309 - mean_squared_error: 57.5309 - val_loss: 55.3231 - val_mean_squared_error: 55.3231\n",
      "Epoch 276/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.5382 - mean_squared_error: 57.5383 - val_loss: 55.5378 - val_mean_squared_error: 55.5378\n",
      "Epoch 277/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.5061 - mean_squared_error: 57.5061 - val_loss: 55.3140 - val_mean_squared_error: 55.3140\n",
      "Epoch 278/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.4955 - mean_squared_error: 57.4955 - val_loss: 55.9857 - val_mean_squared_error: 55.9857\n",
      "Epoch 279/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.5694 - mean_squared_error: 57.5694 - val_loss: 55.4466 - val_mean_squared_error: 55.4466\n",
      "Epoch 280/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.4907 - mean_squared_error: 57.4907 - val_loss: 55.4143 - val_mean_squared_error: 55.4143\n",
      "Epoch 281/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4851 - mean_squared_error: 57.4851 - val_loss: 55.4635 - val_mean_squared_error: 55.4635\n",
      "Epoch 282/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.5601 - mean_squared_error: 57.5601 - val_loss: 55.3471 - val_mean_squared_error: 55.3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5467 - mean_squared_error: 57.5467 - val_loss: 55.8065 - val_mean_squared_error: 55.8065\n",
      "Epoch 284/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.4953 - mean_squared_error: 57.4953 - val_loss: 55.5125 - val_mean_squared_error: 55.5125\n",
      "Epoch 285/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.4821 - mean_squared_error: 57.4821 - val_loss: 55.3289 - val_mean_squared_error: 55.3289\n",
      "Epoch 286/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.4743 - mean_squared_error: 57.4743 - val_loss: 55.3139 - val_mean_squared_error: 55.3139\n",
      "Epoch 287/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.5531 - mean_squared_error: 57.5531 - val_loss: 55.3107 - val_mean_squared_error: 55.3107\n",
      "Epoch 288/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3848 - mean_squared_error: 57.3848 - val_loss: 55.2943 - val_mean_squared_error: 55.2943\n",
      "Epoch 289/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4460 - mean_squared_error: 57.4460 - val_loss: 55.4234 - val_mean_squared_error: 55.4234\n",
      "Epoch 290/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.3991 - mean_squared_error: 57.3990 - val_loss: 55.3234 - val_mean_squared_error: 55.3234\n",
      "Epoch 291/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4752 - mean_squared_error: 57.4752 - val_loss: 55.2802 - val_mean_squared_error: 55.2802\n",
      "Epoch 292/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.4033 - mean_squared_error: 57.4032 - val_loss: 55.2758 - val_mean_squared_error: 55.2758\n",
      "Epoch 293/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.3430 - mean_squared_error: 57.3430 - val_loss: 55.2862 - val_mean_squared_error: 55.2862\n",
      "Epoch 294/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.4686 - mean_squared_error: 57.4686 - val_loss: 55.6525 - val_mean_squared_error: 55.6525\n",
      "Epoch 295/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3340 - mean_squared_error: 57.3340 - val_loss: 55.5399 - val_mean_squared_error: 55.5400\n",
      "Epoch 296/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.4085 - mean_squared_error: 57.4085 - val_loss: 55.3895 - val_mean_squared_error: 55.3895\n",
      "Epoch 297/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.4387 - mean_squared_error: 57.4387 - val_loss: 55.6495 - val_mean_squared_error: 55.6495\n",
      "Epoch 298/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4589 - mean_squared_error: 57.4589 - val_loss: 55.2590 - val_mean_squared_error: 55.2590\n",
      "Epoch 299/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4413 - mean_squared_error: 57.4413 - val_loss: 55.4517 - val_mean_squared_error: 55.4517\n",
      "Epoch 300/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.4013 - mean_squared_error: 57.4013 - val_loss: 55.2907 - val_mean_squared_error: 55.2906\n",
      "Epoch 301/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.4095 - mean_squared_error: 57.4095 - val_loss: 55.2381 - val_mean_squared_error: 55.2381\n",
      "Epoch 302/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.3664 - mean_squared_error: 57.3664 - val_loss: 55.3499 - val_mean_squared_error: 55.3499\n",
      "Epoch 303/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.4687 - mean_squared_error: 57.4687 - val_loss: 55.2989 - val_mean_squared_error: 55.2989\n",
      "Epoch 304/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.3323 - mean_squared_error: 57.3323 - val_loss: 55.2490 - val_mean_squared_error: 55.2491\n",
      "Epoch 305/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 57.3666 - mean_squared_error: 57.3666 - val_loss: 55.3488 - val_mean_squared_error: 55.3488\n",
      "Epoch 306/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.4552 - mean_squared_error: 57.4553 - val_loss: 55.2319 - val_mean_squared_error: 55.2320\n",
      "Epoch 307/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.3594 - mean_squared_error: 57.3594 - val_loss: 55.4774 - val_mean_squared_error: 55.4774\n",
      "Epoch 308/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.3670 - mean_squared_error: 57.3670 - val_loss: 55.2056 - val_mean_squared_error: 55.2056\n",
      "Epoch 309/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3546 - mean_squared_error: 57.3546 - val_loss: 55.2013 - val_mean_squared_error: 55.2013\n",
      "Epoch 310/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.4253 - mean_squared_error: 57.4254 - val_loss: 55.5427 - val_mean_squared_error: 55.5427\n",
      "Epoch 311/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.3743 - mean_squared_error: 57.3742 - val_loss: 55.8431 - val_mean_squared_error: 55.8431\n",
      "Epoch 312/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.5174 - mean_squared_error: 57.5174 - val_loss: 55.9114 - val_mean_squared_error: 55.9114\n",
      "Epoch 313/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.4019 - mean_squared_error: 57.4019 - val_loss: 55.2657 - val_mean_squared_error: 55.2657\n",
      "Epoch 314/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.3375 - mean_squared_error: 57.3375 - val_loss: 55.3255 - val_mean_squared_error: 55.3255\n",
      "Epoch 315/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.2775 - mean_squared_error: 57.2775 - val_loss: 55.2042 - val_mean_squared_error: 55.2042\n",
      "Epoch 316/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.3384 - mean_squared_error: 57.3384 - val_loss: 55.1996 - val_mean_squared_error: 55.1996\n",
      "Epoch 317/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2295 - mean_squared_error: 57.2296 - val_loss: 55.2490 - val_mean_squared_error: 55.2490\n",
      "Epoch 318/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3596 - mean_squared_error: 57.3596 - val_loss: 55.1813 - val_mean_squared_error: 55.1813\n",
      "Epoch 319/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2904 - mean_squared_error: 57.2904 - val_loss: 55.2661 - val_mean_squared_error: 55.2661\n",
      "Epoch 320/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.3060 - mean_squared_error: 57.3060 - val_loss: 55.2419 - val_mean_squared_error: 55.2419\n",
      "Epoch 321/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.2628 - mean_squared_error: 57.2628 - val_loss: 55.2426 - val_mean_squared_error: 55.2426\n",
      "Epoch 322/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.3670 - mean_squared_error: 57.3670 - val_loss: 55.5300 - val_mean_squared_error: 55.5300\n",
      "Epoch 323/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.3853 - mean_squared_error: 57.3853 - val_loss: 55.1856 - val_mean_squared_error: 55.1856\n",
      "Epoch 324/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.3423 - mean_squared_error: 57.3423 - val_loss: 55.2317 - val_mean_squared_error: 55.2317\n",
      "Epoch 325/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.3908 - mean_squared_error: 57.3907 - val_loss: 55.2377 - val_mean_squared_error: 55.2377\n",
      "Epoch 326/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.3282 - mean_squared_error: 57.3282 - val_loss: 55.1649 - val_mean_squared_error: 55.1649\n",
      "Epoch 327/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3063 - mean_squared_error: 57.3063 - val_loss: 55.2764 - val_mean_squared_error: 55.2764\n",
      "Epoch 328/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3171 - mean_squared_error: 57.3172 - val_loss: 55.1913 - val_mean_squared_error: 55.1913\n",
      "Epoch 329/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.2435 - mean_squared_error: 57.2435 - val_loss: 55.2703 - val_mean_squared_error: 55.2703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.3060 - mean_squared_error: 57.3060 - val_loss: 55.2372 - val_mean_squared_error: 55.2372\n",
      "Epoch 331/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2454 - mean_squared_error: 57.2454 - val_loss: 55.1906 - val_mean_squared_error: 55.1906\n",
      "Epoch 332/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.2662 - mean_squared_error: 57.2662 - val_loss: 55.1888 - val_mean_squared_error: 55.1888\n",
      "Epoch 333/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.3517 - mean_squared_error: 57.3517 - val_loss: 55.3867 - val_mean_squared_error: 55.3867\n",
      "Epoch 334/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.3600 - mean_squared_error: 57.3600 - val_loss: 55.2459 - val_mean_squared_error: 55.2459\n",
      "Epoch 335/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.1704 - mean_squared_error: 57.1704 - val_loss: 55.1955 - val_mean_squared_error: 55.1955\n",
      "Epoch 336/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.2689 - mean_squared_error: 57.2689 - val_loss: 55.3770 - val_mean_squared_error: 55.3770\n",
      "Epoch 337/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.2104 - mean_squared_error: 57.2104 - val_loss: 55.1643 - val_mean_squared_error: 55.1643\n",
      "Epoch 338/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.2173 - mean_squared_error: 57.2173 - val_loss: 55.2375 - val_mean_squared_error: 55.2375\n",
      "Epoch 339/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.2976 - mean_squared_error: 57.2976 - val_loss: 55.1701 - val_mean_squared_error: 55.1701\n",
      "Epoch 340/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1970 - mean_squared_error: 57.1970 - val_loss: 55.2469 - val_mean_squared_error: 55.2469\n",
      "Epoch 341/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.2425 - mean_squared_error: 57.2425 - val_loss: 55.2329 - val_mean_squared_error: 55.2329\n",
      "Epoch 342/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.2692 - mean_squared_error: 57.2692 - val_loss: 55.1825 - val_mean_squared_error: 55.1825\n",
      "Epoch 343/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.2110 - mean_squared_error: 57.2110 - val_loss: 55.4045 - val_mean_squared_error: 55.4045\n",
      "Epoch 344/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.2044 - mean_squared_error: 57.2044 - val_loss: 55.2148 - val_mean_squared_error: 55.2148\n",
      "Epoch 345/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1578 - mean_squared_error: 57.1578 - val_loss: 55.3827 - val_mean_squared_error: 55.3827\n",
      "Epoch 346/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1895 - mean_squared_error: 57.1895 - val_loss: 55.2039 - val_mean_squared_error: 55.2039\n",
      "Epoch 347/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1783 - mean_squared_error: 57.1783 - val_loss: 55.2627 - val_mean_squared_error: 55.2627\n",
      "Epoch 348/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.1703 - mean_squared_error: 57.1703 - val_loss: 55.2563 - val_mean_squared_error: 55.2563\n",
      "Epoch 349/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2729 - mean_squared_error: 57.2729 - val_loss: 55.1604 - val_mean_squared_error: 55.1604\n",
      "Epoch 350/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.0816 - mean_squared_error: 57.0816 - val_loss: 55.1136 - val_mean_squared_error: 55.1136\n",
      "Epoch 351/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2971 - mean_squared_error: 57.2971 - val_loss: 55.1386 - val_mean_squared_error: 55.1386\n",
      "Epoch 352/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1469 - mean_squared_error: 57.1469 - val_loss: 55.3452 - val_mean_squared_error: 55.3451\n",
      "Epoch 353/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1102 - mean_squared_error: 57.1102 - val_loss: 55.1403 - val_mean_squared_error: 55.1403\n",
      "Epoch 354/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2020 - mean_squared_error: 57.2020 - val_loss: 55.5668 - val_mean_squared_error: 55.5668\n",
      "Epoch 355/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1952 - mean_squared_error: 57.1952 - val_loss: 55.4720 - val_mean_squared_error: 55.4720\n",
      "Epoch 356/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.2090 - mean_squared_error: 57.2090 - val_loss: 55.6907 - val_mean_squared_error: 55.6907\n",
      "Epoch 357/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.2399 - mean_squared_error: 57.2399 - val_loss: 55.1161 - val_mean_squared_error: 55.1161\n",
      "Epoch 358/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0792 - mean_squared_error: 57.0792 - val_loss: 55.4538 - val_mean_squared_error: 55.4538\n",
      "Epoch 359/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2871 - mean_squared_error: 57.2871 - val_loss: 55.0931 - val_mean_squared_error: 55.0931\n",
      "Epoch 360/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1816 - mean_squared_error: 57.1816 - val_loss: 55.5748 - val_mean_squared_error: 55.5748\n",
      "Epoch 361/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.0580 - mean_squared_error: 57.0580 - val_loss: 55.0881 - val_mean_squared_error: 55.0881\n",
      "Epoch 362/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1166 - mean_squared_error: 57.1166 - val_loss: 55.1100 - val_mean_squared_error: 55.1100\n",
      "Epoch 363/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1729 - mean_squared_error: 57.1729 - val_loss: 55.1006 - val_mean_squared_error: 55.1006\n",
      "Epoch 364/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1065 - mean_squared_error: 57.1065 - val_loss: 55.1491 - val_mean_squared_error: 55.1491\n",
      "Epoch 365/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0806 - mean_squared_error: 57.0806 - val_loss: 55.2063 - val_mean_squared_error: 55.2063\n",
      "Epoch 366/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.0662 - mean_squared_error: 57.0662 - val_loss: 55.1433 - val_mean_squared_error: 55.1433\n",
      "Epoch 367/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.1666 - mean_squared_error: 57.1666 - val_loss: 55.1215 - val_mean_squared_error: 55.1215\n",
      "Epoch 368/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.2124 - mean_squared_error: 57.2124 - val_loss: 55.1028 - val_mean_squared_error: 55.1028\n",
      "Epoch 369/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0394 - mean_squared_error: 57.0394 - val_loss: 56.1520 - val_mean_squared_error: 56.1520\n",
      "Epoch 370/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 57.1657 - mean_squared_error: 57.1657 - val_loss: 55.1902 - val_mean_squared_error: 55.1902\n",
      "Epoch 371/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 57.0664 - mean_squared_error: 57.0664 - val_loss: 55.1039 - val_mean_squared_error: 55.1039\n",
      "Epoch 372/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 57.1554 - mean_squared_error: 57.1554 - val_loss: 55.2188 - val_mean_squared_error: 55.2188\n",
      "Epoch 373/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.0746 - mean_squared_error: 57.0746 - val_loss: 55.3555 - val_mean_squared_error: 55.3555\n",
      "Epoch 374/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0390 - mean_squared_error: 57.0391 - val_loss: 55.0744 - val_mean_squared_error: 55.0744\n",
      "Epoch 375/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 57.0874 - mean_squared_error: 57.0874 - val_loss: 55.0907 - val_mean_squared_error: 55.0907\n",
      "Epoch 376/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 57.0670 - mean_squared_error: 57.0670 - val_loss: 55.0673 - val_mean_squared_error: 55.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 57.0733 - mean_squared_error: 57.0733 - val_loss: 55.1488 - val_mean_squared_error: 55.1488\n",
      "Epoch 378/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.0418 - mean_squared_error: 57.0418 - val_loss: 55.0535 - val_mean_squared_error: 55.0535\n",
      "Epoch 379/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.1485 - mean_squared_error: 57.1485 - val_loss: 55.0689 - val_mean_squared_error: 55.0689\n",
      "Epoch 380/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 57.0553 - mean_squared_error: 57.0553 - val_loss: 55.1521 - val_mean_squared_error: 55.1521\n",
      "Epoch 381/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0434 - mean_squared_error: 57.0434 - val_loss: 55.3046 - val_mean_squared_error: 55.3046\n",
      "Epoch 382/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1558 - mean_squared_error: 57.1558 - val_loss: 55.1343 - val_mean_squared_error: 55.1343\n",
      "Epoch 383/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0788 - mean_squared_error: 57.0788 - val_loss: 55.4554 - val_mean_squared_error: 55.4554\n",
      "Epoch 384/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 57.1242 - mean_squared_error: 57.1242 - val_loss: 55.1060 - val_mean_squared_error: 55.1060\n",
      "Epoch 385/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.1113 - mean_squared_error: 57.1113 - val_loss: 55.1421 - val_mean_squared_error: 55.1421\n",
      "Epoch 386/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9875 - mean_squared_error: 56.9875 - val_loss: 55.5674 - val_mean_squared_error: 55.5674\n",
      "Epoch 387/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0647 - mean_squared_error: 57.0647 - val_loss: 55.0637 - val_mean_squared_error: 55.0637\n",
      "Epoch 388/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0984 - mean_squared_error: 57.0984 - val_loss: 55.1263 - val_mean_squared_error: 55.1263\n",
      "Epoch 389/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.1926 - mean_squared_error: 57.1926 - val_loss: 55.5541 - val_mean_squared_error: 55.5541\n",
      "Epoch 390/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1048 - mean_squared_error: 57.1048 - val_loss: 55.0569 - val_mean_squared_error: 55.0569\n",
      "Epoch 391/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.0279 - mean_squared_error: 57.0280 - val_loss: 55.3520 - val_mean_squared_error: 55.3520\n",
      "Epoch 392/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0781 - mean_squared_error: 57.0781 - val_loss: 55.0556 - val_mean_squared_error: 55.0556\n",
      "Epoch 393/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0817 - mean_squared_error: 57.0817 - val_loss: 55.2542 - val_mean_squared_error: 55.2542\n",
      "Epoch 394/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 57.1117 - mean_squared_error: 57.1117 - val_loss: 55.1070 - val_mean_squared_error: 55.1070\n",
      "Epoch 395/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.9649 - mean_squared_error: 56.9649 - val_loss: 55.5199 - val_mean_squared_error: 55.5199\n",
      "Epoch 396/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1375 - mean_squared_error: 57.1375 - val_loss: 55.1189 - val_mean_squared_error: 55.1189\n",
      "Epoch 397/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0264 - mean_squared_error: 57.0264 - val_loss: 55.0835 - val_mean_squared_error: 55.0835\n",
      "Epoch 398/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.9192 - mean_squared_error: 56.9192 - val_loss: 55.3295 - val_mean_squared_error: 55.3296\n",
      "Epoch 399/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0540 - mean_squared_error: 57.0540 - val_loss: 55.7785 - val_mean_squared_error: 55.7785\n",
      "Epoch 400/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1091 - mean_squared_error: 57.1091 - val_loss: 55.0547 - val_mean_squared_error: 55.0547\n",
      "Epoch 401/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.9814 - mean_squared_error: 56.9814 - val_loss: 55.0582 - val_mean_squared_error: 55.0582\n",
      "Epoch 402/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0038 - mean_squared_error: 57.0038 - val_loss: 55.0623 - val_mean_squared_error: 55.0623\n",
      "Epoch 403/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.9567 - mean_squared_error: 56.9567 - val_loss: 55.3667 - val_mean_squared_error: 55.3667\n",
      "Epoch 404/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0158 - mean_squared_error: 57.0158 - val_loss: 55.1438 - val_mean_squared_error: 55.1438\n",
      "Epoch 405/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 57.0275 - mean_squared_error: 57.0275 - val_loss: 55.4329 - val_mean_squared_error: 55.4329\n",
      "Epoch 406/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 57.0686 - mean_squared_error: 57.0686 - val_loss: 55.1072 - val_mean_squared_error: 55.1073\n",
      "Epoch 407/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.1200 - mean_squared_error: 57.1200 - val_loss: 55.4242 - val_mean_squared_error: 55.4241\n",
      "Epoch 408/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.9943 - mean_squared_error: 56.9943 - val_loss: 55.2474 - val_mean_squared_error: 55.2475\n",
      "Epoch 409/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9845 - mean_squared_error: 56.9845 - val_loss: 55.0293 - val_mean_squared_error: 55.0293\n",
      "Epoch 410/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.9841 - mean_squared_error: 56.9841 - val_loss: 55.4511 - val_mean_squared_error: 55.4511\n",
      "Epoch 411/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9648 - mean_squared_error: 56.9648 - val_loss: 55.0494 - val_mean_squared_error: 55.0494\n",
      "Epoch 412/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.1705 - mean_squared_error: 57.1705 - val_loss: 55.1652 - val_mean_squared_error: 55.1652\n",
      "Epoch 413/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9333 - mean_squared_error: 56.9333 - val_loss: 55.3105 - val_mean_squared_error: 55.3105\n",
      "Epoch 414/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0354 - mean_squared_error: 57.0354 - val_loss: 55.1119 - val_mean_squared_error: 55.1119\n",
      "Epoch 415/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.9691 - mean_squared_error: 56.9691 - val_loss: 55.2611 - val_mean_squared_error: 55.2611\n",
      "Epoch 416/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 57.0214 - mean_squared_error: 57.0213 - val_loss: 55.0433 - val_mean_squared_error: 55.0433\n",
      "Epoch 417/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.9805 - mean_squared_error: 56.9805 - val_loss: 55.0451 - val_mean_squared_error: 55.0451\n",
      "Epoch 418/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.9743 - mean_squared_error: 56.9743 - val_loss: 55.0173 - val_mean_squared_error: 55.0173\n",
      "Epoch 419/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8737 - mean_squared_error: 56.8737 - val_loss: 55.0528 - val_mean_squared_error: 55.0528\n",
      "Epoch 420/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.9171 - mean_squared_error: 56.9171 - val_loss: 55.1504 - val_mean_squared_error: 55.1504\n",
      "Epoch 421/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.9833 - mean_squared_error: 56.9833 - val_loss: 55.0243 - val_mean_squared_error: 55.0243\n",
      "Epoch 422/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 56.9645 - mean_squared_error: 56.9645 - val_loss: 55.0108 - val_mean_squared_error: 55.0108\n",
      "Epoch 423/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8975 - mean_squared_error: 56.8975 - val_loss: 55.2003 - val_mean_squared_error: 55.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8514 - mean_squared_error: 56.8514 - val_loss: 55.1987 - val_mean_squared_error: 55.1987\n",
      "Epoch 425/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8191 - mean_squared_error: 56.8191 - val_loss: 55.0557 - val_mean_squared_error: 55.0557\n",
      "Epoch 426/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9583 - mean_squared_error: 56.9583 - val_loss: 55.0285 - val_mean_squared_error: 55.0285\n",
      "Epoch 427/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.8876 - mean_squared_error: 56.8876 - val_loss: 55.2548 - val_mean_squared_error: 55.2548\n",
      "Epoch 428/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.9360 - mean_squared_error: 56.9360 - val_loss: 55.4705 - val_mean_squared_error: 55.4705\n",
      "Epoch 429/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 57.0686 - mean_squared_error: 57.0686 - val_loss: 55.1644 - val_mean_squared_error: 55.1644\n",
      "Epoch 430/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.8217 - mean_squared_error: 56.8217 - val_loss: 55.0270 - val_mean_squared_error: 55.0270\n",
      "Epoch 431/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 56.8666 - mean_squared_error: 56.8666 - val_loss: 55.0124 - val_mean_squared_error: 55.0124\n",
      "Epoch 432/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8310 - mean_squared_error: 56.8310 - val_loss: 55.0053 - val_mean_squared_error: 55.0053\n",
      "Epoch 433/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9001 - mean_squared_error: 56.9001 - val_loss: 55.7403 - val_mean_squared_error: 55.7403\n",
      "Epoch 434/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.8795 - mean_squared_error: 56.8795 - val_loss: 54.9851 - val_mean_squared_error: 54.9851\n",
      "Epoch 435/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8745 - mean_squared_error: 56.8745 - val_loss: 54.9887 - val_mean_squared_error: 54.9887\n",
      "Epoch 436/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8918 - mean_squared_error: 56.8918 - val_loss: 55.2447 - val_mean_squared_error: 55.2447\n",
      "Epoch 437/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.8384 - mean_squared_error: 56.8384 - val_loss: 55.0828 - val_mean_squared_error: 55.0828\n",
      "Epoch 438/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8903 - mean_squared_error: 56.8903 - val_loss: 55.1891 - val_mean_squared_error: 55.1891\n",
      "Epoch 439/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.9286 - mean_squared_error: 56.9286 - val_loss: 55.0547 - val_mean_squared_error: 55.0547\n",
      "Epoch 440/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7417 - mean_squared_error: 56.7417 - val_loss: 55.6940 - val_mean_squared_error: 55.6940\n",
      "Epoch 441/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8982 - mean_squared_error: 56.8982 - val_loss: 55.4046 - val_mean_squared_error: 55.4046\n",
      "Epoch 442/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.9369 - mean_squared_error: 56.9369 - val_loss: 54.9917 - val_mean_squared_error: 54.9917\n",
      "Epoch 443/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.8735 - mean_squared_error: 56.8734 - val_loss: 55.3319 - val_mean_squared_error: 55.3319\n",
      "Epoch 444/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.8377 - mean_squared_error: 56.8377 - val_loss: 55.0111 - val_mean_squared_error: 55.0111\n",
      "Epoch 445/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.8615 - mean_squared_error: 56.8615 - val_loss: 55.0165 - val_mean_squared_error: 55.0165\n",
      "Epoch 446/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.9474 - mean_squared_error: 56.9474 - val_loss: 55.0174 - val_mean_squared_error: 55.0174\n",
      "Epoch 447/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.8732 - mean_squared_error: 56.8732 - val_loss: 55.0597 - val_mean_squared_error: 55.0597\n",
      "Epoch 448/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8908 - mean_squared_error: 56.8908 - val_loss: 55.3359 - val_mean_squared_error: 55.3359\n",
      "Epoch 449/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.9067 - mean_squared_error: 56.9067 - val_loss: 54.9784 - val_mean_squared_error: 54.9784\n",
      "Epoch 450/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8149 - mean_squared_error: 56.8149 - val_loss: 55.0431 - val_mean_squared_error: 55.0431\n",
      "Epoch 451/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8148 - mean_squared_error: 56.8148 - val_loss: 54.9661 - val_mean_squared_error: 54.9661\n",
      "Epoch 452/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.9480 - mean_squared_error: 56.9480 - val_loss: 55.0778 - val_mean_squared_error: 55.0779\n",
      "Epoch 453/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.8373 - mean_squared_error: 56.8373 - val_loss: 54.9803 - val_mean_squared_error: 54.9803\n",
      "Epoch 454/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.8200 - mean_squared_error: 56.8200 - val_loss: 55.0257 - val_mean_squared_error: 55.0257\n",
      "Epoch 455/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.0072 - mean_squared_error: 57.0072 - val_loss: 55.0007 - val_mean_squared_error: 55.0007\n",
      "Epoch 456/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.9362 - mean_squared_error: 56.9362 - val_loss: 54.9838 - val_mean_squared_error: 54.9838\n",
      "Epoch 457/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.7800 - mean_squared_error: 56.7800 - val_loss: 54.9832 - val_mean_squared_error: 54.9832\n",
      "Epoch 458/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7875 - mean_squared_error: 56.7875 - val_loss: 55.0375 - val_mean_squared_error: 55.0375\n",
      "Epoch 459/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7242 - mean_squared_error: 56.7242 - val_loss: 54.9830 - val_mean_squared_error: 54.9830\n",
      "Epoch 460/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.8229 - mean_squared_error: 56.8229 - val_loss: 55.1800 - val_mean_squared_error: 55.1800\n",
      "Epoch 461/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8809 - mean_squared_error: 56.8809 - val_loss: 55.1599 - val_mean_squared_error: 55.1599\n",
      "Epoch 462/1000\n",
      "5600/5600 [==============================] - 0s 83us/sample - loss: 56.9403 - mean_squared_error: 56.9402 - val_loss: 55.1142 - val_mean_squared_error: 55.1142\n",
      "Epoch 463/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.9176 - mean_squared_error: 56.9176 - val_loss: 55.0034 - val_mean_squared_error: 55.0034\n",
      "Epoch 464/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7680 - mean_squared_error: 56.7680 - val_loss: 55.0016 - val_mean_squared_error: 55.0016\n",
      "Epoch 465/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.8005 - mean_squared_error: 56.8005 - val_loss: 54.9816 - val_mean_squared_error: 54.9816\n",
      "Epoch 466/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.8011 - mean_squared_error: 56.8011 - val_loss: 55.0202 - val_mean_squared_error: 55.0202\n",
      "Epoch 467/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8320 - mean_squared_error: 56.8320 - val_loss: 55.0976 - val_mean_squared_error: 55.0976\n",
      "Epoch 468/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.7578 - mean_squared_error: 56.7578 - val_loss: 55.1089 - val_mean_squared_error: 55.1089\n",
      "Epoch 469/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 57.1286 - mean_squared_error: 57.1286 - val_loss: 55.0681 - val_mean_squared_error: 55.0681\n",
      "Epoch 470/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.8306 - mean_squared_error: 56.8306 - val_loss: 54.9747 - val_mean_squared_error: 54.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7330 - mean_squared_error: 56.7330 - val_loss: 55.2064 - val_mean_squared_error: 55.2064\n",
      "Epoch 472/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7650 - mean_squared_error: 56.7650 - val_loss: 55.1899 - val_mean_squared_error: 55.1899\n",
      "Epoch 473/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.8595 - mean_squared_error: 56.8595 - val_loss: 55.0248 - val_mean_squared_error: 55.0248\n",
      "Epoch 474/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7664 - mean_squared_error: 56.7664 - val_loss: 55.0253 - val_mean_squared_error: 55.0253\n",
      "Epoch 475/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.7638 - mean_squared_error: 56.7638 - val_loss: 54.9665 - val_mean_squared_error: 54.9665\n",
      "Epoch 476/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7491 - mean_squared_error: 56.7491 - val_loss: 55.1321 - val_mean_squared_error: 55.1321\n",
      "Epoch 477/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7732 - mean_squared_error: 56.7732 - val_loss: 54.9324 - val_mean_squared_error: 54.9324\n",
      "Epoch 478/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8097 - mean_squared_error: 56.8097 - val_loss: 55.0904 - val_mean_squared_error: 55.0905\n",
      "Epoch 479/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.7899 - mean_squared_error: 56.7899 - val_loss: 54.9799 - val_mean_squared_error: 54.9799\n",
      "Epoch 480/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7744 - mean_squared_error: 56.7744 - val_loss: 54.9857 - val_mean_squared_error: 54.9857\n",
      "Epoch 481/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7353 - mean_squared_error: 56.7353 - val_loss: 54.9805 - val_mean_squared_error: 54.9805\n",
      "Epoch 482/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.8109 - mean_squared_error: 56.8109 - val_loss: 54.9592 - val_mean_squared_error: 54.9592\n",
      "Epoch 483/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.7862 - mean_squared_error: 56.7862 - val_loss: 55.0426 - val_mean_squared_error: 55.0427\n",
      "Epoch 484/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8338 - mean_squared_error: 56.8338 - val_loss: 55.0660 - val_mean_squared_error: 55.0660\n",
      "Epoch 485/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.7465 - mean_squared_error: 56.7465 - val_loss: 55.8341 - val_mean_squared_error: 55.8341\n",
      "Epoch 486/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7624 - mean_squared_error: 56.7624 - val_loss: 55.0450 - val_mean_squared_error: 55.0450\n",
      "Epoch 487/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6958 - mean_squared_error: 56.6958 - val_loss: 55.2713 - val_mean_squared_error: 55.2713\n",
      "Epoch 488/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.8514 - mean_squared_error: 56.8514 - val_loss: 54.9947 - val_mean_squared_error: 54.9947\n",
      "Epoch 489/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7882 - mean_squared_error: 56.7882 - val_loss: 55.0787 - val_mean_squared_error: 55.0787\n",
      "Epoch 490/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7606 - mean_squared_error: 56.7606 - val_loss: 55.0366 - val_mean_squared_error: 55.0366\n",
      "Epoch 491/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6429 - mean_squared_error: 56.6429 - val_loss: 55.1156 - val_mean_squared_error: 55.1156\n",
      "Epoch 492/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7096 - mean_squared_error: 56.7096 - val_loss: 55.0590 - val_mean_squared_error: 55.0590\n",
      "Epoch 493/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6719 - mean_squared_error: 56.6719 - val_loss: 55.4738 - val_mean_squared_error: 55.4738\n",
      "Epoch 494/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8502 - mean_squared_error: 56.8502 - val_loss: 54.9183 - val_mean_squared_error: 54.9183\n",
      "Epoch 495/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7142 - mean_squared_error: 56.7142 - val_loss: 55.0999 - val_mean_squared_error: 55.0999\n",
      "Epoch 496/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.6879 - mean_squared_error: 56.6879 - val_loss: 55.3530 - val_mean_squared_error: 55.3530\n",
      "Epoch 497/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7463 - mean_squared_error: 56.7463 - val_loss: 55.5145 - val_mean_squared_error: 55.5145\n",
      "Epoch 498/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.7920 - mean_squared_error: 56.7920 - val_loss: 54.9882 - val_mean_squared_error: 54.9882\n",
      "Epoch 499/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6782 - mean_squared_error: 56.6782 - val_loss: 54.9447 - val_mean_squared_error: 54.9447\n",
      "Epoch 500/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.6085 - mean_squared_error: 56.6085 - val_loss: 55.6493 - val_mean_squared_error: 55.6493\n",
      "Epoch 501/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6821 - mean_squared_error: 56.6821 - val_loss: 55.0659 - val_mean_squared_error: 55.0659\n",
      "Epoch 502/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6071 - mean_squared_error: 56.6071 - val_loss: 55.0099 - val_mean_squared_error: 55.0099\n",
      "Epoch 503/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6991 - mean_squared_error: 56.6991 - val_loss: 54.9177 - val_mean_squared_error: 54.9177\n",
      "Epoch 504/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6960 - mean_squared_error: 56.6960 - val_loss: 55.5001 - val_mean_squared_error: 55.5001\n",
      "Epoch 505/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6209 - mean_squared_error: 56.6209 - val_loss: 55.1940 - val_mean_squared_error: 55.1940\n",
      "Epoch 506/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7284 - mean_squared_error: 56.7284 - val_loss: 55.1284 - val_mean_squared_error: 55.1284\n",
      "Epoch 507/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6468 - mean_squared_error: 56.6468 - val_loss: 54.9948 - val_mean_squared_error: 54.9949\n",
      "Epoch 508/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8062 - mean_squared_error: 56.8062 - val_loss: 55.0185 - val_mean_squared_error: 55.0185\n",
      "Epoch 509/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.6292 - mean_squared_error: 56.6292 - val_loss: 54.9622 - val_mean_squared_error: 54.9622\n",
      "Epoch 510/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7113 - mean_squared_error: 56.7113 - val_loss: 55.2778 - val_mean_squared_error: 55.2778\n",
      "Epoch 511/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7004 - mean_squared_error: 56.7005 - val_loss: 55.0405 - val_mean_squared_error: 55.0405\n",
      "Epoch 512/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.6760 - mean_squared_error: 56.6760 - val_loss: 54.9465 - val_mean_squared_error: 54.9465\n",
      "Epoch 513/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7214 - mean_squared_error: 56.7214 - val_loss: 54.9763 - val_mean_squared_error: 54.9763\n",
      "Epoch 514/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6469 - mean_squared_error: 56.6470 - val_loss: 55.0688 - val_mean_squared_error: 55.0688\n",
      "Epoch 515/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6370 - mean_squared_error: 56.6370 - val_loss: 54.9280 - val_mean_squared_error: 54.9280\n",
      "Epoch 516/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6775 - mean_squared_error: 56.6775 - val_loss: 54.9119 - val_mean_squared_error: 54.9119\n",
      "Epoch 517/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6348 - mean_squared_error: 56.6348 - val_loss: 55.3606 - val_mean_squared_error: 55.3606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6908 - mean_squared_error: 56.6908 - val_loss: 55.0183 - val_mean_squared_error: 55.0183\n",
      "Epoch 519/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.6254 - mean_squared_error: 56.6254 - val_loss: 55.0978 - val_mean_squared_error: 55.0978\n",
      "Epoch 520/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6518 - mean_squared_error: 56.6518 - val_loss: 55.2655 - val_mean_squared_error: 55.2654\n",
      "Epoch 521/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6616 - mean_squared_error: 56.6616 - val_loss: 54.9100 - val_mean_squared_error: 54.9100\n",
      "Epoch 522/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.6004 - mean_squared_error: 56.6004 - val_loss: 54.9779 - val_mean_squared_error: 54.9779\n",
      "Epoch 523/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.7011 - mean_squared_error: 56.7011 - val_loss: 55.2541 - val_mean_squared_error: 55.2541\n",
      "Epoch 524/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.6382 - mean_squared_error: 56.6383 - val_loss: 54.9439 - val_mean_squared_error: 54.9439\n",
      "Epoch 525/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.6197 - mean_squared_error: 56.6197 - val_loss: 54.9010 - val_mean_squared_error: 54.9010\n",
      "Epoch 526/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5584 - mean_squared_error: 56.5584 - val_loss: 55.0239 - val_mean_squared_error: 55.0239\n",
      "Epoch 527/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6629 - mean_squared_error: 56.6629 - val_loss: 54.9549 - val_mean_squared_error: 54.9549\n",
      "Epoch 528/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.6121 - mean_squared_error: 56.6121 - val_loss: 54.9210 - val_mean_squared_error: 54.9210\n",
      "Epoch 529/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6227 - mean_squared_error: 56.6227 - val_loss: 54.9267 - val_mean_squared_error: 54.9267\n",
      "Epoch 530/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.6455 - mean_squared_error: 56.6455 - val_loss: 54.9190 - val_mean_squared_error: 54.9190\n",
      "Epoch 531/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6654 - mean_squared_error: 56.6654 - val_loss: 54.9870 - val_mean_squared_error: 54.9870\n",
      "Epoch 532/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6465 - mean_squared_error: 56.6465 - val_loss: 55.0457 - val_mean_squared_error: 55.0457\n",
      "Epoch 533/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5448 - mean_squared_error: 56.5448 - val_loss: 55.4099 - val_mean_squared_error: 55.4099\n",
      "Epoch 534/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5187 - mean_squared_error: 56.5187 - val_loss: 55.0751 - val_mean_squared_error: 55.0751\n",
      "Epoch 535/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.6043 - mean_squared_error: 56.6043 - val_loss: 54.9237 - val_mean_squared_error: 54.9237\n",
      "Epoch 536/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5413 - mean_squared_error: 56.5413 - val_loss: 55.2240 - val_mean_squared_error: 55.2240\n",
      "Epoch 537/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.7270 - mean_squared_error: 56.7271 - val_loss: 54.9384 - val_mean_squared_error: 54.9384\n",
      "Epoch 538/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5821 - mean_squared_error: 56.5821 - val_loss: 54.9171 - val_mean_squared_error: 54.9171\n",
      "Epoch 539/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5769 - mean_squared_error: 56.5768 - val_loss: 54.9368 - val_mean_squared_error: 54.9368\n",
      "Epoch 540/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5588 - mean_squared_error: 56.5587 - val_loss: 54.9289 - val_mean_squared_error: 54.9289\n",
      "Epoch 541/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6675 - mean_squared_error: 56.6675 - val_loss: 55.0551 - val_mean_squared_error: 55.0551\n",
      "Epoch 542/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.8183 - mean_squared_error: 56.8183 - val_loss: 54.9094 - val_mean_squared_error: 54.9094\n",
      "Epoch 543/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5727 - mean_squared_error: 56.5727 - val_loss: 55.0054 - val_mean_squared_error: 55.0054\n",
      "Epoch 544/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.6976 - mean_squared_error: 56.6976 - val_loss: 55.0173 - val_mean_squared_error: 55.0173\n",
      "Epoch 545/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5060 - mean_squared_error: 56.5060 - val_loss: 56.0030 - val_mean_squared_error: 56.0030\n",
      "Epoch 546/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6771 - mean_squared_error: 56.6771 - val_loss: 54.9317 - val_mean_squared_error: 54.9318\n",
      "Epoch 547/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.6579 - mean_squared_error: 56.6579 - val_loss: 54.8944 - val_mean_squared_error: 54.8943\n",
      "Epoch 548/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5016 - mean_squared_error: 56.5016 - val_loss: 55.1708 - val_mean_squared_error: 55.1708\n",
      "Epoch 549/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4847 - mean_squared_error: 56.4847 - val_loss: 54.9954 - val_mean_squared_error: 54.9954\n",
      "Epoch 550/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5224 - mean_squared_error: 56.5224 - val_loss: 54.9608 - val_mean_squared_error: 54.9609\n",
      "Epoch 551/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5556 - mean_squared_error: 56.5555 - val_loss: 55.0401 - val_mean_squared_error: 55.0401\n",
      "Epoch 552/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5276 - mean_squared_error: 56.5276 - val_loss: 55.3182 - val_mean_squared_error: 55.3182\n",
      "Epoch 553/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.7558 - mean_squared_error: 56.7558 - val_loss: 54.9242 - val_mean_squared_error: 54.9243\n",
      "Epoch 554/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.5447 - mean_squared_error: 56.5447 - val_loss: 55.8776 - val_mean_squared_error: 55.8776\n",
      "Epoch 555/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4957 - mean_squared_error: 56.4957 - val_loss: 55.4029 - val_mean_squared_error: 55.4029\n",
      "Epoch 556/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4510 - mean_squared_error: 56.4510 - val_loss: 55.2056 - val_mean_squared_error: 55.2056\n",
      "Epoch 557/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6271 - mean_squared_error: 56.6271 - val_loss: 55.0631 - val_mean_squared_error: 55.0631\n",
      "Epoch 558/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5702 - mean_squared_error: 56.5702 - val_loss: 55.7678 - val_mean_squared_error: 55.7678\n",
      "Epoch 559/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.6233 - mean_squared_error: 56.6233 - val_loss: 55.0924 - val_mean_squared_error: 55.0924\n",
      "Epoch 560/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5524 - mean_squared_error: 56.5524 - val_loss: 54.8835 - val_mean_squared_error: 54.8835\n",
      "Epoch 561/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5783 - mean_squared_error: 56.5784 - val_loss: 55.0617 - val_mean_squared_error: 55.0617\n",
      "Epoch 562/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5507 - mean_squared_error: 56.5507 - val_loss: 54.8936 - val_mean_squared_error: 54.8935\n",
      "Epoch 563/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.6516 - mean_squared_error: 56.6516 - val_loss: 55.1415 - val_mean_squared_error: 55.1415\n",
      "Epoch 564/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4994 - mean_squared_error: 56.4994 - val_loss: 54.9300 - val_mean_squared_error: 54.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4829 - mean_squared_error: 56.4829 - val_loss: 54.9732 - val_mean_squared_error: 54.9732\n",
      "Epoch 566/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5805 - mean_squared_error: 56.5805 - val_loss: 54.9882 - val_mean_squared_error: 54.9882\n",
      "Epoch 567/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5576 - mean_squared_error: 56.5576 - val_loss: 54.9249 - val_mean_squared_error: 54.9249\n",
      "Epoch 568/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.4691 - mean_squared_error: 56.4691 - val_loss: 54.8755 - val_mean_squared_error: 54.8755\n",
      "Epoch 569/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.4971 - mean_squared_error: 56.4971 - val_loss: 54.9397 - val_mean_squared_error: 54.9397\n",
      "Epoch 570/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4307 - mean_squared_error: 56.4307 - val_loss: 54.9621 - val_mean_squared_error: 54.9621\n",
      "Epoch 571/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5420 - mean_squared_error: 56.5420 - val_loss: 54.8757 - val_mean_squared_error: 54.8757\n",
      "Epoch 572/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5161 - mean_squared_error: 56.5161 - val_loss: 55.0772 - val_mean_squared_error: 55.0772\n",
      "Epoch 573/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4930 - mean_squared_error: 56.4930 - val_loss: 54.8840 - val_mean_squared_error: 54.8840\n",
      "Epoch 574/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5469 - mean_squared_error: 56.5469 - val_loss: 54.8781 - val_mean_squared_error: 54.8781\n",
      "Epoch 575/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.4671 - mean_squared_error: 56.4671 - val_loss: 54.9700 - val_mean_squared_error: 54.9700\n",
      "Epoch 576/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5129 - mean_squared_error: 56.5129 - val_loss: 54.9208 - val_mean_squared_error: 54.9208\n",
      "Epoch 577/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.6302 - mean_squared_error: 56.6302 - val_loss: 55.2225 - val_mean_squared_error: 55.2225\n",
      "Epoch 578/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5466 - mean_squared_error: 56.5466 - val_loss: 54.9632 - val_mean_squared_error: 54.9632\n",
      "Epoch 579/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4363 - mean_squared_error: 56.4363 - val_loss: 54.9180 - val_mean_squared_error: 54.9180\n",
      "Epoch 580/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5232 - mean_squared_error: 56.5232 - val_loss: 55.5968 - val_mean_squared_error: 55.5968\n",
      "Epoch 581/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5326 - mean_squared_error: 56.5326 - val_loss: 55.5305 - val_mean_squared_error: 55.5305\n",
      "Epoch 582/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4467 - mean_squared_error: 56.4467 - val_loss: 54.9001 - val_mean_squared_error: 54.9001\n",
      "Epoch 583/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.5705 - mean_squared_error: 56.5705 - val_loss: 55.1368 - val_mean_squared_error: 55.1368\n",
      "Epoch 584/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4860 - mean_squared_error: 56.4860 - val_loss: 55.0959 - val_mean_squared_error: 55.0959\n",
      "Epoch 585/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4260 - mean_squared_error: 56.4260 - val_loss: 54.9614 - val_mean_squared_error: 54.9614\n",
      "Epoch 586/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5411 - mean_squared_error: 56.5411 - val_loss: 54.9712 - val_mean_squared_error: 54.9712\n",
      "Epoch 587/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5966 - mean_squared_error: 56.5965 - val_loss: 54.9167 - val_mean_squared_error: 54.9167\n",
      "Epoch 588/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4426 - mean_squared_error: 56.4426 - val_loss: 55.0972 - val_mean_squared_error: 55.0973\n",
      "Epoch 589/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.4931 - mean_squared_error: 56.4931 - val_loss: 55.6466 - val_mean_squared_error: 55.6466\n",
      "Epoch 590/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4678 - mean_squared_error: 56.4678 - val_loss: 56.0611 - val_mean_squared_error: 56.0611\n",
      "Epoch 591/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4927 - mean_squared_error: 56.4927 - val_loss: 55.1333 - val_mean_squared_error: 55.1333\n",
      "Epoch 592/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4585 - mean_squared_error: 56.4585 - val_loss: 54.9209 - val_mean_squared_error: 54.9209\n",
      "Epoch 593/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4471 - mean_squared_error: 56.4471 - val_loss: 55.0120 - val_mean_squared_error: 55.0120\n",
      "Epoch 594/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4435 - mean_squared_error: 56.4434 - val_loss: 54.9008 - val_mean_squared_error: 54.9007\n",
      "Epoch 595/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4539 - mean_squared_error: 56.4538 - val_loss: 55.0021 - val_mean_squared_error: 55.0021\n",
      "Epoch 596/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4569 - mean_squared_error: 56.4569 - val_loss: 55.0037 - val_mean_squared_error: 55.0037\n",
      "Epoch 597/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.5454 - mean_squared_error: 56.5454 - val_loss: 54.9008 - val_mean_squared_error: 54.9008\n",
      "Epoch 598/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2697 - mean_squared_error: 56.2697 - val_loss: 55.2030 - val_mean_squared_error: 55.2030\n",
      "Epoch 599/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.5344 - mean_squared_error: 56.5344 - val_loss: 55.6353 - val_mean_squared_error: 55.6353\n",
      "Epoch 600/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4873 - mean_squared_error: 56.4873 - val_loss: 54.9117 - val_mean_squared_error: 54.9117\n",
      "Epoch 601/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4387 - mean_squared_error: 56.4387 - val_loss: 54.9112 - val_mean_squared_error: 54.9112\n",
      "Epoch 602/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3634 - mean_squared_error: 56.3634 - val_loss: 55.0229 - val_mean_squared_error: 55.0229\n",
      "Epoch 603/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4582 - mean_squared_error: 56.4582 - val_loss: 54.9794 - val_mean_squared_error: 54.9794\n",
      "Epoch 604/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2806 - mean_squared_error: 56.2806 - val_loss: 55.9291 - val_mean_squared_error: 55.9291\n",
      "Epoch 605/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.5167 - mean_squared_error: 56.5167 - val_loss: 54.8475 - val_mean_squared_error: 54.8475\n",
      "Epoch 606/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.5656 - mean_squared_error: 56.5656 - val_loss: 55.0971 - val_mean_squared_error: 55.0971\n",
      "Epoch 607/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4156 - mean_squared_error: 56.4156 - val_loss: 54.8831 - val_mean_squared_error: 54.8831\n",
      "Epoch 608/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4170 - mean_squared_error: 56.4170 - val_loss: 54.9372 - val_mean_squared_error: 54.9372\n",
      "Epoch 609/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3776 - mean_squared_error: 56.3776 - val_loss: 54.8989 - val_mean_squared_error: 54.8989\n",
      "Epoch 610/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.3914 - mean_squared_error: 56.3914 - val_loss: 54.9848 - val_mean_squared_error: 54.9848\n",
      "Epoch 611/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3800 - mean_squared_error: 56.3800 - val_loss: 54.8741 - val_mean_squared_error: 54.8741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 612/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.4217 - mean_squared_error: 56.4217 - val_loss: 54.8505 - val_mean_squared_error: 54.8505\n",
      "Epoch 613/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.5032 - mean_squared_error: 56.5032 - val_loss: 54.8654 - val_mean_squared_error: 54.8654\n",
      "Epoch 614/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.5016 - mean_squared_error: 56.5016 - val_loss: 54.8818 - val_mean_squared_error: 54.8818\n",
      "Epoch 615/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4826 - mean_squared_error: 56.4826 - val_loss: 54.8461 - val_mean_squared_error: 54.8461\n",
      "Epoch 616/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3852 - mean_squared_error: 56.3853 - val_loss: 54.9461 - val_mean_squared_error: 54.9461\n",
      "Epoch 617/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3301 - mean_squared_error: 56.3301 - val_loss: 54.8742 - val_mean_squared_error: 54.8742\n",
      "Epoch 618/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3256 - mean_squared_error: 56.3256 - val_loss: 54.8772 - val_mean_squared_error: 54.8772\n",
      "Epoch 619/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3866 - mean_squared_error: 56.3866 - val_loss: 54.8521 - val_mean_squared_error: 54.8521\n",
      "Epoch 620/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4258 - mean_squared_error: 56.4258 - val_loss: 55.1341 - val_mean_squared_error: 55.1341\n",
      "Epoch 621/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3923 - mean_squared_error: 56.3923 - val_loss: 54.9400 - val_mean_squared_error: 54.9400\n",
      "Epoch 622/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.4037 - mean_squared_error: 56.4037 - val_loss: 55.4825 - val_mean_squared_error: 55.4825\n",
      "Epoch 623/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3502 - mean_squared_error: 56.3502 - val_loss: 54.9390 - val_mean_squared_error: 54.9390\n",
      "Epoch 624/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3560 - mean_squared_error: 56.3560 - val_loss: 55.2040 - val_mean_squared_error: 55.2040\n",
      "Epoch 625/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3566 - mean_squared_error: 56.3566 - val_loss: 55.3026 - val_mean_squared_error: 55.3026\n",
      "Epoch 626/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4460 - mean_squared_error: 56.4460 - val_loss: 54.8992 - val_mean_squared_error: 54.8992\n",
      "Epoch 627/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4422 - mean_squared_error: 56.4422 - val_loss: 54.8874 - val_mean_squared_error: 54.8874\n",
      "Epoch 628/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3416 - mean_squared_error: 56.3416 - val_loss: 54.8593 - val_mean_squared_error: 54.8593\n",
      "Epoch 629/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3677 - mean_squared_error: 56.3677 - val_loss: 55.3960 - val_mean_squared_error: 55.3960\n",
      "Epoch 630/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1972 - mean_squared_error: 56.1972 - val_loss: 54.8728 - val_mean_squared_error: 54.8728\n",
      "Epoch 631/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3677 - mean_squared_error: 56.3676 - val_loss: 55.6160 - val_mean_squared_error: 55.6160\n",
      "Epoch 632/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.3705 - mean_squared_error: 56.3705 - val_loss: 54.9195 - val_mean_squared_error: 54.9195\n",
      "Epoch 633/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3954 - mean_squared_error: 56.3954 - val_loss: 55.1853 - val_mean_squared_error: 55.1853\n",
      "Epoch 634/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3361 - mean_squared_error: 56.3361 - val_loss: 55.1022 - val_mean_squared_error: 55.1022\n",
      "Epoch 635/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.3267 - mean_squared_error: 56.3267 - val_loss: 54.8725 - val_mean_squared_error: 54.8725\n",
      "Epoch 636/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3634 - mean_squared_error: 56.3634 - val_loss: 54.9437 - val_mean_squared_error: 54.9437\n",
      "Epoch 637/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.3779 - mean_squared_error: 56.3779 - val_loss: 55.0124 - val_mean_squared_error: 55.0124\n",
      "Epoch 638/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3833 - mean_squared_error: 56.3833 - val_loss: 54.8844 - val_mean_squared_error: 54.8844\n",
      "Epoch 639/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3105 - mean_squared_error: 56.3105 - val_loss: 54.8505 - val_mean_squared_error: 54.8505\n",
      "Epoch 640/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.3168 - mean_squared_error: 56.3168 - val_loss: 54.9835 - val_mean_squared_error: 54.9835\n",
      "Epoch 641/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.2292 - mean_squared_error: 56.2292 - val_loss: 55.1020 - val_mean_squared_error: 55.1020\n",
      "Epoch 642/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 56.3109 - mean_squared_error: 56.3109 - val_loss: 54.9491 - val_mean_squared_error: 54.9491\n",
      "Epoch 643/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2473 - mean_squared_error: 56.2473 - val_loss: 54.8432 - val_mean_squared_error: 54.8432\n",
      "Epoch 644/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2857 - mean_squared_error: 56.2858 - val_loss: 55.0702 - val_mean_squared_error: 55.0702\n",
      "Epoch 645/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.4620 - mean_squared_error: 56.4621 - val_loss: 54.9302 - val_mean_squared_error: 54.9302\n",
      "Epoch 646/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3580 - mean_squared_error: 56.3580 - val_loss: 54.8589 - val_mean_squared_error: 54.8589\n",
      "Epoch 647/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.2248 - mean_squared_error: 56.2248 - val_loss: 54.8589 - val_mean_squared_error: 54.8589\n",
      "Epoch 648/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2941 - mean_squared_error: 56.2942 - val_loss: 55.1354 - val_mean_squared_error: 55.1354\n",
      "Epoch 649/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.3247 - mean_squared_error: 56.3247 - val_loss: 55.1356 - val_mean_squared_error: 55.1356\n",
      "Epoch 650/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4477 - mean_squared_error: 56.4477 - val_loss: 55.0645 - val_mean_squared_error: 55.0645\n",
      "Epoch 651/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4896 - mean_squared_error: 56.4896 - val_loss: 54.9109 - val_mean_squared_error: 54.9109\n",
      "Epoch 652/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.3603 - mean_squared_error: 56.3603 - val_loss: 55.2173 - val_mean_squared_error: 55.2173\n",
      "Epoch 653/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3002 - mean_squared_error: 56.3002 - val_loss: 54.9254 - val_mean_squared_error: 54.9254\n",
      "Epoch 654/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4694 - mean_squared_error: 56.4694 - val_loss: 54.8646 - val_mean_squared_error: 54.8646\n",
      "Epoch 655/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3160 - mean_squared_error: 56.3160 - val_loss: 54.9769 - val_mean_squared_error: 54.9769\n",
      "Epoch 656/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2875 - mean_squared_error: 56.2875 - val_loss: 55.0725 - val_mean_squared_error: 55.0725\n",
      "Epoch 657/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.2678 - mean_squared_error: 56.2677 - val_loss: 54.8911 - val_mean_squared_error: 54.8911\n",
      "Epoch 658/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1755 - mean_squared_error: 56.1755 - val_loss: 54.8566 - val_mean_squared_error: 54.8566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3598 - mean_squared_error: 56.3597 - val_loss: 55.5323 - val_mean_squared_error: 55.5323\n",
      "Epoch 660/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3265 - mean_squared_error: 56.3265 - val_loss: 55.0248 - val_mean_squared_error: 55.0248\n",
      "Epoch 661/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.3128 - mean_squared_error: 56.3128 - val_loss: 54.8679 - val_mean_squared_error: 54.8679\n",
      "Epoch 662/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3824 - mean_squared_error: 56.3824 - val_loss: 55.3750 - val_mean_squared_error: 55.3750\n",
      "Epoch 663/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.4484 - mean_squared_error: 56.4483 - val_loss: 55.1134 - val_mean_squared_error: 55.1134\n",
      "Epoch 664/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.3036 - mean_squared_error: 56.3036 - val_loss: 54.9063 - val_mean_squared_error: 54.9063\n",
      "Epoch 665/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.4021 - mean_squared_error: 56.4021 - val_loss: 54.8166 - val_mean_squared_error: 54.8166\n",
      "Epoch 666/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2977 - mean_squared_error: 56.2977 - val_loss: 54.9924 - val_mean_squared_error: 54.9924\n",
      "Epoch 667/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3307 - mean_squared_error: 56.3307 - val_loss: 55.4547 - val_mean_squared_error: 55.4547\n",
      "Epoch 668/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.3184 - mean_squared_error: 56.3184 - val_loss: 54.9126 - val_mean_squared_error: 54.9127\n",
      "Epoch 669/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.2719 - mean_squared_error: 56.2719 - val_loss: 54.9707 - val_mean_squared_error: 54.9707\n",
      "Epoch 670/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3110 - mean_squared_error: 56.3110 - val_loss: 55.1638 - val_mean_squared_error: 55.1638\n",
      "Epoch 671/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3145 - mean_squared_error: 56.3145 - val_loss: 54.9019 - val_mean_squared_error: 54.9019\n",
      "Epoch 672/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.3177 - mean_squared_error: 56.3177 - val_loss: 54.8253 - val_mean_squared_error: 54.8253\n",
      "Epoch 673/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3026 - mean_squared_error: 56.3027 - val_loss: 54.9439 - val_mean_squared_error: 54.9439\n",
      "Epoch 674/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2494 - mean_squared_error: 56.2494 - val_loss: 54.9038 - val_mean_squared_error: 54.9038\n",
      "Epoch 675/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.2928 - mean_squared_error: 56.2928 - val_loss: 54.9106 - val_mean_squared_error: 54.9106\n",
      "Epoch 676/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2713 - mean_squared_error: 56.2713 - val_loss: 55.1835 - val_mean_squared_error: 55.1835\n",
      "Epoch 677/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2366 - mean_squared_error: 56.2367 - val_loss: 54.9735 - val_mean_squared_error: 54.9735\n",
      "Epoch 678/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.2211 - mean_squared_error: 56.2211 - val_loss: 54.9357 - val_mean_squared_error: 54.9357\n",
      "Epoch 679/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2930 - mean_squared_error: 56.2930 - val_loss: 55.1372 - val_mean_squared_error: 55.1372\n",
      "Epoch 680/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2400 - mean_squared_error: 56.2400 - val_loss: 55.0401 - val_mean_squared_error: 55.0401\n",
      "Epoch 681/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3047 - mean_squared_error: 56.3047 - val_loss: 54.8772 - val_mean_squared_error: 54.8772\n",
      "Epoch 682/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2692 - mean_squared_error: 56.2692 - val_loss: 55.2458 - val_mean_squared_error: 55.2458\n",
      "Epoch 683/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.2161 - mean_squared_error: 56.2161 - val_loss: 55.3541 - val_mean_squared_error: 55.3541\n",
      "Epoch 684/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2545 - mean_squared_error: 56.2545 - val_loss: 55.1649 - val_mean_squared_error: 55.1649\n",
      "Epoch 685/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.3077 - mean_squared_error: 56.3077 - val_loss: 54.8470 - val_mean_squared_error: 54.8470\n",
      "Epoch 686/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1761 - mean_squared_error: 56.1762 - val_loss: 54.8506 - val_mean_squared_error: 54.8506\n",
      "Epoch 687/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1317 - mean_squared_error: 56.1317 - val_loss: 54.9442 - val_mean_squared_error: 54.9442\n",
      "Epoch 688/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1432 - mean_squared_error: 56.1432 - val_loss: 55.7829 - val_mean_squared_error: 55.7829\n",
      "Epoch 689/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3239 - mean_squared_error: 56.3239 - val_loss: 54.8961 - val_mean_squared_error: 54.8962\n",
      "Epoch 690/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.3384 - mean_squared_error: 56.3384 - val_loss: 54.9261 - val_mean_squared_error: 54.9261\n",
      "Epoch 691/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.3097 - mean_squared_error: 56.3097 - val_loss: 54.8586 - val_mean_squared_error: 54.8586\n",
      "Epoch 692/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2223 - mean_squared_error: 56.2223 - val_loss: 55.8133 - val_mean_squared_error: 55.8133\n",
      "Epoch 693/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.2881 - mean_squared_error: 56.2881 - val_loss: 55.5305 - val_mean_squared_error: 55.5305\n",
      "Epoch 694/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2656 - mean_squared_error: 56.2656 - val_loss: 54.8254 - val_mean_squared_error: 54.8254\n",
      "Epoch 695/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2575 - mean_squared_error: 56.2575 - val_loss: 54.8195 - val_mean_squared_error: 54.8195\n",
      "Epoch 696/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1325 - mean_squared_error: 56.1325 - val_loss: 55.1965 - val_mean_squared_error: 55.1965\n",
      "Epoch 697/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.3462 - mean_squared_error: 56.3462 - val_loss: 54.8798 - val_mean_squared_error: 54.8798\n",
      "Epoch 698/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1785 - mean_squared_error: 56.1785 - val_loss: 54.9924 - val_mean_squared_error: 54.9924\n",
      "Epoch 699/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1290 - mean_squared_error: 56.1291 - val_loss: 55.0044 - val_mean_squared_error: 55.0044\n",
      "Epoch 700/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1291 - mean_squared_error: 56.1291 - val_loss: 55.0606 - val_mean_squared_error: 55.0606\n",
      "Epoch 701/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.3091 - mean_squared_error: 56.3091 - val_loss: 55.0662 - val_mean_squared_error: 55.0662\n",
      "Epoch 702/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2002 - mean_squared_error: 56.2002 - val_loss: 55.2379 - val_mean_squared_error: 55.2379\n",
      "Epoch 703/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.4064 - mean_squared_error: 56.4064 - val_loss: 54.8226 - val_mean_squared_error: 54.8226\n",
      "Epoch 704/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1658 - mean_squared_error: 56.1658 - val_loss: 55.0841 - val_mean_squared_error: 55.0841\n",
      "Epoch 705/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1498 - mean_squared_error: 56.1498 - val_loss: 54.9627 - val_mean_squared_error: 54.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1852 - mean_squared_error: 56.1852 - val_loss: 54.9143 - val_mean_squared_error: 54.9142\n",
      "Epoch 707/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.2711 - mean_squared_error: 56.2711 - val_loss: 55.0173 - val_mean_squared_error: 55.0173\n",
      "Epoch 708/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2015 - mean_squared_error: 56.2015 - val_loss: 54.9801 - val_mean_squared_error: 54.9801\n",
      "Epoch 709/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2083 - mean_squared_error: 56.2083 - val_loss: 54.8751 - val_mean_squared_error: 54.8751\n",
      "Epoch 710/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1941 - mean_squared_error: 56.1941 - val_loss: 54.8245 - val_mean_squared_error: 54.8245\n",
      "Epoch 711/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1966 - mean_squared_error: 56.1966 - val_loss: 55.0427 - val_mean_squared_error: 55.0428\n",
      "Epoch 712/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1690 - mean_squared_error: 56.1690 - val_loss: 55.1622 - val_mean_squared_error: 55.1622\n",
      "Epoch 713/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2703 - mean_squared_error: 56.2704 - val_loss: 54.9424 - val_mean_squared_error: 54.9423\n",
      "Epoch 714/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1190 - mean_squared_error: 56.1190 - val_loss: 55.0240 - val_mean_squared_error: 55.0240\n",
      "Epoch 715/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1014 - mean_squared_error: 56.1014 - val_loss: 54.8852 - val_mean_squared_error: 54.8852\n",
      "Epoch 716/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.1866 - mean_squared_error: 56.1866 - val_loss: 55.3195 - val_mean_squared_error: 55.3195\n",
      "Epoch 717/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2400 - mean_squared_error: 56.2400 - val_loss: 54.9074 - val_mean_squared_error: 54.9074\n",
      "Epoch 718/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0852 - mean_squared_error: 56.0853 - val_loss: 55.0397 - val_mean_squared_error: 55.0397\n",
      "Epoch 719/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1843 - mean_squared_error: 56.1843 - val_loss: 54.9040 - val_mean_squared_error: 54.9040\n",
      "Epoch 720/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1743 - mean_squared_error: 56.1744 - val_loss: 54.8270 - val_mean_squared_error: 54.8270\n",
      "Epoch 721/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1310 - mean_squared_error: 56.1309 - val_loss: 54.8375 - val_mean_squared_error: 54.8375\n",
      "Epoch 722/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.1161 - mean_squared_error: 56.1161 - val_loss: 54.8356 - val_mean_squared_error: 54.8356\n",
      "Epoch 723/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1669 - mean_squared_error: 56.1669 - val_loss: 54.9828 - val_mean_squared_error: 54.9828\n",
      "Epoch 724/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2416 - mean_squared_error: 56.2416 - val_loss: 54.8552 - val_mean_squared_error: 54.8551\n",
      "Epoch 725/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.1289 - mean_squared_error: 56.1288 - val_loss: 55.3948 - val_mean_squared_error: 55.3948\n",
      "Epoch 726/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0842 - mean_squared_error: 56.0842 - val_loss: 55.2362 - val_mean_squared_error: 55.2361\n",
      "Epoch 727/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2084 - mean_squared_error: 56.2084 - val_loss: 55.9693 - val_mean_squared_error: 55.9693\n",
      "Epoch 728/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.1766 - mean_squared_error: 56.1766 - val_loss: 54.9867 - val_mean_squared_error: 54.9867\n",
      "Epoch 729/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.1783 - mean_squared_error: 56.1782 - val_loss: 54.8757 - val_mean_squared_error: 54.8757\n",
      "Epoch 730/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1832 - mean_squared_error: 56.1832 - val_loss: 54.9423 - val_mean_squared_error: 54.9423\n",
      "Epoch 731/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1293 - mean_squared_error: 56.1293 - val_loss: 54.8203 - val_mean_squared_error: 54.8203\n",
      "Epoch 732/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1565 - mean_squared_error: 56.1565 - val_loss: 54.8328 - val_mean_squared_error: 54.8328\n",
      "Epoch 733/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.2135 - mean_squared_error: 56.2135 - val_loss: 54.8401 - val_mean_squared_error: 54.8401\n",
      "Epoch 734/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.1861 - mean_squared_error: 56.1861 - val_loss: 54.8423 - val_mean_squared_error: 54.8423\n",
      "Epoch 735/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1750 - mean_squared_error: 56.1750 - val_loss: 54.8318 - val_mean_squared_error: 54.8318\n",
      "Epoch 736/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1179 - mean_squared_error: 56.1179 - val_loss: 54.9684 - val_mean_squared_error: 54.9684\n",
      "Epoch 737/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1246 - mean_squared_error: 56.1246 - val_loss: 54.9955 - val_mean_squared_error: 54.9955\n",
      "Epoch 738/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.1458 - mean_squared_error: 56.1458 - val_loss: 54.8274 - val_mean_squared_error: 54.8274\n",
      "Epoch 739/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1179 - mean_squared_error: 56.1179 - val_loss: 54.8574 - val_mean_squared_error: 54.8574\n",
      "Epoch 740/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1775 - mean_squared_error: 56.1775 - val_loss: 55.1656 - val_mean_squared_error: 55.1656\n",
      "Epoch 741/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2003 - mean_squared_error: 56.2003 - val_loss: 54.8000 - val_mean_squared_error: 54.8000\n",
      "Epoch 742/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.1393 - mean_squared_error: 56.1393 - val_loss: 54.8862 - val_mean_squared_error: 54.8862\n",
      "Epoch 743/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0694 - mean_squared_error: 56.0694 - val_loss: 54.8013 - val_mean_squared_error: 54.8013\n",
      "Epoch 744/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0567 - mean_squared_error: 56.0567 - val_loss: 54.8029 - val_mean_squared_error: 54.8029\n",
      "Epoch 745/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0767 - mean_squared_error: 56.0767 - val_loss: 54.8640 - val_mean_squared_error: 54.8640\n",
      "Epoch 746/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1897 - mean_squared_error: 56.1897 - val_loss: 54.8244 - val_mean_squared_error: 54.8244\n",
      "Epoch 747/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.2399 - mean_squared_error: 56.2398 - val_loss: 54.8855 - val_mean_squared_error: 54.8855\n",
      "Epoch 748/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0519 - mean_squared_error: 56.0519 - val_loss: 54.9805 - val_mean_squared_error: 54.9805\n",
      "Epoch 749/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1119 - mean_squared_error: 56.1119 - val_loss: 55.0222 - val_mean_squared_error: 55.0222\n",
      "Epoch 750/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.1590 - mean_squared_error: 56.1590 - val_loss: 54.8314 - val_mean_squared_error: 54.8314\n",
      "Epoch 751/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.1566 - mean_squared_error: 56.1566 - val_loss: 55.1240 - val_mean_squared_error: 55.1240\n",
      "Epoch 752/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2503 - mean_squared_error: 56.2503 - val_loss: 55.9728 - val_mean_squared_error: 55.9728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.2419 - mean_squared_error: 56.2418 - val_loss: 54.8492 - val_mean_squared_error: 54.8491\n",
      "Epoch 754/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1142 - mean_squared_error: 56.1141 - val_loss: 55.0909 - val_mean_squared_error: 55.0909\n",
      "Epoch 755/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0855 - mean_squared_error: 56.0856 - val_loss: 55.0581 - val_mean_squared_error: 55.0581\n",
      "Epoch 756/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.1829 - mean_squared_error: 56.1829 - val_loss: 54.8336 - val_mean_squared_error: 54.8336\n",
      "Epoch 757/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0500 - mean_squared_error: 56.0500 - val_loss: 54.8235 - val_mean_squared_error: 54.8235\n",
      "Epoch 758/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0332 - mean_squared_error: 56.0332 - val_loss: 54.9580 - val_mean_squared_error: 54.9580\n",
      "Epoch 759/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0492 - mean_squared_error: 56.0492 - val_loss: 54.8300 - val_mean_squared_error: 54.8300\n",
      "Epoch 760/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.1374 - mean_squared_error: 56.1374 - val_loss: 54.8358 - val_mean_squared_error: 54.8358\n",
      "Epoch 761/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1160 - mean_squared_error: 56.1160 - val_loss: 54.8400 - val_mean_squared_error: 54.8400\n",
      "Epoch 762/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.2323 - mean_squared_error: 56.2323 - val_loss: 54.8383 - val_mean_squared_error: 54.8383\n",
      "Epoch 763/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1289 - mean_squared_error: 56.1289 - val_loss: 54.8255 - val_mean_squared_error: 54.8254\n",
      "Epoch 764/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1122 - mean_squared_error: 56.1122 - val_loss: 54.8080 - val_mean_squared_error: 54.8080\n",
      "Epoch 765/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0185 - mean_squared_error: 56.0185 - val_loss: 55.4601 - val_mean_squared_error: 55.4601\n",
      "Epoch 766/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0645 - mean_squared_error: 56.0645 - val_loss: 55.5593 - val_mean_squared_error: 55.5593\n",
      "Epoch 767/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.9768 - mean_squared_error: 55.9768 - val_loss: 55.0644 - val_mean_squared_error: 55.0644\n",
      "Epoch 768/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.2505 - mean_squared_error: 56.2505 - val_loss: 54.9831 - val_mean_squared_error: 54.9831\n",
      "Epoch 769/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 56.0055 - mean_squared_error: 56.0056 - val_loss: 54.9698 - val_mean_squared_error: 54.9698\n",
      "Epoch 770/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 56.0374 - mean_squared_error: 56.0375 - val_loss: 54.8614 - val_mean_squared_error: 54.8614\n",
      "Epoch 771/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0177 - mean_squared_error: 56.0177 - val_loss: 55.2593 - val_mean_squared_error: 55.2593\n",
      "Epoch 772/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0468 - mean_squared_error: 56.0468 - val_loss: 54.8036 - val_mean_squared_error: 54.8035\n",
      "Epoch 773/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9976 - mean_squared_error: 55.9976 - val_loss: 54.8193 - val_mean_squared_error: 54.8193\n",
      "Epoch 774/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1254 - mean_squared_error: 56.1254 - val_loss: 55.2095 - val_mean_squared_error: 55.2095\n",
      "Epoch 775/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1492 - mean_squared_error: 56.1492 - val_loss: 55.1121 - val_mean_squared_error: 55.1121\n",
      "Epoch 776/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0774 - mean_squared_error: 56.0774 - val_loss: 54.7806 - val_mean_squared_error: 54.7806\n",
      "Epoch 777/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.1246 - mean_squared_error: 56.1246 - val_loss: 54.7922 - val_mean_squared_error: 54.7921\n",
      "Epoch 778/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0580 - mean_squared_error: 56.0580 - val_loss: 54.7906 - val_mean_squared_error: 54.7906\n",
      "Epoch 779/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0642 - mean_squared_error: 56.0642 - val_loss: 54.8918 - val_mean_squared_error: 54.8917\n",
      "Epoch 780/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0370 - mean_squared_error: 56.0370 - val_loss: 55.4225 - val_mean_squared_error: 55.4225\n",
      "Epoch 781/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.1439 - mean_squared_error: 56.1439 - val_loss: 54.8968 - val_mean_squared_error: 54.8967\n",
      "Epoch 782/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0159 - mean_squared_error: 56.0159 - val_loss: 55.0947 - val_mean_squared_error: 55.0947\n",
      "Epoch 783/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0544 - mean_squared_error: 56.0544 - val_loss: 55.5886 - val_mean_squared_error: 55.5886\n",
      "Epoch 784/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0670 - mean_squared_error: 56.0669 - val_loss: 54.8616 - val_mean_squared_error: 54.8616\n",
      "Epoch 785/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.1031 - mean_squared_error: 56.1031 - val_loss: 54.8662 - val_mean_squared_error: 54.8662\n",
      "Epoch 786/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9123 - mean_squared_error: 55.9123 - val_loss: 54.8129 - val_mean_squared_error: 54.8129\n",
      "Epoch 787/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.1444 - mean_squared_error: 56.1444 - val_loss: 54.8277 - val_mean_squared_error: 54.8277\n",
      "Epoch 788/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0252 - mean_squared_error: 56.0252 - val_loss: 55.2372 - val_mean_squared_error: 55.2372\n",
      "Epoch 789/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0187 - mean_squared_error: 56.0187 - val_loss: 54.8564 - val_mean_squared_error: 54.8564\n",
      "Epoch 790/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0399 - mean_squared_error: 56.0399 - val_loss: 54.8307 - val_mean_squared_error: 54.8307\n",
      "Epoch 791/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8826 - mean_squared_error: 55.8826 - val_loss: 55.1773 - val_mean_squared_error: 55.1773\n",
      "Epoch 792/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0078 - mean_squared_error: 56.0078 - val_loss: 54.9880 - val_mean_squared_error: 54.9880\n",
      "Epoch 793/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1291 - mean_squared_error: 56.1291 - val_loss: 54.8060 - val_mean_squared_error: 54.8060\n",
      "Epoch 794/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9849 - mean_squared_error: 55.9849 - val_loss: 54.9986 - val_mean_squared_error: 54.9986\n",
      "Epoch 795/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0955 - mean_squared_error: 56.0955 - val_loss: 54.7925 - val_mean_squared_error: 54.7925\n",
      "Epoch 796/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9553 - mean_squared_error: 55.9553 - val_loss: 54.8025 - val_mean_squared_error: 54.8025\n",
      "Epoch 797/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0154 - mean_squared_error: 56.0154 - val_loss: 54.8296 - val_mean_squared_error: 54.8296\n",
      "Epoch 798/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0198 - mean_squared_error: 56.0198 - val_loss: 54.8853 - val_mean_squared_error: 54.8853\n",
      "Epoch 799/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 56.0762 - mean_squared_error: 56.0762 - val_loss: 54.9516 - val_mean_squared_error: 54.9516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 56.0625 - mean_squared_error: 56.0625 - val_loss: 54.8378 - val_mean_squared_error: 54.8378\n",
      "Epoch 801/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0718 - mean_squared_error: 56.0718 - val_loss: 55.5554 - val_mean_squared_error: 55.5554\n",
      "Epoch 802/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9717 - mean_squared_error: 55.9717 - val_loss: 55.1964 - val_mean_squared_error: 55.1964\n",
      "Epoch 803/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0570 - mean_squared_error: 56.0570 - val_loss: 54.7933 - val_mean_squared_error: 54.7933\n",
      "Epoch 804/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0142 - mean_squared_error: 56.0142 - val_loss: 54.8781 - val_mean_squared_error: 54.8781\n",
      "Epoch 805/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0321 - mean_squared_error: 56.0322 - val_loss: 54.8382 - val_mean_squared_error: 54.8382\n",
      "Epoch 806/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0496 - mean_squared_error: 56.0496 - val_loss: 54.8136 - val_mean_squared_error: 54.8136\n",
      "Epoch 807/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 56.0047 - mean_squared_error: 56.0047 - val_loss: 55.2386 - val_mean_squared_error: 55.2386\n",
      "Epoch 808/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.9673 - mean_squared_error: 55.9672 - val_loss: 54.8593 - val_mean_squared_error: 54.8593\n",
      "Epoch 809/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9014 - mean_squared_error: 55.9014 - val_loss: 54.9010 - val_mean_squared_error: 54.9009\n",
      "Epoch 810/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0381 - mean_squared_error: 56.0381 - val_loss: 54.7728 - val_mean_squared_error: 54.7728\n",
      "Epoch 811/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9673 - mean_squared_error: 55.9673 - val_loss: 54.9110 - val_mean_squared_error: 54.9110\n",
      "Epoch 812/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 56.0299 - mean_squared_error: 56.0299 - val_loss: 54.8979 - val_mean_squared_error: 54.8979\n",
      "Epoch 813/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0720 - mean_squared_error: 56.0720 - val_loss: 54.8680 - val_mean_squared_error: 54.8680\n",
      "Epoch 814/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9525 - mean_squared_error: 55.9524 - val_loss: 54.8228 - val_mean_squared_error: 54.8228\n",
      "Epoch 815/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9136 - mean_squared_error: 55.9136 - val_loss: 54.8519 - val_mean_squared_error: 54.8519\n",
      "Epoch 816/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0166 - mean_squared_error: 56.0166 - val_loss: 55.5520 - val_mean_squared_error: 55.5520\n",
      "Epoch 817/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.9334 - mean_squared_error: 55.9334 - val_loss: 54.8188 - val_mean_squared_error: 54.8188\n",
      "Epoch 818/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.1355 - mean_squared_error: 56.1355 - val_loss: 54.7901 - val_mean_squared_error: 54.7901\n",
      "Epoch 819/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9350 - mean_squared_error: 55.9350 - val_loss: 54.7975 - val_mean_squared_error: 54.7975\n",
      "Epoch 820/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 56.0190 - mean_squared_error: 56.0190 - val_loss: 54.9618 - val_mean_squared_error: 54.9618\n",
      "Epoch 821/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0952 - mean_squared_error: 56.0952 - val_loss: 55.4525 - val_mean_squared_error: 55.4525\n",
      "Epoch 822/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.9401 - mean_squared_error: 55.9401 - val_loss: 54.8088 - val_mean_squared_error: 54.8088\n",
      "Epoch 823/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9493 - mean_squared_error: 55.9493 - val_loss: 54.8441 - val_mean_squared_error: 54.8441\n",
      "Epoch 824/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.9763 - mean_squared_error: 55.9763 - val_loss: 55.1054 - val_mean_squared_error: 55.1054\n",
      "Epoch 825/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9312 - mean_squared_error: 55.9312 - val_loss: 54.8943 - val_mean_squared_error: 54.8943\n",
      "Epoch 826/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.9251 - mean_squared_error: 55.9251 - val_loss: 54.8963 - val_mean_squared_error: 54.8962\n",
      "Epoch 827/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.9676 - mean_squared_error: 55.9675 - val_loss: 54.8201 - val_mean_squared_error: 54.8201\n",
      "Epoch 828/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0187 - mean_squared_error: 56.0187 - val_loss: 55.7945 - val_mean_squared_error: 55.7945\n",
      "Epoch 829/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0388 - mean_squared_error: 56.0388 - val_loss: 54.8480 - val_mean_squared_error: 54.8480\n",
      "Epoch 830/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 56.0089 - mean_squared_error: 56.0089 - val_loss: 55.8844 - val_mean_squared_error: 55.8844\n",
      "Epoch 831/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.9816 - mean_squared_error: 55.9816 - val_loss: 54.7586 - val_mean_squared_error: 54.7586\n",
      "Epoch 832/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.9101 - mean_squared_error: 55.9102 - val_loss: 54.8166 - val_mean_squared_error: 54.8166\n",
      "Epoch 833/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8961 - mean_squared_error: 55.8960 - val_loss: 54.8411 - val_mean_squared_error: 54.8411\n",
      "Epoch 834/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.9389 - mean_squared_error: 55.9389 - val_loss: 54.9892 - val_mean_squared_error: 54.9892\n",
      "Epoch 835/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8622 - mean_squared_error: 55.8622 - val_loss: 54.8093 - val_mean_squared_error: 54.8093\n",
      "Epoch 836/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9634 - mean_squared_error: 55.9634 - val_loss: 54.8486 - val_mean_squared_error: 54.8486\n",
      "Epoch 837/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.9542 - mean_squared_error: 55.9542 - val_loss: 54.9099 - val_mean_squared_error: 54.9099\n",
      "Epoch 838/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9703 - mean_squared_error: 55.9703 - val_loss: 54.7786 - val_mean_squared_error: 54.7786\n",
      "Epoch 839/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.8641 - mean_squared_error: 55.8641 - val_loss: 54.8671 - val_mean_squared_error: 54.8671\n",
      "Epoch 840/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.8798 - mean_squared_error: 55.8798 - val_loss: 54.7912 - val_mean_squared_error: 54.7912\n",
      "Epoch 841/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9549 - mean_squared_error: 55.9549 - val_loss: 55.0994 - val_mean_squared_error: 55.0994\n",
      "Epoch 842/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 56.0027 - mean_squared_error: 56.0027 - val_loss: 55.5584 - val_mean_squared_error: 55.5584\n",
      "Epoch 843/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8546 - mean_squared_error: 55.8546 - val_loss: 54.7817 - val_mean_squared_error: 54.7817\n",
      "Epoch 844/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8330 - mean_squared_error: 55.8330 - val_loss: 54.9888 - val_mean_squared_error: 54.9888\n",
      "Epoch 845/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9103 - mean_squared_error: 55.9103 - val_loss: 54.8106 - val_mean_squared_error: 54.8106\n",
      "Epoch 846/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8444 - mean_squared_error: 55.8445 - val_loss: 54.8581 - val_mean_squared_error: 54.8581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.8628 - mean_squared_error: 55.8628 - val_loss: 55.0664 - val_mean_squared_error: 55.0664\n",
      "Epoch 848/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9032 - mean_squared_error: 55.9032 - val_loss: 55.6021 - val_mean_squared_error: 55.6021\n",
      "Epoch 849/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.8957 - mean_squared_error: 55.8957 - val_loss: 54.9542 - val_mean_squared_error: 54.9542\n",
      "Epoch 850/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8979 - mean_squared_error: 55.8979 - val_loss: 54.9621 - val_mean_squared_error: 54.9621\n",
      "Epoch 851/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8562 - mean_squared_error: 55.8562 - val_loss: 54.8110 - val_mean_squared_error: 54.8110\n",
      "Epoch 852/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.9198 - mean_squared_error: 55.9198 - val_loss: 54.8019 - val_mean_squared_error: 54.8019\n",
      "Epoch 853/1000\n",
      "5600/5600 [==============================] - 0s 57us/sample - loss: 55.8466 - mean_squared_error: 55.8466 - val_loss: 55.0954 - val_mean_squared_error: 55.0954\n",
      "Epoch 854/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9561 - mean_squared_error: 55.9561 - val_loss: 54.8714 - val_mean_squared_error: 54.8714\n",
      "Epoch 855/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9176 - mean_squared_error: 55.9176 - val_loss: 55.1470 - val_mean_squared_error: 55.1470\n",
      "Epoch 856/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9526 - mean_squared_error: 55.9525 - val_loss: 54.7835 - val_mean_squared_error: 54.7835\n",
      "Epoch 857/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8056 - mean_squared_error: 55.8056 - val_loss: 55.7841 - val_mean_squared_error: 55.7841\n",
      "Epoch 858/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.8848 - mean_squared_error: 55.8849 - val_loss: 54.8293 - val_mean_squared_error: 54.8293\n",
      "Epoch 859/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.8239 - mean_squared_error: 55.8239 - val_loss: 54.9010 - val_mean_squared_error: 54.9010\n",
      "Epoch 860/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8568 - mean_squared_error: 55.8568 - val_loss: 54.9988 - val_mean_squared_error: 54.9988\n",
      "Epoch 861/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.8751 - mean_squared_error: 55.8751 - val_loss: 54.9577 - val_mean_squared_error: 54.9577\n",
      "Epoch 862/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.8691 - mean_squared_error: 55.8691 - val_loss: 55.1200 - val_mean_squared_error: 55.1200\n",
      "Epoch 863/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9066 - mean_squared_error: 55.9066 - val_loss: 55.2660 - val_mean_squared_error: 55.2660\n",
      "Epoch 864/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9478 - mean_squared_error: 55.9478 - val_loss: 55.9420 - val_mean_squared_error: 55.9420\n",
      "Epoch 865/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8941 - mean_squared_error: 55.8941 - val_loss: 54.8206 - val_mean_squared_error: 54.8206\n",
      "Epoch 866/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8230 - mean_squared_error: 55.8230 - val_loss: 56.6701 - val_mean_squared_error: 56.6701\n",
      "Epoch 867/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 56.0538 - mean_squared_error: 56.0537 - val_loss: 54.7872 - val_mean_squared_error: 54.7872\n",
      "Epoch 868/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8657 - mean_squared_error: 55.8657 - val_loss: 54.8499 - val_mean_squared_error: 54.8499\n",
      "Epoch 869/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9532 - mean_squared_error: 55.9532 - val_loss: 54.8480 - val_mean_squared_error: 54.8480\n",
      "Epoch 870/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8667 - mean_squared_error: 55.8667 - val_loss: 54.9532 - val_mean_squared_error: 54.9532\n",
      "Epoch 871/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8914 - mean_squared_error: 55.8914 - val_loss: 54.8274 - val_mean_squared_error: 54.8274\n",
      "Epoch 872/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.8165 - mean_squared_error: 55.8165 - val_loss: 55.0429 - val_mean_squared_error: 55.0429\n",
      "Epoch 873/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8468 - mean_squared_error: 55.8467 - val_loss: 55.0417 - val_mean_squared_error: 55.0417\n",
      "Epoch 874/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8516 - mean_squared_error: 55.8516 - val_loss: 54.8400 - val_mean_squared_error: 54.8400\n",
      "Epoch 875/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8806 - mean_squared_error: 55.8806 - val_loss: 54.8325 - val_mean_squared_error: 54.8325\n",
      "Epoch 876/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.8534 - mean_squared_error: 55.8533 - val_loss: 55.1836 - val_mean_squared_error: 55.1836\n",
      "Epoch 877/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.8969 - mean_squared_error: 55.8968 - val_loss: 54.8203 - val_mean_squared_error: 54.8203\n",
      "Epoch 878/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8404 - mean_squared_error: 55.8403 - val_loss: 54.9313 - val_mean_squared_error: 54.9313\n",
      "Epoch 879/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9314 - mean_squared_error: 55.9314 - val_loss: 54.8400 - val_mean_squared_error: 54.8400\n",
      "Epoch 880/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7835 - mean_squared_error: 55.7835 - val_loss: 54.8840 - val_mean_squared_error: 54.8840\n",
      "Epoch 881/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8190 - mean_squared_error: 55.8190 - val_loss: 54.8184 - val_mean_squared_error: 54.8184\n",
      "Epoch 882/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9465 - mean_squared_error: 55.9465 - val_loss: 55.0578 - val_mean_squared_error: 55.0578\n",
      "Epoch 883/1000\n",
      "5600/5600 [==============================] - 0s 55us/sample - loss: 55.7795 - mean_squared_error: 55.7795 - val_loss: 54.9237 - val_mean_squared_error: 54.9237\n",
      "Epoch 884/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.8765 - mean_squared_error: 55.8765 - val_loss: 54.8136 - val_mean_squared_error: 54.8136\n",
      "Epoch 885/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9294 - mean_squared_error: 55.9294 - val_loss: 55.2018 - val_mean_squared_error: 55.2018\n",
      "Epoch 886/1000\n",
      "5600/5600 [==============================] - 0s 56us/sample - loss: 55.9377 - mean_squared_error: 55.9377 - val_loss: 54.9898 - val_mean_squared_error: 54.9898\n",
      "Epoch 887/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.8986 - mean_squared_error: 55.8985 - val_loss: 54.8417 - val_mean_squared_error: 54.8417\n",
      "Epoch 888/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.8270 - mean_squared_error: 55.8270 - val_loss: 55.3722 - val_mean_squared_error: 55.3722\n",
      "Epoch 889/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.8773 - mean_squared_error: 55.8773 - val_loss: 55.5628 - val_mean_squared_error: 55.5628\n",
      "Epoch 890/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.9010 - mean_squared_error: 55.9010 - val_loss: 54.8472 - val_mean_squared_error: 54.8472\n",
      "Epoch 891/1000\n",
      "5600/5600 [==============================] - 0s 59us/sample - loss: 55.7794 - mean_squared_error: 55.7794 - val_loss: 54.9612 - val_mean_squared_error: 54.9612\n",
      "Epoch 892/1000\n",
      "5600/5600 [==============================] - 0s 58us/sample - loss: 55.9348 - mean_squared_error: 55.9348 - val_loss: 55.0841 - val_mean_squared_error: 55.0841\n",
      "Epoch 893/1000\n",
      "5600/5600 [==============================] - 0s 89us/sample - loss: 55.9139 - mean_squared_error: 55.9139 - val_loss: 54.8203 - val_mean_squared_error: 54.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.8661 - mean_squared_error: 55.8661 - val_loss: 54.8332 - val_mean_squared_error: 54.8332\n",
      "Epoch 895/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.8633 - mean_squared_error: 55.8633 - val_loss: 54.8217 - val_mean_squared_error: 54.8217\n",
      "Epoch 896/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 55.8134 - mean_squared_error: 55.8134 - val_loss: 54.8446 - val_mean_squared_error: 54.8446\n",
      "Epoch 897/1000\n",
      "5600/5600 [==============================] - 1s 90us/sample - loss: 55.8366 - mean_squared_error: 55.8366 - val_loss: 54.8939 - val_mean_squared_error: 54.8939\n",
      "Epoch 898/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.7863 - mean_squared_error: 55.7863 - val_loss: 55.0535 - val_mean_squared_error: 55.0535\n",
      "Epoch 899/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.9258 - mean_squared_error: 55.9259 - val_loss: 54.8918 - val_mean_squared_error: 54.8918\n",
      "Epoch 900/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.8883 - mean_squared_error: 55.8883 - val_loss: 54.8570 - val_mean_squared_error: 54.8570\n",
      "Epoch 901/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.8037 - mean_squared_error: 55.8037 - val_loss: 54.8564 - val_mean_squared_error: 54.8564\n",
      "Epoch 902/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.8155 - mean_squared_error: 55.8155 - val_loss: 54.8369 - val_mean_squared_error: 54.8369\n",
      "Epoch 903/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.8210 - mean_squared_error: 55.8210 - val_loss: 54.9004 - val_mean_squared_error: 54.9004\n",
      "Epoch 904/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7983 - mean_squared_error: 55.7983 - val_loss: 55.3310 - val_mean_squared_error: 55.3310\n",
      "Epoch 905/1000\n",
      "5600/5600 [==============================] - 1s 94us/sample - loss: 55.7148 - mean_squared_error: 55.7147 - val_loss: 54.9052 - val_mean_squared_error: 54.9052\n",
      "Epoch 906/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7588 - mean_squared_error: 55.7588 - val_loss: 54.9295 - val_mean_squared_error: 54.9295\n",
      "Epoch 907/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 55.9768 - mean_squared_error: 55.9768 - val_loss: 54.7979 - val_mean_squared_error: 54.7979\n",
      "Epoch 908/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 56.0072 - mean_squared_error: 56.0072 - val_loss: 54.8140 - val_mean_squared_error: 54.8140\n",
      "Epoch 909/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.8564 - mean_squared_error: 55.8564 - val_loss: 54.8244 - val_mean_squared_error: 54.8244\n",
      "Epoch 910/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 55.7949 - mean_squared_error: 55.7949 - val_loss: 54.7938 - val_mean_squared_error: 54.7938\n",
      "Epoch 911/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.7594 - mean_squared_error: 55.7594 - val_loss: 54.8287 - val_mean_squared_error: 54.8287\n",
      "Epoch 912/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.7237 - mean_squared_error: 55.7237 - val_loss: 55.2410 - val_mean_squared_error: 55.2410\n",
      "Epoch 913/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.8144 - mean_squared_error: 55.8144 - val_loss: 54.9123 - val_mean_squared_error: 54.9123\n",
      "Epoch 914/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 55.8620 - mean_squared_error: 55.8620 - val_loss: 55.0957 - val_mean_squared_error: 55.0957\n",
      "Epoch 915/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.9044 - mean_squared_error: 55.9044 - val_loss: 54.9714 - val_mean_squared_error: 54.9714\n",
      "Epoch 916/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.8168 - mean_squared_error: 55.8168 - val_loss: 55.1074 - val_mean_squared_error: 55.1074\n",
      "Epoch 917/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.8185 - mean_squared_error: 55.8185 - val_loss: 54.8065 - val_mean_squared_error: 54.8065\n",
      "Epoch 918/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7560 - mean_squared_error: 55.7560 - val_loss: 54.8380 - val_mean_squared_error: 54.8380\n",
      "Epoch 919/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.7403 - mean_squared_error: 55.7403 - val_loss: 54.8606 - val_mean_squared_error: 54.8606\n",
      "Epoch 920/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.7636 - mean_squared_error: 55.7636 - val_loss: 55.2239 - val_mean_squared_error: 55.2239\n",
      "Epoch 921/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7932 - mean_squared_error: 55.7931 - val_loss: 54.7940 - val_mean_squared_error: 54.7940\n",
      "Epoch 922/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.8067 - mean_squared_error: 55.8067 - val_loss: 54.9806 - val_mean_squared_error: 54.9806\n",
      "Epoch 923/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.7208 - mean_squared_error: 55.7208 - val_loss: 54.7986 - val_mean_squared_error: 54.7986\n",
      "Epoch 924/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.8233 - mean_squared_error: 55.8233 - val_loss: 54.8871 - val_mean_squared_error: 54.8871\n",
      "Epoch 925/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.8042 - mean_squared_error: 55.8042 - val_loss: 54.9151 - val_mean_squared_error: 54.9150\n",
      "Epoch 926/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.7044 - mean_squared_error: 55.7044 - val_loss: 54.8054 - val_mean_squared_error: 54.8054\n",
      "Epoch 927/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.7049 - mean_squared_error: 55.7049 - val_loss: 55.6667 - val_mean_squared_error: 55.6667\n",
      "Epoch 928/1000\n",
      "5600/5600 [==============================] - 0s 62us/sample - loss: 55.6900 - mean_squared_error: 55.6899 - val_loss: 55.1025 - val_mean_squared_error: 55.1025\n",
      "Epoch 929/1000\n",
      "5600/5600 [==============================] - 0s 78us/sample - loss: 55.8394 - mean_squared_error: 55.8394 - val_loss: 54.8074 - val_mean_squared_error: 54.8074\n",
      "Epoch 930/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.9600 - mean_squared_error: 55.9600 - val_loss: 54.8124 - val_mean_squared_error: 54.8125\n",
      "Epoch 931/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 55.7473 - mean_squared_error: 55.7473 - val_loss: 54.8130 - val_mean_squared_error: 54.8130\n",
      "Epoch 932/1000\n",
      "5600/5600 [==============================] - 1s 111us/sample - loss: 55.7179 - mean_squared_error: 55.7179 - val_loss: 55.0736 - val_mean_squared_error: 55.0736\n",
      "Epoch 933/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.8107 - mean_squared_error: 55.8107 - val_loss: 54.8765 - val_mean_squared_error: 54.8765\n",
      "Epoch 934/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.8536 - mean_squared_error: 55.8535 - val_loss: 54.7966 - val_mean_squared_error: 54.7966\n",
      "Epoch 935/1000\n",
      "5600/5600 [==============================] - 0s 76us/sample - loss: 55.7659 - mean_squared_error: 55.7659 - val_loss: 55.4116 - val_mean_squared_error: 55.4116\n",
      "Epoch 936/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.8175 - mean_squared_error: 55.8175 - val_loss: 54.8095 - val_mean_squared_error: 54.8095\n",
      "Epoch 937/1000\n",
      "5600/5600 [==============================] - 0s 87us/sample - loss: 55.8497 - mean_squared_error: 55.8497 - val_loss: 54.8566 - val_mean_squared_error: 54.8566\n",
      "Epoch 938/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.7867 - mean_squared_error: 55.7867 - val_loss: 55.4051 - val_mean_squared_error: 55.4051\n",
      "Epoch 939/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.8650 - mean_squared_error: 55.8649 - val_loss: 54.8473 - val_mean_squared_error: 54.8473\n",
      "Epoch 940/1000\n",
      "5600/5600 [==============================] - 0s 84us/sample - loss: 55.7906 - mean_squared_error: 55.7906 - val_loss: 55.0014 - val_mean_squared_error: 55.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 941/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7806 - mean_squared_error: 55.7807 - val_loss: 55.0205 - val_mean_squared_error: 55.0205\n",
      "Epoch 942/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.7381 - mean_squared_error: 55.7381 - val_loss: 54.9831 - val_mean_squared_error: 54.9831\n",
      "Epoch 943/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7722 - mean_squared_error: 55.7722 - val_loss: 54.9203 - val_mean_squared_error: 54.9203\n",
      "Epoch 944/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.7889 - mean_squared_error: 55.7889 - val_loss: 54.8444 - val_mean_squared_error: 54.8444\n",
      "Epoch 945/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7913 - mean_squared_error: 55.7912 - val_loss: 55.5003 - val_mean_squared_error: 55.5003\n",
      "Epoch 946/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7694 - mean_squared_error: 55.7694 - val_loss: 54.9149 - val_mean_squared_error: 54.9149\n",
      "Epoch 947/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.6747 - mean_squared_error: 55.6747 - val_loss: 54.9513 - val_mean_squared_error: 54.9513\n",
      "Epoch 948/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.7932 - mean_squared_error: 55.7932 - val_loss: 55.0117 - val_mean_squared_error: 55.0117\n",
      "Epoch 949/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7096 - mean_squared_error: 55.7096 - val_loss: 54.8564 - val_mean_squared_error: 54.8563\n",
      "Epoch 950/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.7456 - mean_squared_error: 55.7455 - val_loss: 54.8817 - val_mean_squared_error: 54.8817\n",
      "Epoch 951/1000\n",
      "5600/5600 [==============================] - 0s 63us/sample - loss: 55.7338 - mean_squared_error: 55.7338 - val_loss: 54.9477 - val_mean_squared_error: 54.9477\n",
      "Epoch 952/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.7019 - mean_squared_error: 55.7019 - val_loss: 54.8055 - val_mean_squared_error: 54.8055\n",
      "Epoch 953/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.6877 - mean_squared_error: 55.6877 - val_loss: 54.9591 - val_mean_squared_error: 54.9591\n",
      "Epoch 954/1000\n",
      "5600/5600 [==============================] - 0s 64us/sample - loss: 55.6391 - mean_squared_error: 55.6391 - val_loss: 55.1917 - val_mean_squared_error: 55.1917\n",
      "Epoch 955/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.6820 - mean_squared_error: 55.6820 - val_loss: 55.1086 - val_mean_squared_error: 55.1086\n",
      "Epoch 956/1000\n",
      "5600/5600 [==============================] - 0s 81us/sample - loss: 55.7410 - mean_squared_error: 55.7411 - val_loss: 54.8982 - val_mean_squared_error: 54.8982\n",
      "Epoch 957/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.7037 - mean_squared_error: 55.7037 - val_loss: 55.2772 - val_mean_squared_error: 55.2772\n",
      "Epoch 958/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.7560 - mean_squared_error: 55.7560 - val_loss: 54.8111 - val_mean_squared_error: 54.8111\n",
      "Epoch 959/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7078 - mean_squared_error: 55.7078 - val_loss: 54.7740 - val_mean_squared_error: 54.7740\n",
      "Epoch 960/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.7729 - mean_squared_error: 55.7729 - val_loss: 54.8906 - val_mean_squared_error: 54.8906\n",
      "Epoch 961/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7084 - mean_squared_error: 55.7084 - val_loss: 54.9083 - val_mean_squared_error: 54.9083\n",
      "Epoch 962/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.6656 - mean_squared_error: 55.6656 - val_loss: 54.8381 - val_mean_squared_error: 54.8381\n",
      "Epoch 963/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.7219 - mean_squared_error: 55.7219 - val_loss: 54.8466 - val_mean_squared_error: 54.8466\n",
      "Epoch 964/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.6874 - mean_squared_error: 55.6874 - val_loss: 54.7918 - val_mean_squared_error: 54.7918\n",
      "Epoch 965/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 55.6410 - mean_squared_error: 55.6410 - val_loss: 54.8633 - val_mean_squared_error: 54.8633\n",
      "Epoch 966/1000\n",
      "5600/5600 [==============================] - 0s 68us/sample - loss: 55.7909 - mean_squared_error: 55.7910 - val_loss: 54.9809 - val_mean_squared_error: 54.9809\n",
      "Epoch 967/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.7650 - mean_squared_error: 55.7649 - val_loss: 54.8441 - val_mean_squared_error: 54.8441\n",
      "Epoch 968/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.6334 - mean_squared_error: 55.6334 - val_loss: 54.8888 - val_mean_squared_error: 54.8888\n",
      "Epoch 969/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.6887 - mean_squared_error: 55.6887 - val_loss: 55.1359 - val_mean_squared_error: 55.1359\n",
      "Epoch 970/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.5925 - mean_squared_error: 55.5925 - val_loss: 54.8782 - val_mean_squared_error: 54.8781\n",
      "Epoch 971/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.6558 - mean_squared_error: 55.6558 - val_loss: 54.9187 - val_mean_squared_error: 54.9187\n",
      "Epoch 972/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.6460 - mean_squared_error: 55.6460 - val_loss: 54.9534 - val_mean_squared_error: 54.9534\n",
      "Epoch 973/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.8140 - mean_squared_error: 55.8140 - val_loss: 54.8479 - val_mean_squared_error: 54.8479\n",
      "Epoch 974/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.5774 - mean_squared_error: 55.5774 - val_loss: 54.8386 - val_mean_squared_error: 54.8386\n",
      "Epoch 975/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.5699 - mean_squared_error: 55.5699 - val_loss: 56.4422 - val_mean_squared_error: 56.4422\n",
      "Epoch 976/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.7104 - mean_squared_error: 55.7105 - val_loss: 54.8634 - val_mean_squared_error: 54.8634\n",
      "Epoch 977/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.6748 - mean_squared_error: 55.6749 - val_loss: 54.9365 - val_mean_squared_error: 54.9365\n",
      "Epoch 978/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 55.6713 - mean_squared_error: 55.6713 - val_loss: 54.7950 - val_mean_squared_error: 54.7950\n",
      "Epoch 979/1000\n",
      "5600/5600 [==============================] - 0s 70us/sample - loss: 55.5710 - mean_squared_error: 55.5710 - val_loss: 54.9323 - val_mean_squared_error: 54.9323\n",
      "Epoch 980/1000\n",
      "5600/5600 [==============================] - 0s 74us/sample - loss: 55.7254 - mean_squared_error: 55.7255 - val_loss: 54.7978 - val_mean_squared_error: 54.7978\n",
      "Epoch 981/1000\n",
      "5600/5600 [==============================] - 0s 65us/sample - loss: 55.7225 - mean_squared_error: 55.7225 - val_loss: 54.9940 - val_mean_squared_error: 54.9940\n",
      "Epoch 982/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.6513 - mean_squared_error: 55.6514 - val_loss: 54.9219 - val_mean_squared_error: 54.9219\n",
      "Epoch 983/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.6209 - mean_squared_error: 55.6209 - val_loss: 54.8252 - val_mean_squared_error: 54.8252\n",
      "Epoch 984/1000\n",
      "5600/5600 [==============================] - 0s 73us/sample - loss: 55.7982 - mean_squared_error: 55.7982 - val_loss: 54.8444 - val_mean_squared_error: 54.8444\n",
      "Epoch 985/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.7136 - mean_squared_error: 55.7136 - val_loss: 55.0177 - val_mean_squared_error: 55.0177\n",
      "Epoch 986/1000\n",
      "5600/5600 [==============================] - 0s 85us/sample - loss: 55.7497 - mean_squared_error: 55.7497 - val_loss: 54.9352 - val_mean_squared_error: 54.9352\n",
      "Epoch 987/1000\n",
      "5600/5600 [==============================] - 1s 92us/sample - loss: 55.6305 - mean_squared_error: 55.6305 - val_loss: 54.8188 - val_mean_squared_error: 54.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 988/1000\n",
      "5600/5600 [==============================] - 1s 112us/sample - loss: 55.6887 - mean_squared_error: 55.6887 - val_loss: 54.7987 - val_mean_squared_error: 54.7988\n",
      "Epoch 989/1000\n",
      "5600/5600 [==============================] - 1s 91us/sample - loss: 55.6928 - mean_squared_error: 55.6928 - val_loss: 54.9781 - val_mean_squared_error: 54.9781\n",
      "Epoch 990/1000\n",
      "5600/5600 [==============================] - 0s 67us/sample - loss: 55.6655 - mean_squared_error: 55.6655 - val_loss: 54.9287 - val_mean_squared_error: 54.9287\n",
      "Epoch 991/1000\n",
      "5600/5600 [==============================] - 0s 71us/sample - loss: 55.8149 - mean_squared_error: 55.8149 - val_loss: 54.8063 - val_mean_squared_error: 54.8063\n",
      "Epoch 992/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.5921 - mean_squared_error: 55.5921 - val_loss: 55.0886 - val_mean_squared_error: 55.0886\n",
      "Epoch 993/1000\n",
      "5600/5600 [==============================] - 0s 72us/sample - loss: 55.6620 - mean_squared_error: 55.6620 - val_loss: 54.8038 - val_mean_squared_error: 54.8038\n",
      "Epoch 994/1000\n",
      "5600/5600 [==============================] - 0s 66us/sample - loss: 55.6780 - mean_squared_error: 55.6780 - val_loss: 54.8389 - val_mean_squared_error: 54.8389\n",
      "Epoch 995/1000\n",
      "5600/5600 [==============================] - 0s 69us/sample - loss: 55.5674 - mean_squared_error: 55.5674 - val_loss: 55.5306 - val_mean_squared_error: 55.5306\n",
      "Epoch 996/1000\n",
      "5600/5600 [==============================] - 0s 82us/sample - loss: 55.7069 - mean_squared_error: 55.7069 - val_loss: 55.3909 - val_mean_squared_error: 55.3909\n",
      "Epoch 997/1000\n",
      "5600/5600 [==============================] - 0s 60us/sample - loss: 55.7087 - mean_squared_error: 55.7087 - val_loss: 54.8031 - val_mean_squared_error: 54.8031\n",
      "Epoch 998/1000\n",
      "5600/5600 [==============================] - 0s 79us/sample - loss: 55.6068 - mean_squared_error: 55.6068 - val_loss: 54.8231 - val_mean_squared_error: 54.8231\n",
      "Epoch 999/1000\n",
      "5600/5600 [==============================] - 0s 61us/sample - loss: 55.7635 - mean_squared_error: 55.7635 - val_loss: 55.0477 - val_mean_squared_error: 55.0477\n",
      "Epoch 1000/1000\n",
      "5600/5600 [==============================] - 0s 77us/sample - loss: 55.6805 - mean_squared_error: 55.6805 - val_loss: 54.9627 - val_mean_squared_error: 54.9627\n"
     ]
    }
   ],
   "source": [
    "# quantitative: ANN\n",
    "# specify network layers\n",
    "quant_ann = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid', input_shape = (13, )),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'linear')\n",
    "])\n",
    "\n",
    "# compile and fit network\n",
    "quant_ann.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error']) \n",
    "history = quant_ann.fit(X_train, y_train, epochs = 1000, batch_size = 32, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('Model MSE vs. Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(bottom = 40)\n",
    "plt.ylim(top = 100)\n",
    "plt.legend(['Training', 'Validation'], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mean_squared_error', 'val_loss', 'val_mean_squared_error'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
